{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94f8f1f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 40, 22, 976]           1,040\n",
      "            Conv2d-2           [-1, 40, 1, 976]          35,240\n",
      "       BatchNorm2d-3           [-1, 40, 1, 976]              80\n",
      "               ELU-4           [-1, 40, 1, 976]               0\n",
      "         AvgPool2d-5            [-1, 40, 1, 61]               0\n",
      "           Dropout-6            [-1, 40, 1, 61]               0\n",
      "            Conv2d-7            [-1, 40, 1, 61]           1,640\n",
      "         Rearrange-8               [-1, 61, 40]               0\n",
      "         LayerNorm-9               [-1, 61, 40]              80\n",
      "           Linear-10               [-1, 61, 40]           1,640\n",
      "           Linear-11               [-1, 61, 40]           1,640\n",
      "           Linear-12               [-1, 61, 40]           1,640\n",
      "          Dropout-13           [-1, 10, 61, 61]               0\n",
      "           Linear-14               [-1, 61, 40]           1,640\n",
      "MultiHeadAttention-15               [-1, 61, 40]               0\n",
      "          Dropout-16               [-1, 61, 40]               0\n",
      "      ResidualAdd-17               [-1, 61, 40]               0\n",
      "        LayerNorm-18               [-1, 61, 40]              80\n",
      "           Linear-19              [-1, 61, 160]           6,560\n",
      "             GELU-20              [-1, 61, 160]               0\n",
      "          Dropout-21              [-1, 61, 160]               0\n",
      "           Linear-22               [-1, 61, 40]           6,440\n",
      "          Dropout-23               [-1, 61, 40]               0\n",
      "      ResidualAdd-24               [-1, 61, 40]               0\n",
      "        LayerNorm-25               [-1, 61, 40]              80\n",
      "           Linear-26               [-1, 61, 40]           1,640\n",
      "           Linear-27               [-1, 61, 40]           1,640\n",
      "           Linear-28               [-1, 61, 40]           1,640\n",
      "          Dropout-29           [-1, 10, 61, 61]               0\n",
      "           Linear-30               [-1, 61, 40]           1,640\n",
      "MultiHeadAttention-31               [-1, 61, 40]               0\n",
      "          Dropout-32               [-1, 61, 40]               0\n",
      "      ResidualAdd-33               [-1, 61, 40]               0\n",
      "        LayerNorm-34               [-1, 61, 40]              80\n",
      "           Linear-35              [-1, 61, 160]           6,560\n",
      "             GELU-36              [-1, 61, 160]               0\n",
      "          Dropout-37              [-1, 61, 160]               0\n",
      "           Linear-38               [-1, 61, 40]           6,440\n",
      "          Dropout-39               [-1, 61, 40]               0\n",
      "      ResidualAdd-40               [-1, 61, 40]               0\n",
      "        LayerNorm-41               [-1, 61, 40]              80\n",
      "           Linear-42               [-1, 61, 40]           1,640\n",
      "           Linear-43               [-1, 61, 40]           1,640\n",
      "           Linear-44               [-1, 61, 40]           1,640\n",
      "          Dropout-45           [-1, 10, 61, 61]               0\n",
      "           Linear-46               [-1, 61, 40]           1,640\n",
      "MultiHeadAttention-47               [-1, 61, 40]               0\n",
      "          Dropout-48               [-1, 61, 40]               0\n",
      "      ResidualAdd-49               [-1, 61, 40]               0\n",
      "        LayerNorm-50               [-1, 61, 40]              80\n",
      "           Linear-51              [-1, 61, 160]           6,560\n",
      "             GELU-52              [-1, 61, 160]               0\n",
      "          Dropout-53              [-1, 61, 160]               0\n",
      "           Linear-54               [-1, 61, 40]           6,440\n",
      "          Dropout-55               [-1, 61, 40]               0\n",
      "      ResidualAdd-56               [-1, 61, 40]               0\n",
      "        LayerNorm-57               [-1, 61, 40]              80\n",
      "           Linear-58               [-1, 61, 40]           1,640\n",
      "           Linear-59               [-1, 61, 40]           1,640\n",
      "           Linear-60               [-1, 61, 40]           1,640\n",
      "          Dropout-61           [-1, 10, 61, 61]               0\n",
      "           Linear-62               [-1, 61, 40]           1,640\n",
      "MultiHeadAttention-63               [-1, 61, 40]               0\n",
      "          Dropout-64               [-1, 61, 40]               0\n",
      "      ResidualAdd-65               [-1, 61, 40]               0\n",
      "        LayerNorm-66               [-1, 61, 40]              80\n",
      "           Linear-67              [-1, 61, 160]           6,560\n",
      "             GELU-68              [-1, 61, 160]               0\n",
      "          Dropout-69              [-1, 61, 160]               0\n",
      "           Linear-70               [-1, 61, 40]           6,440\n",
      "          Dropout-71               [-1, 61, 40]               0\n",
      "      ResidualAdd-72               [-1, 61, 40]               0\n",
      "        LayerNorm-73               [-1, 61, 40]              80\n",
      "           Linear-74               [-1, 61, 40]           1,640\n",
      "           Linear-75               [-1, 61, 40]           1,640\n",
      "           Linear-76               [-1, 61, 40]           1,640\n",
      "          Dropout-77           [-1, 10, 61, 61]               0\n",
      "           Linear-78               [-1, 61, 40]           1,640\n",
      "MultiHeadAttention-79               [-1, 61, 40]               0\n",
      "          Dropout-80               [-1, 61, 40]               0\n",
      "      ResidualAdd-81               [-1, 61, 40]               0\n",
      "        LayerNorm-82               [-1, 61, 40]              80\n",
      "           Linear-83              [-1, 61, 160]           6,560\n",
      "             GELU-84              [-1, 61, 160]               0\n",
      "          Dropout-85              [-1, 61, 160]               0\n",
      "           Linear-86               [-1, 61, 40]           6,440\n",
      "          Dropout-87               [-1, 61, 40]               0\n",
      "      ResidualAdd-88               [-1, 61, 40]               0\n",
      "        LayerNorm-89               [-1, 61, 40]              80\n",
      "           Linear-90               [-1, 61, 40]           1,640\n",
      "           Linear-91               [-1, 61, 40]           1,640\n",
      "           Linear-92               [-1, 61, 40]           1,640\n",
      "          Dropout-93           [-1, 10, 61, 61]               0\n",
      "           Linear-94               [-1, 61, 40]           1,640\n",
      "MultiHeadAttention-95               [-1, 61, 40]               0\n",
      "          Dropout-96               [-1, 61, 40]               0\n",
      "      ResidualAdd-97               [-1, 61, 40]               0\n",
      "        LayerNorm-98               [-1, 61, 40]              80\n",
      "           Linear-99              [-1, 61, 160]           6,560\n",
      "            GELU-100              [-1, 61, 160]               0\n",
      "         Dropout-101              [-1, 61, 160]               0\n",
      "          Linear-102               [-1, 61, 40]           6,440\n",
      "         Dropout-103               [-1, 61, 40]               0\n",
      "     ResidualAdd-104               [-1, 61, 40]               0\n",
      "         Flatten-105                 [-1, 2440]               0\n",
      "         Dropout-106                 [-1, 2440]               0\n",
      "          Linear-107                    [-1, 4]           9,764\n",
      "       Conformer-108                    [-1, 4]               0\n",
      "================================================================\n",
      "Total params: 166,084\n",
      "Trainable params: 166,084\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.08\n",
      "Forward/backward pass size (MB): 11.94\n",
      "Params size (MB): 0.63\n",
      "Estimated Total Size (MB): 12.66\n",
      "----------------------------------------------------------------\n",
      "Tue Apr 30 09:47:27 2024\n",
      "seed is 1715\n",
      "Subject 1\n",
      "-------------------- train size： (288, 1, 22, 1000) test size： (288, 22, 1000)\n",
      "1_0 train_acc: 0.2809 train_loss: 1.543524\tval_acc: 0.392157 val_loss: 1.3172971\n",
      "1_1 train_acc: 0.3670 train_loss: 1.443488\tval_acc: 0.421569 val_loss: 1.2277728\n",
      "1_2 train_acc: 0.3820 train_loss: 1.377914\tval_acc: 0.470588 val_loss: 1.1565098\n",
      "1_3 train_acc: 0.3970 train_loss: 1.323579\tval_acc: 0.509804 val_loss: 1.1261851\n",
      "1_5 train_acc: 0.4569 train_loss: 1.300803\tval_acc: 0.578431 val_loss: 0.9819031\n",
      "1_7 train_acc: 0.6142 train_loss: 0.895630\tval_acc: 0.666667 val_loss: 0.8178831\n",
      "1_9 train_acc: 0.5805 train_loss: 0.997986\tval_acc: 0.725490 val_loss: 0.6492054\n",
      "1_12 train_acc: 0.6629 train_loss: 0.754332\tval_acc: 0.750000 val_loss: 0.5357719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1_17 train_acc: 0.7903 train_loss: 0.531925\tval_acc: 0.852941 val_loss: 0.3680937\n",
      "1_20 train_acc: 0.7978 train_loss: 0.543555\tval_acc: 0.862745 val_loss: 0.3142221\n",
      "1_26 train_acc: 0.8614 train_loss: 0.367490\tval_acc: 0.901961 val_loss: 0.2921475\n",
      "1_33 train_acc: 0.9176 train_loss: 0.208764\tval_acc: 0.887255 val_loss: 0.2638387\n",
      "1_35 train_acc: 0.8689 train_loss: 0.320940\tval_acc: 0.887255 val_loss: 0.2609027\n",
      "1_39 train_acc: 0.8914 train_loss: 0.263110\tval_acc: 0.887255 val_loss: 0.2559395\n",
      "1_40 train_acc: 0.8614 train_loss: 0.374204\tval_acc: 0.931373 val_loss: 0.1919388\n",
      "1_42 train_acc: 0.9326 train_loss: 0.190137\tval_acc: 0.901961 val_loss: 0.1916173\n",
      "1_43 train_acc: 0.9326 train_loss: 0.158938\tval_acc: 0.946078 val_loss: 0.1543802\n",
      "1_49 train_acc: 0.9101 train_loss: 0.233476\tval_acc: 0.950980 val_loss: 0.1175394\n",
      "1_54 train_acc: 0.9326 train_loss: 0.174541\tval_acc: 0.960784 val_loss: 0.1094528\n",
      "1_76 train_acc: 0.9513 train_loss: 0.141057\tval_acc: 0.950980 val_loss: 0.0883269\n",
      "1_85 train_acc: 0.9476 train_loss: 0.161703\tval_acc: 0.980392 val_loss: 0.0690770\n",
      "1_86 train_acc: 0.9663 train_loss: 0.095980\tval_acc: 0.980392 val_loss: 0.0491998\n",
      "1_88 train_acc: 0.9625 train_loss: 0.109621\tval_acc: 0.985294 val_loss: 0.0403085\n",
      "1_90 train_acc: 0.9476 train_loss: 0.176292\tval_acc: 0.980392 val_loss: 0.0330054\n",
      "1_106 train_acc: 0.9588 train_loss: 0.136764\tval_acc: 0.995098 val_loss: 0.0253119\n",
      "1_123 train_acc: 0.9775 train_loss: 0.063574\tval_acc: 0.990196 val_loss: 0.0207239\n",
      "1_126 train_acc: 0.9588 train_loss: 0.104048\tval_acc: 1.000000 val_loss: 0.0202764\n",
      "1_131 train_acc: 0.9738 train_loss: 0.090457\tval_acc: 1.000000 val_loss: 0.0120035\n",
      "1_137 train_acc: 0.9775 train_loss: 0.074309\tval_acc: 1.000000 val_loss: 0.0093269\n",
      "1_156 train_acc: 0.9625 train_loss: 0.105152\tval_acc: 1.000000 val_loss: 0.0085255\n",
      "1_159 train_acc: 0.9625 train_loss: 0.082920\tval_acc: 1.000000 val_loss: 0.0070668\n",
      "1_181 train_acc: 0.9850 train_loss: 0.045500\tval_acc: 1.000000 val_loss: 0.0040861\n",
      "1_192 train_acc: 0.9888 train_loss: 0.037263\tval_acc: 1.000000 val_loss: 0.0011659\n",
      "1_215 train_acc: 0.9775 train_loss: 0.063197\tval_acc: 1.000000 val_loss: 0.0006672\n",
      "1_245 train_acc: 0.9925 train_loss: 0.028007\tval_acc: 1.000000 val_loss: 0.0006660\n",
      "1_277 train_acc: 0.9888 train_loss: 0.019230\tval_acc: 1.000000 val_loss: 0.0002104\n",
      "1_279 train_acc: 0.9888 train_loss: 0.025885\tval_acc: 1.000000 val_loss: 0.0002030\n",
      "1_334 train_acc: 0.9925 train_loss: 0.037739\tval_acc: 1.000000 val_loss: 0.0001388\n",
      "1_356 train_acc: 0.9888 train_loss: 0.047437\tval_acc: 1.000000 val_loss: 0.0000878\n",
      "1_400 train_acc: 0.9850 train_loss: 0.046640\tval_acc: 1.000000 val_loss: 0.0000683\n",
      "1_442 train_acc: 0.9925 train_loss: 0.014346\tval_acc: 1.000000 val_loss: 0.0000420\n",
      "1_486 train_acc: 0.9925 train_loss: 0.014076\tval_acc: 1.000000 val_loss: 0.0000383\n",
      "1_526 train_acc: 0.9925 train_loss: 0.026306\tval_acc: 1.000000 val_loss: 0.0000223\n",
      "1_536 train_acc: 0.9925 train_loss: 0.023569\tval_acc: 1.000000 val_loss: 0.0000169\n",
      "1_598 train_acc: 0.9925 train_loss: 0.022218\tval_acc: 1.000000 val_loss: 0.0000078\n",
      "1_636 train_acc: 0.9963 train_loss: 0.016781\tval_acc: 1.000000 val_loss: 0.0000065\n",
      "1_676 train_acc: 0.9925 train_loss: 0.037386\tval_acc: 1.000000 val_loss: 0.0000034\n",
      "1_741 train_acc: 1.0000 train_loss: 0.003935\tval_acc: 1.000000 val_loss: 0.0000013\n",
      "1_842 train_acc: 0.9888 train_loss: 0.022542\tval_acc: 1.000000 val_loss: 0.0000009\n",
      "1_911 train_acc: 1.0000 train_loss: 0.001271\tval_acc: 1.000000 val_loss: 0.0000009\n",
      "1_923 train_acc: 1.0000 train_loss: 0.003223\tval_acc: 1.000000 val_loss: 0.0000002\n",
      "epoch:  923 \tThe test accuracy is: 0.8506944444444444\n",
      " THE BEST ACCURACY IS 0.8506944444444444\tkappa is 0.8009259259259259\n",
      "subject 1 duration: 1:35:41.122703\n",
      "seed is 1160\n",
      "Subject 2\n",
      "-------------------- train size： (288, 1, 22, 1000) test size： (288, 22, 1000)\n",
      "2_0 train_acc: 0.2884 train_loss: 1.596548\tval_acc: 0.264706 val_loss: 1.4054848\n",
      "2_1 train_acc: 0.3109 train_loss: 1.473047\tval_acc: 0.387255 val_loss: 1.2968948\n",
      "2_2 train_acc: 0.3258 train_loss: 1.485732\tval_acc: 0.441176 val_loss: 1.1953679\n",
      "2_3 train_acc: 0.4232 train_loss: 1.303966\tval_acc: 0.529412 val_loss: 1.0992932\n",
      "2_4 train_acc: 0.4457 train_loss: 1.264418\tval_acc: 0.549020 val_loss: 1.0599228\n",
      "2_5 train_acc: 0.4307 train_loss: 1.298041\tval_acc: 0.539216 val_loss: 1.0344718\n",
      "2_6 train_acc: 0.4981 train_loss: 1.172230\tval_acc: 0.617647 val_loss: 0.9450704\n",
      "2_7 train_acc: 0.5169 train_loss: 1.097913\tval_acc: 0.622549 val_loss: 0.8885412\n",
      "2_8 train_acc: 0.5655 train_loss: 1.062977\tval_acc: 0.651961 val_loss: 0.8318842\n",
      "2_9 train_acc: 0.5843 train_loss: 0.988506\tval_acc: 0.671569 val_loss: 0.8240720\n",
      "2_10 train_acc: 0.6142 train_loss: 0.875866\tval_acc: 0.710784 val_loss: 0.7843693\n",
      "2_11 train_acc: 0.6667 train_loss: 0.853541\tval_acc: 0.656863 val_loss: 0.7784059\n",
      "2_12 train_acc: 0.6367 train_loss: 0.918740\tval_acc: 0.730392 val_loss: 0.6779312\n",
      "2_17 train_acc: 0.6779 train_loss: 0.798533\tval_acc: 0.750000 val_loss: 0.6203963\n",
      "2_18 train_acc: 0.7004 train_loss: 0.729735\tval_acc: 0.789216 val_loss: 0.5480293\n",
      "2_19 train_acc: 0.7378 train_loss: 0.647951\tval_acc: 0.813725 val_loss: 0.4784418\n",
      "2_23 train_acc: 0.6742 train_loss: 0.806107\tval_acc: 0.818627 val_loss: 0.4684921\n",
      "2_27 train_acc: 0.7903 train_loss: 0.561135\tval_acc: 0.833333 val_loss: 0.4631828\n",
      "2_28 train_acc: 0.6779 train_loss: 0.748608\tval_acc: 0.838235 val_loss: 0.4004650\n",
      "2_31 train_acc: 0.7640 train_loss: 0.533988\tval_acc: 0.848039 val_loss: 0.3707951\n",
      "2_42 train_acc: 0.8052 train_loss: 0.552080\tval_acc: 0.887255 val_loss: 0.3062741\n",
      "2_46 train_acc: 0.7978 train_loss: 0.480561\tval_acc: 0.877451 val_loss: 0.2985273\n",
      "2_47 train_acc: 0.7753 train_loss: 0.606319\tval_acc: 0.916667 val_loss: 0.2940542\n",
      "2_49 train_acc: 0.7828 train_loss: 0.528926\tval_acc: 0.892157 val_loss: 0.2801925\n",
      "2_50 train_acc: 0.8427 train_loss: 0.437839\tval_acc: 0.916667 val_loss: 0.2585050\n",
      "2_55 train_acc: 0.8127 train_loss: 0.491320\tval_acc: 0.926471 val_loss: 0.2284151\n",
      "2_64 train_acc: 0.8277 train_loss: 0.432752\tval_acc: 0.955882 val_loss: 0.1714683\n",
      "2_71 train_acc: 0.8577 train_loss: 0.332912\tval_acc: 0.941176 val_loss: 0.1624919\n",
      "2_74 train_acc: 0.8352 train_loss: 0.426967\tval_acc: 0.960784 val_loss: 0.1323858\n",
      "2_79 train_acc: 0.8240 train_loss: 0.427645\tval_acc: 0.960784 val_loss: 0.1311261\n",
      "2_80 train_acc: 0.8539 train_loss: 0.377355\tval_acc: 0.970588 val_loss: 0.1295934\n",
      "2_81 train_acc: 0.8240 train_loss: 0.381686\tval_acc: 0.965686 val_loss: 0.1181958\n",
      "2_85 train_acc: 0.8464 train_loss: 0.374869\tval_acc: 0.970588 val_loss: 0.1171307\n",
      "2_88 train_acc: 0.8652 train_loss: 0.391541\tval_acc: 0.980392 val_loss: 0.0910944\n",
      "2_90 train_acc: 0.8689 train_loss: 0.356822\tval_acc: 0.975490 val_loss: 0.0906625\n",
      "2_94 train_acc: 0.8764 train_loss: 0.305166\tval_acc: 0.980392 val_loss: 0.0747151\n",
      "2_103 train_acc: 0.9139 train_loss: 0.239095\tval_acc: 0.985294 val_loss: 0.0605121\n",
      "2_108 train_acc: 0.9251 train_loss: 0.237705\tval_acc: 0.990196 val_loss: 0.0582303\n",
      "2_114 train_acc: 0.8951 train_loss: 0.291461\tval_acc: 0.995098 val_loss: 0.0378363\n",
      "2_120 train_acc: 0.8727 train_loss: 0.351225\tval_acc: 1.000000 val_loss: 0.0349192\n",
      "2_126 train_acc: 0.9213 train_loss: 0.258564\tval_acc: 1.000000 val_loss: 0.0209618\n",
      "2_144 train_acc: 0.9101 train_loss: 0.271148\tval_acc: 1.000000 val_loss: 0.0151120\n",
      "2_156 train_acc: 0.9551 train_loss: 0.152415\tval_acc: 1.000000 val_loss: 0.0129028\n",
      "2_160 train_acc: 0.9551 train_loss: 0.133493\tval_acc: 1.000000 val_loss: 0.0128535\n",
      "2_162 train_acc: 0.9213 train_loss: 0.150039\tval_acc: 1.000000 val_loss: 0.0112575\n",
      "2_175 train_acc: 0.9401 train_loss: 0.156200\tval_acc: 1.000000 val_loss: 0.0079403\n",
      "2_176 train_acc: 0.9176 train_loss: 0.202456\tval_acc: 1.000000 val_loss: 0.0069382\n",
      "2_188 train_acc: 0.9288 train_loss: 0.177499\tval_acc: 1.000000 val_loss: 0.0067820\n",
      "2_192 train_acc: 0.9438 train_loss: 0.190063\tval_acc: 1.000000 val_loss: 0.0061298\n",
      "2_194 train_acc: 0.9139 train_loss: 0.230238\tval_acc: 1.000000 val_loss: 0.0053284\n",
      "2_198 train_acc: 0.9476 train_loss: 0.122425\tval_acc: 1.000000 val_loss: 0.0051301\n",
      "2_202 train_acc: 0.9438 train_loss: 0.140545\tval_acc: 1.000000 val_loss: 0.0039144\n",
      "2_214 train_acc: 0.9551 train_loss: 0.116017\tval_acc: 1.000000 val_loss: 0.0037672\n",
      "2_226 train_acc: 0.9513 train_loss: 0.158438\tval_acc: 1.000000 val_loss: 0.0028489\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2_228 train_acc: 0.9288 train_loss: 0.196045\tval_acc: 1.000000 val_loss: 0.0026569\n",
      "2_253 train_acc: 0.9476 train_loss: 0.128107\tval_acc: 1.000000 val_loss: 0.0024609\n",
      "2_257 train_acc: 0.9476 train_loss: 0.136660\tval_acc: 1.000000 val_loss: 0.0018530\n",
      "2_258 train_acc: 0.9288 train_loss: 0.152044\tval_acc: 1.000000 val_loss: 0.0014816\n",
      "2_267 train_acc: 0.9476 train_loss: 0.140395\tval_acc: 1.000000 val_loss: 0.0012483\n",
      "2_281 train_acc: 0.9775 train_loss: 0.071973\tval_acc: 1.000000 val_loss: 0.0009510\n",
      "2_283 train_acc: 0.9738 train_loss: 0.094998\tval_acc: 1.000000 val_loss: 0.0009349\n",
      "2_285 train_acc: 0.9888 train_loss: 0.062351\tval_acc: 1.000000 val_loss: 0.0005657\n",
      "2_348 train_acc: 0.9888 train_loss: 0.052947\tval_acc: 1.000000 val_loss: 0.0004125\n",
      "2_349 train_acc: 0.9588 train_loss: 0.102466\tval_acc: 1.000000 val_loss: 0.0003722\n",
      "2_369 train_acc: 0.9663 train_loss: 0.115746\tval_acc: 1.000000 val_loss: 0.0003409\n",
      "2_376 train_acc: 0.9625 train_loss: 0.094445\tval_acc: 1.000000 val_loss: 0.0003292\n",
      "2_379 train_acc: 0.9513 train_loss: 0.153051\tval_acc: 1.000000 val_loss: 0.0003106\n",
      "2_380 train_acc: 0.9438 train_loss: 0.110917\tval_acc: 1.000000 val_loss: 0.0002161\n",
      "2_432 train_acc: 0.9813 train_loss: 0.064699\tval_acc: 1.000000 val_loss: 0.0001486\n",
      "2_433 train_acc: 0.9775 train_loss: 0.071515\tval_acc: 1.000000 val_loss: 0.0001173\n",
      "2_453 train_acc: 0.9625 train_loss: 0.102012\tval_acc: 1.000000 val_loss: 0.0000523\n",
      "2_481 train_acc: 0.9813 train_loss: 0.054088\tval_acc: 1.000000 val_loss: 0.0000482\n",
      "2_494 train_acc: 0.9775 train_loss: 0.051197\tval_acc: 1.000000 val_loss: 0.0000316\n",
      "2_518 train_acc: 0.9813 train_loss: 0.057181\tval_acc: 1.000000 val_loss: 0.0000282\n",
      "2_540 train_acc: 0.9738 train_loss: 0.060753\tval_acc: 1.000000 val_loss: 0.0000242\n",
      "2_548 train_acc: 0.9775 train_loss: 0.069068\tval_acc: 1.000000 val_loss: 0.0000199\n",
      "2_564 train_acc: 0.9850 train_loss: 0.040015\tval_acc: 1.000000 val_loss: 0.0000166\n",
      "2_579 train_acc: 0.9850 train_loss: 0.038841\tval_acc: 1.000000 val_loss: 0.0000141\n",
      "2_636 train_acc: 0.9813 train_loss: 0.050292\tval_acc: 1.000000 val_loss: 0.0000073\n",
      "2_672 train_acc: 0.9850 train_loss: 0.031257\tval_acc: 1.000000 val_loss: 0.0000052\n",
      "2_751 train_acc: 1.0000 train_loss: 0.005479\tval_acc: 1.000000 val_loss: 0.0000029\n",
      "2_773 train_acc: 0.9963 train_loss: 0.018872\tval_acc: 1.000000 val_loss: 0.0000027\n",
      "2_850 train_acc: 0.9813 train_loss: 0.051997\tval_acc: 1.000000 val_loss: 0.0000027\n",
      "2_856 train_acc: 0.9963 train_loss: 0.015646\tval_acc: 1.000000 val_loss: 0.0000021\n",
      "2_871 train_acc: 0.9850 train_loss: 0.046673\tval_acc: 1.000000 val_loss: 0.0000012\n",
      "2_985 train_acc: 0.9963 train_loss: 0.017137\tval_acc: 1.000000 val_loss: 0.0000006\n",
      "epoch:  985 \tThe test accuracy is: 0.4895833333333333\n",
      " THE BEST ACCURACY IS 0.4895833333333333\tkappa is 0.3194444444444444\n",
      "subject 2 duration: 1:42:43.931505\n",
      "seed is 1080\n",
      "Subject 3\n",
      "-------------------- train size： (288, 1, 22, 1000) test size： (288, 22, 1000)\n",
      "3_0 train_acc: 0.2884 train_loss: 1.651179\tval_acc: 0.235294 val_loss: 1.6537699\n",
      "3_1 train_acc: 0.3184 train_loss: 1.522879\tval_acc: 0.308824 val_loss: 1.6067052\n",
      "3_3 train_acc: 0.4569 train_loss: 1.280603\tval_acc: 0.421569 val_loss: 1.2252921\n",
      "3_5 train_acc: 0.5955 train_loss: 1.028165\tval_acc: 0.563725 val_loss: 1.0774244\n",
      "3_8 train_acc: 0.7004 train_loss: 0.693351\tval_acc: 0.647059 val_loss: 1.0151776\n",
      "3_9 train_acc: 0.7453 train_loss: 0.620609\tval_acc: 0.681373 val_loss: 0.8806023\n",
      "3_10 train_acc: 0.7828 train_loss: 0.582180\tval_acc: 0.705882 val_loss: 0.8242627\n",
      "3_16 train_acc: 0.8427 train_loss: 0.353796\tval_acc: 0.754902 val_loss: 0.8086720\n",
      "3_17 train_acc: 0.8502 train_loss: 0.413878\tval_acc: 0.872549 val_loss: 0.3816538\n",
      "3_19 train_acc: 0.8951 train_loss: 0.262468\tval_acc: 0.877451 val_loss: 0.3125235\n",
      "3_20 train_acc: 0.8801 train_loss: 0.338001\tval_acc: 0.926471 val_loss: 0.2987868\n",
      "3_27 train_acc: 0.9326 train_loss: 0.200684\tval_acc: 0.906863 val_loss: 0.2952083\n",
      "3_28 train_acc: 0.8839 train_loss: 0.306158\tval_acc: 0.916667 val_loss: 0.2876928\n",
      "3_29 train_acc: 0.9176 train_loss: 0.211441\tval_acc: 0.921569 val_loss: 0.2379640\n",
      "3_30 train_acc: 0.9476 train_loss: 0.158031\tval_acc: 0.926471 val_loss: 0.2227840\n",
      "3_34 train_acc: 0.9476 train_loss: 0.140288\tval_acc: 0.941176 val_loss: 0.2007702\n",
      "3_37 train_acc: 0.9288 train_loss: 0.175352\tval_acc: 0.946078 val_loss: 0.1896500\n",
      "3_42 train_acc: 0.9363 train_loss: 0.177674\tval_acc: 0.970588 val_loss: 0.0892744\n",
      "3_44 train_acc: 0.9326 train_loss: 0.211680\tval_acc: 0.970588 val_loss: 0.0851062\n",
      "3_56 train_acc: 0.9326 train_loss: 0.163316\tval_acc: 0.970588 val_loss: 0.0822929\n",
      "3_65 train_acc: 0.9588 train_loss: 0.094363\tval_acc: 0.970588 val_loss: 0.0707331\n",
      "3_81 train_acc: 0.9625 train_loss: 0.106670\tval_acc: 0.980392 val_loss: 0.0677936\n",
      "3_82 train_acc: 0.9476 train_loss: 0.157388\tval_acc: 0.985294 val_loss: 0.0342606\n",
      "3_86 train_acc: 0.9663 train_loss: 0.099125\tval_acc: 0.985294 val_loss: 0.0269690\n",
      "3_99 train_acc: 0.9738 train_loss: 0.046875\tval_acc: 0.985294 val_loss: 0.0228275\n",
      "3_123 train_acc: 0.9888 train_loss: 0.068638\tval_acc: 0.995098 val_loss: 0.0116338\n",
      "3_143 train_acc: 0.9588 train_loss: 0.109238\tval_acc: 1.000000 val_loss: 0.0095896\n",
      "3_155 train_acc: 0.9775 train_loss: 0.077334\tval_acc: 1.000000 val_loss: 0.0044685\n",
      "3_156 train_acc: 0.9738 train_loss: 0.056271\tval_acc: 1.000000 val_loss: 0.0014388\n",
      "3_227 train_acc: 0.9888 train_loss: 0.027940\tval_acc: 1.000000 val_loss: 0.0012059\n",
      "3_241 train_acc: 1.0000 train_loss: 0.006532\tval_acc: 1.000000 val_loss: 0.0000735\n",
      "3_353 train_acc: 1.0000 train_loss: 0.002258\tval_acc: 1.000000 val_loss: 0.0000555\n",
      "3_415 train_acc: 0.9925 train_loss: 0.010449\tval_acc: 1.000000 val_loss: 0.0000122\n",
      "3_417 train_acc: 0.9925 train_loss: 0.046471\tval_acc: 1.000000 val_loss: 0.0000108\n",
      "3_677 train_acc: 1.0000 train_loss: 0.001306\tval_acc: 1.000000 val_loss: 0.0000039\n",
      "3_705 train_acc: 1.0000 train_loss: 0.002246\tval_acc: 1.000000 val_loss: 0.0000024\n",
      "3_737 train_acc: 0.9925 train_loss: 0.021396\tval_acc: 1.000000 val_loss: 0.0000007\n",
      "3_823 train_acc: 1.0000 train_loss: 0.005230\tval_acc: 1.000000 val_loss: 0.0000003\n",
      "3_869 train_acc: 1.0000 train_loss: 0.000991\tval_acc: 1.000000 val_loss: 0.0000002\n",
      "epoch:  869 \tThe test accuracy is: 0.9131944444444444\n",
      " THE BEST ACCURACY IS 0.9131944444444444\tkappa is 0.8842592592592593\n",
      "subject 3 duration: 1:42:01.579019\n",
      "seed is 56\n",
      "Subject 4\n",
      "-------------------- train size： (288, 1, 22, 1000) test size： (288, 22, 1000)\n",
      "4_0 train_acc: 0.3071 train_loss: 1.615760\tval_acc: 0.323529 val_loss: 1.3835495\n",
      "4_1 train_acc: 0.2996 train_loss: 1.517784\tval_acc: 0.382353 val_loss: 1.2522016\n",
      "4_2 train_acc: 0.3483 train_loss: 1.420937\tval_acc: 0.485294 val_loss: 1.1782597\n",
      "4_3 train_acc: 0.4007 train_loss: 1.423088\tval_acc: 0.514706 val_loss: 1.1074611\n",
      "4_4 train_acc: 0.3820 train_loss: 1.438489\tval_acc: 0.539216 val_loss: 1.0754156\n",
      "4_5 train_acc: 0.4494 train_loss: 1.252875\tval_acc: 0.598039 val_loss: 0.9881304\n",
      "4_8 train_acc: 0.5281 train_loss: 1.111728\tval_acc: 0.588235 val_loss: 0.9389601\n",
      "4_9 train_acc: 0.6255 train_loss: 1.022910\tval_acc: 0.656863 val_loss: 0.8499331\n",
      "4_10 train_acc: 0.5993 train_loss: 0.935772\tval_acc: 0.691176 val_loss: 0.7490018\n",
      "4_12 train_acc: 0.6404 train_loss: 0.950605\tval_acc: 0.686275 val_loss: 0.7128816\n",
      "4_16 train_acc: 0.6330 train_loss: 0.932996\tval_acc: 0.745098 val_loss: 0.6278237\n",
      "4_18 train_acc: 0.6854 train_loss: 0.811842\tval_acc: 0.774510 val_loss: 0.6157119\n",
      "4_20 train_acc: 0.7378 train_loss: 0.659653\tval_acc: 0.799020 val_loss: 0.5241506\n",
      "4_22 train_acc: 0.7079 train_loss: 0.743340\tval_acc: 0.813725 val_loss: 0.4983234\n",
      "4_25 train_acc: 0.7341 train_loss: 0.698325\tval_acc: 0.852941 val_loss: 0.4335606\n",
      "4_29 train_acc: 0.7828 train_loss: 0.543851\tval_acc: 0.892157 val_loss: 0.2822564\n",
      "4_40 train_acc: 0.8502 train_loss: 0.400082\tval_acc: 0.911765 val_loss: 0.2811640\n",
      "4_44 train_acc: 0.7865 train_loss: 0.570025\tval_acc: 0.926471 val_loss: 0.2272004\n",
      "4_48 train_acc: 0.8464 train_loss: 0.341941\tval_acc: 0.926471 val_loss: 0.2270937\n",
      "4_52 train_acc: 0.8577 train_loss: 0.402346\tval_acc: 0.941176 val_loss: 0.2056547\n",
      "4_56 train_acc: 0.8727 train_loss: 0.368814\tval_acc: 0.946078 val_loss: 0.1700027\n",
      "4_59 train_acc: 0.8839 train_loss: 0.294773\tval_acc: 0.955882 val_loss: 0.1375795\n",
      "4_63 train_acc: 0.8390 train_loss: 0.457542\tval_acc: 0.965686 val_loss: 0.1206788\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4_70 train_acc: 0.8951 train_loss: 0.264941\tval_acc: 0.965686 val_loss: 0.1146097\n",
      "4_72 train_acc: 0.8614 train_loss: 0.402047\tval_acc: 0.965686 val_loss: 0.1128757\n",
      "4_73 train_acc: 0.9026 train_loss: 0.259905\tval_acc: 0.980392 val_loss: 0.0949056\n",
      "4_74 train_acc: 0.9064 train_loss: 0.257923\tval_acc: 0.975490 val_loss: 0.0795194\n",
      "4_87 train_acc: 0.9101 train_loss: 0.233146\tval_acc: 0.975490 val_loss: 0.0676463\n",
      "4_90 train_acc: 0.8727 train_loss: 0.321016\tval_acc: 0.995098 val_loss: 0.0553836\n",
      "4_97 train_acc: 0.9326 train_loss: 0.183821\tval_acc: 0.995098 val_loss: 0.0415028\n",
      "4_102 train_acc: 0.8876 train_loss: 0.311548\tval_acc: 1.000000 val_loss: 0.0308961\n",
      "4_119 train_acc: 0.9251 train_loss: 0.214155\tval_acc: 0.995098 val_loss: 0.0250017\n",
      "4_126 train_acc: 0.9326 train_loss: 0.193490\tval_acc: 0.990196 val_loss: 0.0240490\n",
      "4_128 train_acc: 0.9700 train_loss: 0.114817\tval_acc: 1.000000 val_loss: 0.0119049\n",
      "4_143 train_acc: 0.9176 train_loss: 0.200697\tval_acc: 1.000000 val_loss: 0.0101807\n",
      "4_147 train_acc: 0.9064 train_loss: 0.240540\tval_acc: 1.000000 val_loss: 0.0062988\n",
      "4_185 train_acc: 0.9625 train_loss: 0.082810\tval_acc: 1.000000 val_loss: 0.0045883\n",
      "4_187 train_acc: 0.9700 train_loss: 0.090319\tval_acc: 1.000000 val_loss: 0.0034464\n",
      "4_190 train_acc: 0.9513 train_loss: 0.101598\tval_acc: 1.000000 val_loss: 0.0032206\n",
      "4_205 train_acc: 0.9738 train_loss: 0.066892\tval_acc: 1.000000 val_loss: 0.0025114\n",
      "4_212 train_acc: 0.9700 train_loss: 0.054515\tval_acc: 1.000000 val_loss: 0.0021995\n",
      "4_235 train_acc: 0.9700 train_loss: 0.079342\tval_acc: 1.000000 val_loss: 0.0017109\n",
      "4_257 train_acc: 0.9700 train_loss: 0.059015\tval_acc: 1.000000 val_loss: 0.0014946\n",
      "4_262 train_acc: 0.9700 train_loss: 0.086967\tval_acc: 1.000000 val_loss: 0.0012770\n",
      "4_263 train_acc: 0.9813 train_loss: 0.069758\tval_acc: 1.000000 val_loss: 0.0010361\n",
      "4_268 train_acc: 0.9850 train_loss: 0.062722\tval_acc: 1.000000 val_loss: 0.0009800\n",
      "4_269 train_acc: 0.9588 train_loss: 0.081308\tval_acc: 1.000000 val_loss: 0.0006588\n",
      "4_290 train_acc: 0.9588 train_loss: 0.101235\tval_acc: 1.000000 val_loss: 0.0006046\n",
      "4_291 train_acc: 0.9813 train_loss: 0.078679\tval_acc: 1.000000 val_loss: 0.0005960\n",
      "4_349 train_acc: 0.9738 train_loss: 0.100438\tval_acc: 1.000000 val_loss: 0.0003702\n",
      "4_357 train_acc: 0.9925 train_loss: 0.029147\tval_acc: 1.000000 val_loss: 0.0002955\n",
      "4_367 train_acc: 0.9738 train_loss: 0.073176\tval_acc: 1.000000 val_loss: 0.0002682\n",
      "4_402 train_acc: 0.9738 train_loss: 0.051825\tval_acc: 1.000000 val_loss: 0.0000918\n",
      "4_420 train_acc: 0.9925 train_loss: 0.018023\tval_acc: 1.000000 val_loss: 0.0000625\n",
      "4_579 train_acc: 0.9850 train_loss: 0.044176\tval_acc: 1.000000 val_loss: 0.0000465\n",
      "4_589 train_acc: 0.9925 train_loss: 0.044369\tval_acc: 1.000000 val_loss: 0.0000312\n",
      "4_592 train_acc: 0.9925 train_loss: 0.026486\tval_acc: 1.000000 val_loss: 0.0000134\n",
      "4_635 train_acc: 1.0000 train_loss: 0.012470\tval_acc: 1.000000 val_loss: 0.0000081\n",
      "4_857 train_acc: 0.9925 train_loss: 0.018665\tval_acc: 1.000000 val_loss: 0.0000053\n",
      "4_907 train_acc: 0.9925 train_loss: 0.032873\tval_acc: 1.000000 val_loss: 0.0000039\n",
      "4_915 train_acc: 0.9850 train_loss: 0.077760\tval_acc: 1.000000 val_loss: 0.0000014\n",
      "epoch:  915 \tThe test accuracy is: 0.7847222222222222\n",
      " THE BEST ACCURACY IS 0.7847222222222222\tkappa is 0.712962962962963\n",
      "subject 4 duration: 1:39:39.004428\n",
      "seed is 1037\n",
      "Subject 5\n",
      "-------------------- train size： (288, 1, 22, 1000) test size： (288, 22, 1000)\n",
      "5_0 train_acc: 0.2734 train_loss: 1.651492\tval_acc: 0.250000 val_loss: 1.4736793\n",
      "5_1 train_acc: 0.2846 train_loss: 1.592362\tval_acc: 0.426471 val_loss: 1.2679439\n",
      "5_2 train_acc: 0.3895 train_loss: 1.347910\tval_acc: 0.529412 val_loss: 1.1019559\n",
      "5_3 train_acc: 0.4757 train_loss: 1.183066\tval_acc: 0.598039 val_loss: 0.9654025\n",
      "5_4 train_acc: 0.5618 train_loss: 1.075028\tval_acc: 0.647059 val_loss: 0.7876505\n",
      "5_5 train_acc: 0.6479 train_loss: 0.858672\tval_acc: 0.745098 val_loss: 0.7058498\n",
      "5_7 train_acc: 0.6629 train_loss: 0.815449\tval_acc: 0.754902 val_loss: 0.6894705\n",
      "5_8 train_acc: 0.6929 train_loss: 0.744164\tval_acc: 0.769608 val_loss: 0.5870042\n",
      "5_9 train_acc: 0.7378 train_loss: 0.687368\tval_acc: 0.833333 val_loss: 0.4848314\n",
      "5_10 train_acc: 0.7715 train_loss: 0.569970\tval_acc: 0.857843 val_loss: 0.4811179\n",
      "5_11 train_acc: 0.7753 train_loss: 0.578331\tval_acc: 0.877451 val_loss: 0.4054126\n",
      "5_17 train_acc: 0.8277 train_loss: 0.448944\tval_acc: 0.916667 val_loss: 0.2982578\n",
      "5_18 train_acc: 0.8427 train_loss: 0.370080\tval_acc: 0.901961 val_loss: 0.2830009\n",
      "5_22 train_acc: 0.8427 train_loss: 0.420025\tval_acc: 0.911765 val_loss: 0.2603674\n",
      "5_23 train_acc: 0.8764 train_loss: 0.354061\tval_acc: 0.921569 val_loss: 0.2381526\n",
      "5_24 train_acc: 0.8352 train_loss: 0.440219\tval_acc: 0.916667 val_loss: 0.2109362\n",
      "5_26 train_acc: 0.8876 train_loss: 0.331955\tval_acc: 0.950980 val_loss: 0.2031223\n",
      "5_29 train_acc: 0.8577 train_loss: 0.423845\tval_acc: 0.926471 val_loss: 0.2004321\n",
      "5_38 train_acc: 0.9101 train_loss: 0.238071\tval_acc: 0.936275 val_loss: 0.1698691\n",
      "5_43 train_acc: 0.9139 train_loss: 0.240568\tval_acc: 0.960784 val_loss: 0.1179059\n",
      "5_47 train_acc: 0.9176 train_loss: 0.250714\tval_acc: 0.950980 val_loss: 0.1123602\n",
      "5_50 train_acc: 0.9026 train_loss: 0.228399\tval_acc: 0.970588 val_loss: 0.0937621\n",
      "5_56 train_acc: 0.9176 train_loss: 0.255583\tval_acc: 0.975490 val_loss: 0.0925699\n",
      "5_57 train_acc: 0.9251 train_loss: 0.250804\tval_acc: 0.965686 val_loss: 0.0859985\n",
      "5_58 train_acc: 0.8914 train_loss: 0.233407\tval_acc: 0.970588 val_loss: 0.0763143\n",
      "5_70 train_acc: 0.9663 train_loss: 0.111931\tval_acc: 0.970588 val_loss: 0.0744895\n",
      "5_71 train_acc: 0.9326 train_loss: 0.180945\tval_acc: 0.985294 val_loss: 0.0499890\n",
      "5_74 train_acc: 0.9251 train_loss: 0.168413\tval_acc: 0.995098 val_loss: 0.0489267\n",
      "5_79 train_acc: 0.9213 train_loss: 0.187625\tval_acc: 0.985294 val_loss: 0.0435012\n",
      "5_80 train_acc: 0.9176 train_loss: 0.248371\tval_acc: 1.000000 val_loss: 0.0285401\n",
      "5_100 train_acc: 0.9588 train_loss: 0.139798\tval_acc: 0.990196 val_loss: 0.0282658\n",
      "5_104 train_acc: 0.9513 train_loss: 0.143871\tval_acc: 1.000000 val_loss: 0.0232230\n",
      "5_111 train_acc: 0.9288 train_loss: 0.226744\tval_acc: 1.000000 val_loss: 0.0210192\n",
      "5_121 train_acc: 0.9438 train_loss: 0.129925\tval_acc: 0.995098 val_loss: 0.0197928\n",
      "5_122 train_acc: 0.9251 train_loss: 0.181658\tval_acc: 0.995098 val_loss: 0.0136362\n",
      "5_124 train_acc: 0.9363 train_loss: 0.189206\tval_acc: 1.000000 val_loss: 0.0125748\n",
      "5_130 train_acc: 0.9401 train_loss: 0.187717\tval_acc: 1.000000 val_loss: 0.0120085\n",
      "5_135 train_acc: 0.9513 train_loss: 0.116115\tval_acc: 1.000000 val_loss: 0.0116966\n",
      "5_137 train_acc: 0.9326 train_loss: 0.177738\tval_acc: 1.000000 val_loss: 0.0100251\n",
      "5_138 train_acc: 0.9588 train_loss: 0.106805\tval_acc: 1.000000 val_loss: 0.0067738\n",
      "5_158 train_acc: 0.9738 train_loss: 0.078927\tval_acc: 1.000000 val_loss: 0.0051960\n",
      "5_164 train_acc: 0.9738 train_loss: 0.078907\tval_acc: 1.000000 val_loss: 0.0050969\n",
      "5_168 train_acc: 0.9551 train_loss: 0.119535\tval_acc: 1.000000 val_loss: 0.0030310\n",
      "5_174 train_acc: 0.9476 train_loss: 0.119567\tval_acc: 1.000000 val_loss: 0.0028925\n",
      "5_194 train_acc: 0.9663 train_loss: 0.110985\tval_acc: 1.000000 val_loss: 0.0023504\n",
      "5_199 train_acc: 0.9588 train_loss: 0.161311\tval_acc: 1.000000 val_loss: 0.0020899\n",
      "5_220 train_acc: 0.9625 train_loss: 0.087914\tval_acc: 1.000000 val_loss: 0.0015653\n",
      "5_232 train_acc: 0.9775 train_loss: 0.056654\tval_acc: 1.000000 val_loss: 0.0015007\n",
      "5_243 train_acc: 0.9775 train_loss: 0.078136\tval_acc: 1.000000 val_loss: 0.0007812\n",
      "5_280 train_acc: 0.9888 train_loss: 0.035548\tval_acc: 1.000000 val_loss: 0.0005882\n",
      "5_305 train_acc: 0.9663 train_loss: 0.081644\tval_acc: 1.000000 val_loss: 0.0005430\n",
      "5_324 train_acc: 0.9813 train_loss: 0.047338\tval_acc: 1.000000 val_loss: 0.0004827\n",
      "5_327 train_acc: 0.9925 train_loss: 0.028638\tval_acc: 1.000000 val_loss: 0.0004227\n",
      "5_342 train_acc: 0.9738 train_loss: 0.054011\tval_acc: 1.000000 val_loss: 0.0003767\n",
      "5_346 train_acc: 0.9888 train_loss: 0.046156\tval_acc: 1.000000 val_loss: 0.0002555\n",
      "5_371 train_acc: 0.9888 train_loss: 0.039491\tval_acc: 1.000000 val_loss: 0.0002225\n",
      "5_372 train_acc: 0.9813 train_loss: 0.042072\tval_acc: 1.000000 val_loss: 0.0002141\n",
      "5_395 train_acc: 0.9738 train_loss: 0.046588\tval_acc: 1.000000 val_loss: 0.0001531\n",
      "5_471 train_acc: 0.9700 train_loss: 0.048837\tval_acc: 1.000000 val_loss: 0.0001423\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5_476 train_acc: 0.9850 train_loss: 0.043259\tval_acc: 1.000000 val_loss: 0.0001151\n",
      "5_481 train_acc: 0.9850 train_loss: 0.040852\tval_acc: 1.000000 val_loss: 0.0000982\n",
      "5_484 train_acc: 0.9850 train_loss: 0.058219\tval_acc: 1.000000 val_loss: 0.0000935\n",
      "5_496 train_acc: 0.9775 train_loss: 0.046338\tval_acc: 1.000000 val_loss: 0.0000799\n",
      "5_507 train_acc: 0.9738 train_loss: 0.063079\tval_acc: 1.000000 val_loss: 0.0000687\n",
      "5_522 train_acc: 0.9850 train_loss: 0.033498\tval_acc: 1.000000 val_loss: 0.0000298\n",
      "5_523 train_acc: 0.9775 train_loss: 0.050686\tval_acc: 1.000000 val_loss: 0.0000170\n",
      "5_649 train_acc: 0.9888 train_loss: 0.021600\tval_acc: 1.000000 val_loss: 0.0000083\n",
      "5_725 train_acc: 0.9850 train_loss: 0.057643\tval_acc: 1.000000 val_loss: 0.0000067\n",
      "5_737 train_acc: 0.9888 train_loss: 0.023736\tval_acc: 1.000000 val_loss: 0.0000065\n",
      "5_830 train_acc: 0.9888 train_loss: 0.025827\tval_acc: 1.000000 val_loss: 0.0000051\n",
      "5_835 train_acc: 0.9850 train_loss: 0.063178\tval_acc: 1.000000 val_loss: 0.0000050\n",
      "5_844 train_acc: 0.9963 train_loss: 0.008686\tval_acc: 1.000000 val_loss: 0.0000032\n",
      "5_867 train_acc: 1.0000 train_loss: 0.003039\tval_acc: 1.000000 val_loss: 0.0000020\n",
      "5_883 train_acc: 0.9850 train_loss: 0.041680\tval_acc: 1.000000 val_loss: 0.0000020\n",
      "5_887 train_acc: 0.9925 train_loss: 0.025727\tval_acc: 1.000000 val_loss: 0.0000018\n",
      "5_912 train_acc: 0.9925 train_loss: 0.012314\tval_acc: 1.000000 val_loss: 0.0000016\n",
      "5_914 train_acc: 0.9850 train_loss: 0.031542\tval_acc: 1.000000 val_loss: 0.0000015\n",
      "5_915 train_acc: 0.9850 train_loss: 0.039929\tval_acc: 1.000000 val_loss: 0.0000013\n",
      "5_958 train_acc: 0.9738 train_loss: 0.047307\tval_acc: 1.000000 val_loss: 0.0000007\n",
      "epoch:  958 \tThe test accuracy is: 0.75\n",
      " THE BEST ACCURACY IS 0.75\tkappa is 0.6666666666666667\n",
      "subject 5 duration: 1:34:41.857461\n",
      "seed is 1964\n",
      "Subject 6\n",
      "-------------------- train size： (288, 1, 22, 1000) test size： (288, 22, 1000)\n",
      "6_0 train_acc: 0.2322 train_loss: 1.632046\tval_acc: 0.338235 val_loss: 1.4160635\n",
      "6_3 train_acc: 0.4345 train_loss: 1.400536\tval_acc: 0.524510 val_loss: 1.1555172\n",
      "6_7 train_acc: 0.5730 train_loss: 1.059669\tval_acc: 0.647059 val_loss: 0.8694322\n",
      "6_11 train_acc: 0.6067 train_loss: 0.914332\tval_acc: 0.730392 val_loss: 0.7269475\n",
      "6_12 train_acc: 0.7116 train_loss: 0.770871\tval_acc: 0.750000 val_loss: 0.6498346\n",
      "6_15 train_acc: 0.7378 train_loss: 0.656181\tval_acc: 0.764706 val_loss: 0.6147310\n",
      "6_19 train_acc: 0.7041 train_loss: 0.759747\tval_acc: 0.799020 val_loss: 0.4976497\n",
      "6_22 train_acc: 0.8052 train_loss: 0.576158\tval_acc: 0.877451 val_loss: 0.3678153\n",
      "6_28 train_acc: 0.8127 train_loss: 0.493223\tval_acc: 0.857843 val_loss: 0.3470566\n",
      "6_32 train_acc: 0.8127 train_loss: 0.438802\tval_acc: 0.906863 val_loss: 0.2543619\n",
      "6_41 train_acc: 0.8240 train_loss: 0.473179\tval_acc: 0.911765 val_loss: 0.2402031\n",
      "6_44 train_acc: 0.8689 train_loss: 0.352525\tval_acc: 0.921569 val_loss: 0.2080877\n",
      "6_47 train_acc: 0.8614 train_loss: 0.313698\tval_acc: 0.950980 val_loss: 0.1759942\n",
      "6_50 train_acc: 0.8577 train_loss: 0.342994\tval_acc: 0.921569 val_loss: 0.1572946\n",
      "6_61 train_acc: 0.8352 train_loss: 0.346219\tval_acc: 0.941176 val_loss: 0.1175757\n",
      "6_70 train_acc: 0.8989 train_loss: 0.259193\tval_acc: 0.985294 val_loss: 0.0799381\n",
      "6_75 train_acc: 0.8914 train_loss: 0.292009\tval_acc: 0.985294 val_loss: 0.0641368\n",
      "6_87 train_acc: 0.8764 train_loss: 0.264645\tval_acc: 0.985294 val_loss: 0.0627586\n",
      "6_94 train_acc: 0.8951 train_loss: 0.281061\tval_acc: 0.990196 val_loss: 0.0560411\n",
      "6_97 train_acc: 0.9401 train_loss: 0.181743\tval_acc: 0.980392 val_loss: 0.0521987\n",
      "6_98 train_acc: 0.9064 train_loss: 0.269853\tval_acc: 1.000000 val_loss: 0.0297950\n",
      "6_119 train_acc: 0.9064 train_loss: 0.241212\tval_acc: 1.000000 val_loss: 0.0285679\n",
      "6_122 train_acc: 0.8989 train_loss: 0.285768\tval_acc: 1.000000 val_loss: 0.0280147\n",
      "6_123 train_acc: 0.8989 train_loss: 0.258893\tval_acc: 1.000000 val_loss: 0.0205733\n",
      "6_129 train_acc: 0.9176 train_loss: 0.209933\tval_acc: 1.000000 val_loss: 0.0204087\n",
      "6_138 train_acc: 0.9326 train_loss: 0.195536\tval_acc: 1.000000 val_loss: 0.0132177\n",
      "6_158 train_acc: 0.9326 train_loss: 0.175952\tval_acc: 1.000000 val_loss: 0.0125742\n",
      "6_163 train_acc: 0.9476 train_loss: 0.137295\tval_acc: 1.000000 val_loss: 0.0120465\n",
      "6_166 train_acc: 0.9288 train_loss: 0.213397\tval_acc: 1.000000 val_loss: 0.0100892\n",
      "6_187 train_acc: 0.9476 train_loss: 0.172782\tval_acc: 1.000000 val_loss: 0.0068126\n",
      "6_191 train_acc: 0.9588 train_loss: 0.130757\tval_acc: 1.000000 val_loss: 0.0027694\n",
      "6_216 train_acc: 0.9551 train_loss: 0.107345\tval_acc: 1.000000 val_loss: 0.0021376\n",
      "6_227 train_acc: 0.9625 train_loss: 0.084559\tval_acc: 1.000000 val_loss: 0.0016957\n",
      "6_256 train_acc: 0.9663 train_loss: 0.075428\tval_acc: 1.000000 val_loss: 0.0013953\n",
      "6_260 train_acc: 0.9775 train_loss: 0.076247\tval_acc: 1.000000 val_loss: 0.0012459\n",
      "6_275 train_acc: 0.9775 train_loss: 0.064082\tval_acc: 1.000000 val_loss: 0.0011499\n",
      "6_280 train_acc: 0.9888 train_loss: 0.055716\tval_acc: 1.000000 val_loss: 0.0008376\n",
      "6_283 train_acc: 0.9551 train_loss: 0.161778\tval_acc: 1.000000 val_loss: 0.0006371\n",
      "6_284 train_acc: 0.9588 train_loss: 0.121258\tval_acc: 1.000000 val_loss: 0.0006078\n",
      "6_298 train_acc: 0.9850 train_loss: 0.049490\tval_acc: 1.000000 val_loss: 0.0006046\n",
      "6_314 train_acc: 0.9813 train_loss: 0.081571\tval_acc: 1.000000 val_loss: 0.0004699\n",
      "6_316 train_acc: 0.9775 train_loss: 0.071518\tval_acc: 1.000000 val_loss: 0.0003279\n",
      "6_343 train_acc: 0.9663 train_loss: 0.088695\tval_acc: 1.000000 val_loss: 0.0002755\n",
      "6_386 train_acc: 0.9775 train_loss: 0.056836\tval_acc: 1.000000 val_loss: 0.0002527\n",
      "6_398 train_acc: 0.9775 train_loss: 0.064414\tval_acc: 1.000000 val_loss: 0.0001985\n",
      "6_405 train_acc: 0.9738 train_loss: 0.061808\tval_acc: 1.000000 val_loss: 0.0001337\n",
      "6_410 train_acc: 0.9738 train_loss: 0.073375\tval_acc: 1.000000 val_loss: 0.0001092\n",
      "6_457 train_acc: 0.9775 train_loss: 0.074526\tval_acc: 1.000000 val_loss: 0.0000565\n",
      "6_497 train_acc: 0.9850 train_loss: 0.030704\tval_acc: 1.000000 val_loss: 0.0000518\n",
      "6_519 train_acc: 0.9775 train_loss: 0.074524\tval_acc: 1.000000 val_loss: 0.0000313\n",
      "6_554 train_acc: 0.9963 train_loss: 0.020901\tval_acc: 1.000000 val_loss: 0.0000311\n",
      "6_586 train_acc: 0.9888 train_loss: 0.044252\tval_acc: 1.000000 val_loss: 0.0000224\n",
      "6_701 train_acc: 0.9850 train_loss: 0.041796\tval_acc: 1.000000 val_loss: 0.0000185\n",
      "6_707 train_acc: 0.9700 train_loss: 0.063152\tval_acc: 1.000000 val_loss: 0.0000181\n",
      "6_731 train_acc: 0.9888 train_loss: 0.046930\tval_acc: 1.000000 val_loss: 0.0000128\n",
      "6_743 train_acc: 0.9850 train_loss: 0.053041\tval_acc: 1.000000 val_loss: 0.0000110\n",
      "6_771 train_acc: 0.9925 train_loss: 0.011748\tval_acc: 1.000000 val_loss: 0.0000077\n",
      "6_789 train_acc: 0.9813 train_loss: 0.052815\tval_acc: 1.000000 val_loss: 0.0000064\n",
      "6_792 train_acc: 0.9963 train_loss: 0.013464\tval_acc: 1.000000 val_loss: 0.0000047\n",
      "6_797 train_acc: 1.0000 train_loss: 0.010629\tval_acc: 1.000000 val_loss: 0.0000046\n",
      "6_918 train_acc: 1.0000 train_loss: 0.008641\tval_acc: 1.000000 val_loss: 0.0000046\n",
      "6_954 train_acc: 0.9963 train_loss: 0.010748\tval_acc: 1.000000 val_loss: 0.0000022\n",
      "epoch:  954 \tThe test accuracy is: 0.6527777777777778\n",
      " THE BEST ACCURACY IS 0.6527777777777778\tkappa is 0.537037037037037\n",
      "subject 6 duration: 1:31:31.353746\n",
      "seed is 1389\n",
      "Subject 7\n",
      "-------------------- train size： (288, 1, 22, 1000) test size： (288, 22, 1000)\n",
      "7_0 train_acc: 0.2697 train_loss: 1.584915\tval_acc: 0.254902 val_loss: 1.4001306\n",
      "7_1 train_acc: 0.3521 train_loss: 1.425448\tval_acc: 0.534314 val_loss: 1.1077269\n",
      "7_2 train_acc: 0.5543 train_loss: 1.060689\tval_acc: 0.617647 val_loss: 0.9351745\n",
      "7_3 train_acc: 0.5955 train_loss: 0.973637\tval_acc: 0.725490 val_loss: 0.7459800\n",
      "7_4 train_acc: 0.6517 train_loss: 0.948232\tval_acc: 0.710784 val_loss: 0.7041662\n",
      "7_5 train_acc: 0.6816 train_loss: 0.825147\tval_acc: 0.735294 val_loss: 0.6398264\n",
      "7_6 train_acc: 0.7341 train_loss: 0.676982\tval_acc: 0.803922 val_loss: 0.5356752\n",
      "7_9 train_acc: 0.7566 train_loss: 0.608013\tval_acc: 0.877451 val_loss: 0.3194790\n",
      "7_18 train_acc: 0.8427 train_loss: 0.490760\tval_acc: 0.897059 val_loss: 0.3000523\n",
      "7_22 train_acc: 0.8876 train_loss: 0.300040\tval_acc: 0.911765 val_loss: 0.2583870\n",
      "7_25 train_acc: 0.8951 train_loss: 0.245697\tval_acc: 0.911765 val_loss: 0.2548347\n",
      "7_26 train_acc: 0.8240 train_loss: 0.566287\tval_acc: 0.911765 val_loss: 0.2385136\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7_30 train_acc: 0.9326 train_loss: 0.189407\tval_acc: 0.911765 val_loss: 0.2384445\n",
      "7_31 train_acc: 0.8727 train_loss: 0.313953\tval_acc: 0.926471 val_loss: 0.1927720\n",
      "7_38 train_acc: 0.9026 train_loss: 0.212736\tval_acc: 0.946078 val_loss: 0.1417108\n",
      "7_45 train_acc: 0.9513 train_loss: 0.138357\tval_acc: 0.955882 val_loss: 0.1171893\n",
      "7_54 train_acc: 0.9326 train_loss: 0.205837\tval_acc: 0.955882 val_loss: 0.1081715\n",
      "7_56 train_acc: 0.8839 train_loss: 0.267905\tval_acc: 0.980392 val_loss: 0.0583233\n",
      "7_65 train_acc: 0.9438 train_loss: 0.173838\tval_acc: 0.985294 val_loss: 0.0542993\n",
      "7_67 train_acc: 0.9438 train_loss: 0.169603\tval_acc: 0.975490 val_loss: 0.0512776\n",
      "7_69 train_acc: 0.9326 train_loss: 0.174287\tval_acc: 0.980392 val_loss: 0.0511000\n",
      "7_71 train_acc: 0.9438 train_loss: 0.169604\tval_acc: 0.985294 val_loss: 0.0335278\n",
      "7_86 train_acc: 0.9663 train_loss: 0.098888\tval_acc: 0.990196 val_loss: 0.0297412\n",
      "7_88 train_acc: 0.9326 train_loss: 0.176562\tval_acc: 0.995098 val_loss: 0.0245194\n",
      "7_89 train_acc: 0.9438 train_loss: 0.122840\tval_acc: 1.000000 val_loss: 0.0196953\n",
      "7_91 train_acc: 0.9551 train_loss: 0.107342\tval_acc: 0.995098 val_loss: 0.0171200\n",
      "7_98 train_acc: 0.9625 train_loss: 0.107710\tval_acc: 1.000000 val_loss: 0.0170882\n",
      "7_104 train_acc: 0.9663 train_loss: 0.088633\tval_acc: 1.000000 val_loss: 0.0121019\n",
      "7_106 train_acc: 0.9625 train_loss: 0.097235\tval_acc: 1.000000 val_loss: 0.0072380\n",
      "7_129 train_acc: 0.9588 train_loss: 0.090283\tval_acc: 1.000000 val_loss: 0.0062254\n",
      "7_136 train_acc: 0.9813 train_loss: 0.054246\tval_acc: 1.000000 val_loss: 0.0030397\n",
      "7_150 train_acc: 0.9813 train_loss: 0.075230\tval_acc: 1.000000 val_loss: 0.0016444\n",
      "7_157 train_acc: 0.9813 train_loss: 0.047010\tval_acc: 1.000000 val_loss: 0.0008681\n",
      "7_173 train_acc: 0.9813 train_loss: 0.062769\tval_acc: 1.000000 val_loss: 0.0007825\n",
      "7_256 train_acc: 0.9850 train_loss: 0.034435\tval_acc: 1.000000 val_loss: 0.0005476\n",
      "7_271 train_acc: 0.9888 train_loss: 0.047598\tval_acc: 1.000000 val_loss: 0.0002409\n",
      "7_279 train_acc: 0.9925 train_loss: 0.026173\tval_acc: 1.000000 val_loss: 0.0001353\n",
      "7_296 train_acc: 0.9925 train_loss: 0.017142\tval_acc: 1.000000 val_loss: 0.0001190\n",
      "7_298 train_acc: 0.9963 train_loss: 0.013943\tval_acc: 1.000000 val_loss: 0.0000992\n",
      "7_321 train_acc: 0.9888 train_loss: 0.040309\tval_acc: 1.000000 val_loss: 0.0000539\n",
      "7_334 train_acc: 0.9888 train_loss: 0.015810\tval_acc: 1.000000 val_loss: 0.0000408\n",
      "7_365 train_acc: 0.9850 train_loss: 0.036237\tval_acc: 1.000000 val_loss: 0.0000290\n",
      "7_396 train_acc: 0.9963 train_loss: 0.014293\tval_acc: 1.000000 val_loss: 0.0000116\n",
      "7_424 train_acc: 0.9963 train_loss: 0.004594\tval_acc: 1.000000 val_loss: 0.0000073\n",
      "7_443 train_acc: 0.9963 train_loss: 0.007362\tval_acc: 1.000000 val_loss: 0.0000073\n",
      "7_502 train_acc: 0.9963 train_loss: 0.023611\tval_acc: 1.000000 val_loss: 0.0000037\n",
      "7_521 train_acc: 0.9963 train_loss: 0.004092\tval_acc: 1.000000 val_loss: 0.0000024\n",
      "7_563 train_acc: 0.9925 train_loss: 0.013139\tval_acc: 1.000000 val_loss: 0.0000024\n",
      "7_565 train_acc: 0.9850 train_loss: 0.032583\tval_acc: 1.000000 val_loss: 0.0000012\n",
      "7_623 train_acc: 0.9963 train_loss: 0.006429\tval_acc: 1.000000 val_loss: 0.0000001\n",
      "7_903 train_acc: 1.0000 train_loss: 0.000126\tval_acc: 1.000000 val_loss: 0.0000001\n",
      "epoch:  903 \tThe test accuracy is: 0.8784722222222222\n",
      " THE BEST ACCURACY IS 0.8784722222222222\tkappa is 0.837962962962963\n",
      "subject 7 duration: 1:31:16.611691\n",
      "seed is 1264\n",
      "Subject 8\n",
      "-------------------- train size： (288, 1, 22, 1000) test size： (288, 22, 1000)\n",
      "8_0 train_acc: 0.2547 train_loss: 1.619793\tval_acc: 0.313725 val_loss: 1.4684159\n",
      "8_1 train_acc: 0.3708 train_loss: 1.417911\tval_acc: 0.455882 val_loss: 1.1874164\n",
      "8_4 train_acc: 0.4831 train_loss: 1.205183\tval_acc: 0.622549 val_loss: 1.0146054\n",
      "8_5 train_acc: 0.5318 train_loss: 1.081514\tval_acc: 0.666667 val_loss: 0.8351977\n",
      "8_6 train_acc: 0.6292 train_loss: 0.886697\tval_acc: 0.715686 val_loss: 0.6629404\n",
      "8_12 train_acc: 0.7678 train_loss: 0.580231\tval_acc: 0.779412 val_loss: 0.6595117\n",
      "8_13 train_acc: 0.7378 train_loss: 0.675191\tval_acc: 0.779412 val_loss: 0.6381119\n",
      "8_15 train_acc: 0.7903 train_loss: 0.516089\tval_acc: 0.794118 val_loss: 0.6277689\n",
      "8_16 train_acc: 0.7978 train_loss: 0.563598\tval_acc: 0.799020 val_loss: 0.5602533\n",
      "8_18 train_acc: 0.8352 train_loss: 0.454553\tval_acc: 0.862745 val_loss: 0.5024732\n",
      "8_20 train_acc: 0.8240 train_loss: 0.420830\tval_acc: 0.838235 val_loss: 0.4778617\n",
      "8_22 train_acc: 0.8801 train_loss: 0.336360\tval_acc: 0.916667 val_loss: 0.2134879\n",
      "8_29 train_acc: 0.9064 train_loss: 0.286539\tval_acc: 0.921569 val_loss: 0.1894192\n",
      "8_30 train_acc: 0.9251 train_loss: 0.231996\tval_acc: 0.931373 val_loss: 0.1852131\n",
      "8_37 train_acc: 0.9363 train_loss: 0.222957\tval_acc: 0.931373 val_loss: 0.1560865\n",
      "8_39 train_acc: 0.9401 train_loss: 0.169329\tval_acc: 0.980392 val_loss: 0.0791560\n",
      "8_60 train_acc: 0.9700 train_loss: 0.067921\tval_acc: 0.985294 val_loss: 0.0348173\n",
      "8_72 train_acc: 0.9738 train_loss: 0.094007\tval_acc: 0.995098 val_loss: 0.0276580\n",
      "8_86 train_acc: 0.9775 train_loss: 0.049069\tval_acc: 0.990196 val_loss: 0.0193759\n",
      "8_113 train_acc: 0.9551 train_loss: 0.150765\tval_acc: 0.990196 val_loss: 0.0172801\n",
      "8_114 train_acc: 0.9700 train_loss: 0.071363\tval_acc: 0.995098 val_loss: 0.0142972\n",
      "8_124 train_acc: 0.9925 train_loss: 0.017786\tval_acc: 0.995098 val_loss: 0.0080239\n",
      "8_158 train_acc: 0.9663 train_loss: 0.104946\tval_acc: 1.000000 val_loss: 0.0062648\n",
      "8_160 train_acc: 0.9775 train_loss: 0.083770\tval_acc: 1.000000 val_loss: 0.0046474\n",
      "8_168 train_acc: 0.9888 train_loss: 0.026646\tval_acc: 1.000000 val_loss: 0.0008146\n",
      "8_290 train_acc: 0.9888 train_loss: 0.047878\tval_acc: 1.000000 val_loss: 0.0004402\n",
      "8_303 train_acc: 0.9775 train_loss: 0.070267\tval_acc: 1.000000 val_loss: 0.0003814\n",
      "8_309 train_acc: 0.9850 train_loss: 0.052731\tval_acc: 1.000000 val_loss: 0.0002992\n",
      "8_345 train_acc: 0.9925 train_loss: 0.020853\tval_acc: 1.000000 val_loss: 0.0000536\n",
      "8_395 train_acc: 0.9963 train_loss: 0.007545\tval_acc: 1.000000 val_loss: 0.0000382\n",
      "8_500 train_acc: 0.9963 train_loss: 0.023864\tval_acc: 1.000000 val_loss: 0.0000082\n",
      "8_528 train_acc: 0.9888 train_loss: 0.075225\tval_acc: 1.000000 val_loss: 0.0000071\n",
      "8_556 train_acc: 0.9963 train_loss: 0.018790\tval_acc: 1.000000 val_loss: 0.0000046\n",
      "8_733 train_acc: 0.9963 train_loss: 0.004219\tval_acc: 1.000000 val_loss: 0.0000036\n",
      "8_794 train_acc: 0.9888 train_loss: 0.024713\tval_acc: 1.000000 val_loss: 0.0000034\n",
      "8_825 train_acc: 0.9925 train_loss: 0.006878\tval_acc: 1.000000 val_loss: 0.0000012\n",
      "8_833 train_acc: 0.9813 train_loss: 0.047592\tval_acc: 1.000000 val_loss: 0.0000009\n",
      "8_900 train_acc: 1.0000 train_loss: 0.003938\tval_acc: 1.000000 val_loss: 0.0000009\n",
      "epoch:  900 \tThe test accuracy is: 0.8715277777777778\n",
      " THE BEST ACCURACY IS 0.8715277777777778\tkappa is 0.8287037037037037\n",
      "subject 8 duration: 1:29:52.112899\n",
      "seed is 1382\n",
      "Subject 9\n",
      "-------------------- train size： (288, 1, 22, 1000) test size： (288, 22, 1000)\n",
      "9_0 train_acc: 0.2734 train_loss: 1.600402\tval_acc: 0.318627 val_loss: 1.3652297\n",
      "9_2 train_acc: 0.3970 train_loss: 1.306029\tval_acc: 0.480392 val_loss: 1.2294433\n",
      "9_4 train_acc: 0.5693 train_loss: 1.033936\tval_acc: 0.617647 val_loss: 0.9793842\n",
      "9_5 train_acc: 0.5843 train_loss: 1.034843\tval_acc: 0.705882 val_loss: 0.7514609\n",
      "9_7 train_acc: 0.6142 train_loss: 0.978914\tval_acc: 0.705882 val_loss: 0.7493306\n",
      "9_8 train_acc: 0.7154 train_loss: 0.770119\tval_acc: 0.769608 val_loss: 0.6474898\n",
      "9_12 train_acc: 0.8652 train_loss: 0.470594\tval_acc: 0.808824 val_loss: 0.5233395\n",
      "9_15 train_acc: 0.8015 train_loss: 0.457152\tval_acc: 0.823529 val_loss: 0.3912879\n",
      "9_19 train_acc: 0.8539 train_loss: 0.367715\tval_acc: 0.867647 val_loss: 0.3368090\n",
      "9_20 train_acc: 0.9026 train_loss: 0.362944\tval_acc: 0.955882 val_loss: 0.1679484\n",
      "9_29 train_acc: 0.8652 train_loss: 0.269491\tval_acc: 0.936275 val_loss: 0.1371378\n",
      "9_32 train_acc: 0.9213 train_loss: 0.186285\tval_acc: 0.955882 val_loss: 0.1147014\n",
      "9_33 train_acc: 0.8727 train_loss: 0.316273\tval_acc: 0.975490 val_loss: 0.0739253\n",
      "9_40 train_acc: 0.9139 train_loss: 0.249528\tval_acc: 0.980392 val_loss: 0.0632112\n",
      "9_43 train_acc: 0.9026 train_loss: 0.244593\tval_acc: 0.985294 val_loss: 0.0573731\n",
      "9_44 train_acc: 0.9101 train_loss: 0.222569\tval_acc: 0.985294 val_loss: 0.0362506\n",
      "9_49 train_acc: 0.9363 train_loss: 0.175001\tval_acc: 0.990196 val_loss: 0.0294353\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9_54 train_acc: 0.9738 train_loss: 0.071242\tval_acc: 0.995098 val_loss: 0.0213025\n",
      "9_55 train_acc: 0.9363 train_loss: 0.170063\tval_acc: 0.995098 val_loss: 0.0166856\n",
      "9_69 train_acc: 0.9551 train_loss: 0.088410\tval_acc: 0.995098 val_loss: 0.0155613\n",
      "9_70 train_acc: 0.9551 train_loss: 0.120158\tval_acc: 0.995098 val_loss: 0.0088509\n",
      "9_71 train_acc: 0.9401 train_loss: 0.134399\tval_acc: 1.000000 val_loss: 0.0068639\n",
      "9_78 train_acc: 0.9775 train_loss: 0.068026\tval_acc: 1.000000 val_loss: 0.0061781\n",
      "9_79 train_acc: 0.9888 train_loss: 0.047694\tval_acc: 1.000000 val_loss: 0.0052984\n",
      "9_88 train_acc: 0.9625 train_loss: 0.093630\tval_acc: 1.000000 val_loss: 0.0049481\n",
      "9_94 train_acc: 0.9888 train_loss: 0.036819\tval_acc: 1.000000 val_loss: 0.0035867\n",
      "9_107 train_acc: 0.9775 train_loss: 0.054968\tval_acc: 1.000000 val_loss: 0.0022312\n",
      "9_114 train_acc: 0.9813 train_loss: 0.047123\tval_acc: 1.000000 val_loss: 0.0011052\n",
      "9_116 train_acc: 0.9700 train_loss: 0.086519\tval_acc: 1.000000 val_loss: 0.0006453\n",
      "9_148 train_acc: 0.9738 train_loss: 0.063428\tval_acc: 1.000000 val_loss: 0.0005571\n",
      "9_159 train_acc: 0.9813 train_loss: 0.058411\tval_acc: 1.000000 val_loss: 0.0004156\n",
      "9_161 train_acc: 0.9850 train_loss: 0.035706\tval_acc: 1.000000 val_loss: 0.0002177\n",
      "9_165 train_acc: 0.9738 train_loss: 0.070169\tval_acc: 1.000000 val_loss: 0.0001667\n",
      "9_185 train_acc: 0.9813 train_loss: 0.042669\tval_acc: 1.000000 val_loss: 0.0000629\n",
      "9_252 train_acc: 0.9963 train_loss: 0.007481\tval_acc: 1.000000 val_loss: 0.0000315\n",
      "9_263 train_acc: 0.9700 train_loss: 0.067483\tval_acc: 1.000000 val_loss: 0.0000110\n",
      "9_287 train_acc: 0.9738 train_loss: 0.050684\tval_acc: 1.000000 val_loss: 0.0000051\n",
      "9_315 train_acc: 1.0000 train_loss: 0.006536\tval_acc: 1.000000 val_loss: 0.0000046\n",
      "9_316 train_acc: 1.0000 train_loss: 0.005176\tval_acc: 1.000000 val_loss: 0.0000034\n",
      "9_403 train_acc: 0.9925 train_loss: 0.021438\tval_acc: 1.000000 val_loss: 0.0000029\n",
      "9_404 train_acc: 0.9963 train_loss: 0.015170\tval_acc: 1.000000 val_loss: 0.0000018\n",
      "9_406 train_acc: 0.9963 train_loss: 0.004522\tval_acc: 1.000000 val_loss: 0.0000007\n",
      "9_525 train_acc: 0.9963 train_loss: 0.005202\tval_acc: 1.000000 val_loss: 0.0000002\n",
      "9_553 train_acc: 0.9925 train_loss: 0.042995\tval_acc: 1.000000 val_loss: 0.0000002\n",
      "9_603 train_acc: 0.9925 train_loss: 0.028049\tval_acc: 1.000000 val_loss: 0.0000001\n",
      "9_852 train_acc: 1.0000 train_loss: 0.000709\tval_acc: 1.000000 val_loss: 0.0000001\n",
      "9_971 train_acc: 1.0000 train_loss: 0.000315\tval_acc: 1.000000 val_loss: 0.0000000\n",
      "epoch:  971 \tThe test accuracy is: 0.7986111111111112\n",
      " THE BEST ACCURACY IS 0.7986111111111112\tkappa is 0.7314814814814814\n",
      "subject 9 duration: 1:32:02.954793\n",
      "**The average Best accuracy is: 77.66203703703702kappa is: 70.21604938271605\n",
      "\n",
      "best epochs:  [923, 985, 869, 915, 958, 954, 903, 900, 971]\n",
      "---------  all result  ---------\n",
      "        accuray  precision     recall         f1      kappa\n",
      "0     85.069444  85.098815  85.069444  85.034520  80.092593\n",
      "1     48.958333  57.841005  48.958333  48.466644  31.944444\n",
      "2     91.319444  91.755912  91.319444  91.283626  88.425926\n",
      "3     78.472222  79.010232  78.472222  78.378825  71.296296\n",
      "4     75.000000  74.625941  75.000000  74.556313  66.666667\n",
      "5     65.277778  67.292779  65.277778  65.469864  53.703704\n",
      "6     87.847222  89.010912  87.847222  87.796128  83.796296\n",
      "7     87.152778  87.944449  87.152778  87.302604  82.870370\n",
      "8     79.861111  80.983997  79.861111  79.626796  73.148148\n",
      "mean  77.662037  79.284894  77.662037  77.546147  70.216049\n",
      "std   13.346619  11.108728  13.346619  13.465049  17.795492\n",
      "****************************************\n",
      "Wed May  1 00:06:58 2024\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 40, 3, 976]           1,040\n",
      "            Conv2d-2           [-1, 40, 1, 976]           4,840\n",
      "       BatchNorm2d-3           [-1, 40, 1, 976]              80\n",
      "               ELU-4           [-1, 40, 1, 976]               0\n",
      "         AvgPool2d-5            [-1, 40, 1, 61]               0\n",
      "           Dropout-6            [-1, 40, 1, 61]               0\n",
      "            Conv2d-7            [-1, 40, 1, 61]           1,640\n",
      "         Rearrange-8               [-1, 61, 40]               0\n",
      "         LayerNorm-9               [-1, 61, 40]              80\n",
      "           Linear-10               [-1, 61, 40]           1,640\n",
      "           Linear-11               [-1, 61, 40]           1,640\n",
      "           Linear-12               [-1, 61, 40]           1,640\n",
      "          Dropout-13           [-1, 10, 61, 61]               0\n",
      "           Linear-14               [-1, 61, 40]           1,640\n",
      "MultiHeadAttention-15               [-1, 61, 40]               0\n",
      "          Dropout-16               [-1, 61, 40]               0\n",
      "      ResidualAdd-17               [-1, 61, 40]               0\n",
      "        LayerNorm-18               [-1, 61, 40]              80\n",
      "           Linear-19              [-1, 61, 160]           6,560\n",
      "             GELU-20              [-1, 61, 160]               0\n",
      "          Dropout-21              [-1, 61, 160]               0\n",
      "           Linear-22               [-1, 61, 40]           6,440\n",
      "          Dropout-23               [-1, 61, 40]               0\n",
      "      ResidualAdd-24               [-1, 61, 40]               0\n",
      "        LayerNorm-25               [-1, 61, 40]              80\n",
      "           Linear-26               [-1, 61, 40]           1,640\n",
      "           Linear-27               [-1, 61, 40]           1,640\n",
      "           Linear-28               [-1, 61, 40]           1,640\n",
      "          Dropout-29           [-1, 10, 61, 61]               0\n",
      "           Linear-30               [-1, 61, 40]           1,640\n",
      "MultiHeadAttention-31               [-1, 61, 40]               0\n",
      "          Dropout-32               [-1, 61, 40]               0\n",
      "      ResidualAdd-33               [-1, 61, 40]               0\n",
      "        LayerNorm-34               [-1, 61, 40]              80\n",
      "           Linear-35              [-1, 61, 160]           6,560\n",
      "             GELU-36              [-1, 61, 160]               0\n",
      "          Dropout-37              [-1, 61, 160]               0\n",
      "           Linear-38               [-1, 61, 40]           6,440\n",
      "          Dropout-39               [-1, 61, 40]               0\n",
      "      ResidualAdd-40               [-1, 61, 40]               0\n",
      "        LayerNorm-41               [-1, 61, 40]              80\n",
      "           Linear-42               [-1, 61, 40]           1,640\n",
      "           Linear-43               [-1, 61, 40]           1,640\n",
      "           Linear-44               [-1, 61, 40]           1,640\n",
      "          Dropout-45           [-1, 10, 61, 61]               0\n",
      "           Linear-46               [-1, 61, 40]           1,640\n",
      "MultiHeadAttention-47               [-1, 61, 40]               0\n",
      "          Dropout-48               [-1, 61, 40]               0\n",
      "      ResidualAdd-49               [-1, 61, 40]               0\n",
      "        LayerNorm-50               [-1, 61, 40]              80\n",
      "           Linear-51              [-1, 61, 160]           6,560\n",
      "             GELU-52              [-1, 61, 160]               0\n",
      "          Dropout-53              [-1, 61, 160]               0\n",
      "           Linear-54               [-1, 61, 40]           6,440\n",
      "          Dropout-55               [-1, 61, 40]               0\n",
      "      ResidualAdd-56               [-1, 61, 40]               0\n",
      "        LayerNorm-57               [-1, 61, 40]              80\n",
      "           Linear-58               [-1, 61, 40]           1,640\n",
      "           Linear-59               [-1, 61, 40]           1,640\n",
      "           Linear-60               [-1, 61, 40]           1,640\n",
      "          Dropout-61           [-1, 10, 61, 61]               0\n",
      "           Linear-62               [-1, 61, 40]           1,640\n",
      "MultiHeadAttention-63               [-1, 61, 40]               0\n",
      "          Dropout-64               [-1, 61, 40]               0\n",
      "      ResidualAdd-65               [-1, 61, 40]               0\n",
      "        LayerNorm-66               [-1, 61, 40]              80\n",
      "           Linear-67              [-1, 61, 160]           6,560\n",
      "             GELU-68              [-1, 61, 160]               0\n",
      "          Dropout-69              [-1, 61, 160]               0\n",
      "           Linear-70               [-1, 61, 40]           6,440\n",
      "          Dropout-71               [-1, 61, 40]               0\n",
      "      ResidualAdd-72               [-1, 61, 40]               0\n",
      "        LayerNorm-73               [-1, 61, 40]              80\n",
      "           Linear-74               [-1, 61, 40]           1,640\n",
      "           Linear-75               [-1, 61, 40]           1,640\n",
      "           Linear-76               [-1, 61, 40]           1,640\n",
      "          Dropout-77           [-1, 10, 61, 61]               0\n",
      "           Linear-78               [-1, 61, 40]           1,640\n",
      "MultiHeadAttention-79               [-1, 61, 40]               0\n",
      "          Dropout-80               [-1, 61, 40]               0\n",
      "      ResidualAdd-81               [-1, 61, 40]               0\n",
      "        LayerNorm-82               [-1, 61, 40]              80\n",
      "           Linear-83              [-1, 61, 160]           6,560\n",
      "             GELU-84              [-1, 61, 160]               0\n",
      "          Dropout-85              [-1, 61, 160]               0\n",
      "           Linear-86               [-1, 61, 40]           6,440\n",
      "          Dropout-87               [-1, 61, 40]               0\n",
      "      ResidualAdd-88               [-1, 61, 40]               0\n",
      "        LayerNorm-89               [-1, 61, 40]              80\n",
      "           Linear-90               [-1, 61, 40]           1,640\n",
      "           Linear-91               [-1, 61, 40]           1,640\n",
      "           Linear-92               [-1, 61, 40]           1,640\n",
      "          Dropout-93           [-1, 10, 61, 61]               0\n",
      "           Linear-94               [-1, 61, 40]           1,640\n",
      "MultiHeadAttention-95               [-1, 61, 40]               0\n",
      "          Dropout-96               [-1, 61, 40]               0\n",
      "      ResidualAdd-97               [-1, 61, 40]               0\n",
      "        LayerNorm-98               [-1, 61, 40]              80\n",
      "           Linear-99              [-1, 61, 160]           6,560\n",
      "            GELU-100              [-1, 61, 160]               0\n",
      "         Dropout-101              [-1, 61, 160]               0\n",
      "          Linear-102               [-1, 61, 40]           6,440\n",
      "         Dropout-103               [-1, 61, 40]               0\n",
      "     ResidualAdd-104               [-1, 61, 40]               0\n",
      "         Flatten-105                 [-1, 2440]               0\n",
      "         Dropout-106                 [-1, 2440]               0\n",
      "          Linear-107                    [-1, 2]           4,882\n",
      "       Conformer-108                    [-1, 2]               0\n",
      "================================================================\n",
      "Total params: 130,802\n",
      "Trainable params: 130,802\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 6.28\n",
      "Params size (MB): 0.50\n",
      "Estimated Total Size (MB): 6.79\n",
      "----------------------------------------------------------------\n",
      "Wed May  1 00:06:59 2024\n",
      "seed is 1168\n",
      "Subject 1\n",
      "-------------------- train size： (400, 1, 3, 1000) test size： (320, 3, 1000)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1_0 train_acc: 0.5738 train_loss: 0.787573\tval_acc: 0.522968 val_loss: 0.6818902\n",
      "1_1 train_acc: 0.5738 train_loss: 0.740669\tval_acc: 0.639576 val_loss: 0.6516152\n",
      "1_2 train_acc: 0.5779 train_loss: 0.741151\tval_acc: 0.671378 val_loss: 0.6066140\n",
      "1_3 train_acc: 0.6311 train_loss: 0.637833\tval_acc: 0.696113 val_loss: 0.6026442\n",
      "1_4 train_acc: 0.6475 train_loss: 0.691990\tval_acc: 0.717314 val_loss: 0.5871862\n",
      "1_6 train_acc: 0.6721 train_loss: 0.633349\tval_acc: 0.756184 val_loss: 0.5088624\n",
      "1_10 train_acc: 0.7131 train_loss: 0.566417\tval_acc: 0.756184 val_loss: 0.5055631\n",
      "1_11 train_acc: 0.7500 train_loss: 0.484529\tval_acc: 0.809187 val_loss: 0.3832566\n",
      "1_20 train_acc: 0.7418 train_loss: 0.563054\tval_acc: 0.851590 val_loss: 0.3593350\n",
      "1_22 train_acc: 0.7869 train_loss: 0.467433\tval_acc: 0.840989 val_loss: 0.3584891\n",
      "1_24 train_acc: 0.8402 train_loss: 0.335536\tval_acc: 0.869258 val_loss: 0.2845435\n",
      "1_28 train_acc: 0.8566 train_loss: 0.336474\tval_acc: 0.883392 val_loss: 0.2658859\n",
      "1_30 train_acc: 0.8770 train_loss: 0.340233\tval_acc: 0.897527 val_loss: 0.2569401\n",
      "1_33 train_acc: 0.8770 train_loss: 0.263659\tval_acc: 0.918728 val_loss: 0.2309899\n",
      "1_34 train_acc: 0.9098 train_loss: 0.229015\tval_acc: 0.901060 val_loss: 0.2103079\n",
      "1_41 train_acc: 0.9221 train_loss: 0.207777\tval_acc: 0.932862 val_loss: 0.2039619\n",
      "1_42 train_acc: 0.8811 train_loss: 0.286669\tval_acc: 0.918728 val_loss: 0.1754661\n",
      "1_45 train_acc: 0.9098 train_loss: 0.246297\tval_acc: 0.929329 val_loss: 0.1611169\n",
      "1_63 train_acc: 0.9139 train_loss: 0.204602\tval_acc: 0.950530 val_loss: 0.1585398\n",
      "1_64 train_acc: 0.9262 train_loss: 0.160509\tval_acc: 0.939929 val_loss: 0.1374788\n",
      "1_65 train_acc: 0.9221 train_loss: 0.230907\tval_acc: 0.946996 val_loss: 0.1247141\n",
      "1_86 train_acc: 0.9057 train_loss: 0.266896\tval_acc: 0.964664 val_loss: 0.1195994\n",
      "1_87 train_acc: 0.9262 train_loss: 0.175603\tval_acc: 0.943463 val_loss: 0.1110295\n",
      "1_88 train_acc: 0.9180 train_loss: 0.220432\tval_acc: 0.961131 val_loss: 0.0893805\n",
      "1_104 train_acc: 0.8975 train_loss: 0.234140\tval_acc: 0.975265 val_loss: 0.0811313\n",
      "1_115 train_acc: 0.9180 train_loss: 0.205091\tval_acc: 0.971731 val_loss: 0.0639789\n",
      "1_145 train_acc: 0.9426 train_loss: 0.131498\tval_acc: 0.978799 val_loss: 0.0616820\n",
      "1_148 train_acc: 0.9344 train_loss: 0.166578\tval_acc: 0.982332 val_loss: 0.0575907\n",
      "1_154 train_acc: 0.9303 train_loss: 0.191317\tval_acc: 0.982332 val_loss: 0.0446921\n",
      "1_161 train_acc: 0.9344 train_loss: 0.173920\tval_acc: 0.989399 val_loss: 0.0429027\n",
      "1_175 train_acc: 0.9385 train_loss: 0.153912\tval_acc: 0.985866 val_loss: 0.0403671\n",
      "1_200 train_acc: 0.9672 train_loss: 0.090764\tval_acc: 0.985866 val_loss: 0.0391870\n",
      "1_214 train_acc: 0.9713 train_loss: 0.069887\tval_acc: 0.985866 val_loss: 0.0336558\n",
      "1_243 train_acc: 0.9467 train_loss: 0.136327\tval_acc: 0.992933 val_loss: 0.0335403\n",
      "1_244 train_acc: 0.9672 train_loss: 0.079688\tval_acc: 0.989399 val_loss: 0.0296433\n",
      "1_251 train_acc: 0.9344 train_loss: 0.159682\tval_acc: 1.000000 val_loss: 0.0179226\n",
      "1_297 train_acc: 0.9549 train_loss: 0.142854\tval_acc: 1.000000 val_loss: 0.0178262\n",
      "1_339 train_acc: 0.9795 train_loss: 0.075019\tval_acc: 0.996466 val_loss: 0.0172837\n",
      "1_341 train_acc: 0.9672 train_loss: 0.134587\tval_acc: 0.996466 val_loss: 0.0129673\n",
      "1_350 train_acc: 0.9672 train_loss: 0.108893\tval_acc: 1.000000 val_loss: 0.0128998\n",
      "1_369 train_acc: 0.9426 train_loss: 0.132266\tval_acc: 1.000000 val_loss: 0.0092066\n",
      "1_390 train_acc: 0.9631 train_loss: 0.082314\tval_acc: 0.996466 val_loss: 0.0088869\n",
      "1_446 train_acc: 0.9549 train_loss: 0.108172\tval_acc: 1.000000 val_loss: 0.0076935\n",
      "1_457 train_acc: 0.9713 train_loss: 0.075965\tval_acc: 1.000000 val_loss: 0.0042182\n",
      "1_470 train_acc: 0.9590 train_loss: 0.149079\tval_acc: 1.000000 val_loss: 0.0040072\n",
      "1_564 train_acc: 0.9713 train_loss: 0.071291\tval_acc: 1.000000 val_loss: 0.0036844\n",
      "1_571 train_acc: 0.9918 train_loss: 0.031092\tval_acc: 1.000000 val_loss: 0.0023026\n",
      "1_653 train_acc: 0.9549 train_loss: 0.122879\tval_acc: 1.000000 val_loss: 0.0021556\n",
      "1_667 train_acc: 0.9754 train_loss: 0.079775\tval_acc: 1.000000 val_loss: 0.0020324\n",
      "1_673 train_acc: 0.9795 train_loss: 0.059940\tval_acc: 1.000000 val_loss: 0.0013585\n",
      "1_732 train_acc: 0.9754 train_loss: 0.060271\tval_acc: 1.000000 val_loss: 0.0012213\n",
      "1_753 train_acc: 0.9918 train_loss: 0.028131\tval_acc: 1.000000 val_loss: 0.0011678\n",
      "1_789 train_acc: 0.9795 train_loss: 0.067208\tval_acc: 1.000000 val_loss: 0.0011012\n",
      "1_796 train_acc: 0.9631 train_loss: 0.115510\tval_acc: 1.000000 val_loss: 0.0009799\n",
      "1_821 train_acc: 0.9754 train_loss: 0.069711\tval_acc: 1.000000 val_loss: 0.0009434\n",
      "1_824 train_acc: 0.9590 train_loss: 0.089920\tval_acc: 1.000000 val_loss: 0.0008595\n",
      "1_835 train_acc: 0.9713 train_loss: 0.055827\tval_acc: 1.000000 val_loss: 0.0004582\n",
      "epoch:  835 \tThe test accuracy is: 0.73125\n",
      " THE BEST ACCURACY IS 0.73125\tkappa is 0.4625\n",
      "subject 1 duration: 1:43:25.671181\n",
      "seed is 1722\n",
      "Subject 2\n",
      "-------------------- train size： (400, 1, 3, 1000) test size： (280, 3, 1000)\n",
      "2_0 train_acc: 0.5615 train_loss: 0.735408\tval_acc: 0.618375 val_loss: 0.6547394\n",
      "2_1 train_acc: 0.5902 train_loss: 0.746282\tval_acc: 0.650177 val_loss: 0.6069638\n",
      "2_2 train_acc: 0.6680 train_loss: 0.630422\tval_acc: 0.763251 val_loss: 0.5233340\n",
      "2_3 train_acc: 0.7377 train_loss: 0.595254\tval_acc: 0.763251 val_loss: 0.5005054\n",
      "2_5 train_acc: 0.7131 train_loss: 0.638358\tval_acc: 0.763251 val_loss: 0.4839481\n",
      "2_6 train_acc: 0.7910 train_loss: 0.486941\tval_acc: 0.770318 val_loss: 0.4708943\n",
      "2_7 train_acc: 0.6844 train_loss: 0.615791\tval_acc: 0.784452 val_loss: 0.4553995\n",
      "2_9 train_acc: 0.7664 train_loss: 0.481214\tval_acc: 0.819788 val_loss: 0.4386138\n",
      "2_11 train_acc: 0.7664 train_loss: 0.517931\tval_acc: 0.819788 val_loss: 0.4355685\n",
      "2_13 train_acc: 0.7623 train_loss: 0.462304\tval_acc: 0.833922 val_loss: 0.3922701\n",
      "2_16 train_acc: 0.7787 train_loss: 0.509469\tval_acc: 0.837456 val_loss: 0.3827473\n",
      "2_20 train_acc: 0.7541 train_loss: 0.493959\tval_acc: 0.816254 val_loss: 0.3682908\n",
      "2_21 train_acc: 0.7992 train_loss: 0.432276\tval_acc: 0.855124 val_loss: 0.3573313\n",
      "2_30 train_acc: 0.7746 train_loss: 0.481986\tval_acc: 0.837456 val_loss: 0.3441370\n",
      "2_39 train_acc: 0.7746 train_loss: 0.500023\tval_acc: 0.851590 val_loss: 0.3391600\n",
      "2_41 train_acc: 0.7910 train_loss: 0.412778\tval_acc: 0.883392 val_loss: 0.2889534\n",
      "2_64 train_acc: 0.7992 train_loss: 0.470133\tval_acc: 0.886926 val_loss: 0.2493562\n",
      "2_76 train_acc: 0.8484 train_loss: 0.377126\tval_acc: 0.890459 val_loss: 0.2450945\n",
      "2_94 train_acc: 0.8197 train_loss: 0.419291\tval_acc: 0.901060 val_loss: 0.2383663\n",
      "2_96 train_acc: 0.8361 train_loss: 0.384950\tval_acc: 0.911661 val_loss: 0.2376566\n",
      "2_97 train_acc: 0.8443 train_loss: 0.383295\tval_acc: 0.908127 val_loss: 0.2262478\n",
      "2_102 train_acc: 0.8607 train_loss: 0.311166\tval_acc: 0.918728 val_loss: 0.2186460\n",
      "2_107 train_acc: 0.8525 train_loss: 0.329730\tval_acc: 0.929329 val_loss: 0.2024778\n",
      "2_130 train_acc: 0.8893 train_loss: 0.280295\tval_acc: 0.943463 val_loss: 0.1639328\n",
      "2_145 train_acc: 0.8893 train_loss: 0.289543\tval_acc: 0.950530 val_loss: 0.1577761\n",
      "2_157 train_acc: 0.8811 train_loss: 0.306818\tval_acc: 0.961131 val_loss: 0.1542857\n",
      "2_160 train_acc: 0.8607 train_loss: 0.314345\tval_acc: 0.961131 val_loss: 0.1403164\n",
      "2_166 train_acc: 0.8934 train_loss: 0.235834\tval_acc: 0.961131 val_loss: 0.1277997\n",
      "2_171 train_acc: 0.8689 train_loss: 0.305634\tval_acc: 0.968198 val_loss: 0.1266240\n",
      "2_183 train_acc: 0.9180 train_loss: 0.219568\tval_acc: 0.978799 val_loss: 0.0993994\n",
      "2_207 train_acc: 0.9057 train_loss: 0.250293\tval_acc: 0.964664 val_loss: 0.0968722\n",
      "2_210 train_acc: 0.8934 train_loss: 0.301694\tval_acc: 0.968198 val_loss: 0.0920028\n",
      "2_212 train_acc: 0.9221 train_loss: 0.190825\tval_acc: 0.985866 val_loss: 0.0827114\n",
      "2_221 train_acc: 0.8934 train_loss: 0.244078\tval_acc: 0.975265 val_loss: 0.0733710\n",
      "2_253 train_acc: 0.8975 train_loss: 0.241100\tval_acc: 0.982332 val_loss: 0.0634527\n",
      "2_255 train_acc: 0.8975 train_loss: 0.205065\tval_acc: 0.989399 val_loss: 0.0549306\n",
      "2_272 train_acc: 0.9344 train_loss: 0.187568\tval_acc: 0.985866 val_loss: 0.0498795\n",
      "2_276 train_acc: 0.9098 train_loss: 0.217392\tval_acc: 0.989399 val_loss: 0.0452872\n",
      "2_280 train_acc: 0.9262 train_loss: 0.179467\tval_acc: 0.992933 val_loss: 0.0451849\n",
      "2_292 train_acc: 0.9016 train_loss: 0.190521\tval_acc: 0.996466 val_loss: 0.0390369\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2_304 train_acc: 0.9344 train_loss: 0.198449\tval_acc: 0.992933 val_loss: 0.0351071\n",
      "2_321 train_acc: 0.9098 train_loss: 0.190162\tval_acc: 0.992933 val_loss: 0.0278896\n",
      "2_339 train_acc: 0.9139 train_loss: 0.209165\tval_acc: 1.000000 val_loss: 0.0244654\n",
      "2_341 train_acc: 0.9303 train_loss: 0.169127\tval_acc: 1.000000 val_loss: 0.0221939\n",
      "2_348 train_acc: 0.9467 train_loss: 0.172295\tval_acc: 1.000000 val_loss: 0.0205490\n",
      "2_356 train_acc: 0.9303 train_loss: 0.131061\tval_acc: 1.000000 val_loss: 0.0187838\n",
      "2_372 train_acc: 0.9426 train_loss: 0.137031\tval_acc: 1.000000 val_loss: 0.0124411\n",
      "2_402 train_acc: 0.9344 train_loss: 0.145060\tval_acc: 1.000000 val_loss: 0.0112761\n",
      "2_429 train_acc: 0.9508 train_loss: 0.142136\tval_acc: 1.000000 val_loss: 0.0092350\n",
      "2_460 train_acc: 0.9303 train_loss: 0.183681\tval_acc: 1.000000 val_loss: 0.0086558\n",
      "2_467 train_acc: 0.9426 train_loss: 0.132482\tval_acc: 1.000000 val_loss: 0.0083351\n",
      "2_468 train_acc: 0.9549 train_loss: 0.117106\tval_acc: 1.000000 val_loss: 0.0048715\n",
      "2_531 train_acc: 0.9262 train_loss: 0.192102\tval_acc: 1.000000 val_loss: 0.0043645\n",
      "2_541 train_acc: 0.9590 train_loss: 0.098613\tval_acc: 1.000000 val_loss: 0.0042971\n",
      "2_551 train_acc: 0.9221 train_loss: 0.224254\tval_acc: 1.000000 val_loss: 0.0040436\n",
      "2_565 train_acc: 0.9508 train_loss: 0.120595\tval_acc: 1.000000 val_loss: 0.0033806\n",
      "2_573 train_acc: 0.9590 train_loss: 0.098222\tval_acc: 1.000000 val_loss: 0.0027166\n",
      "2_598 train_acc: 0.9754 train_loss: 0.073352\tval_acc: 1.000000 val_loss: 0.0024957\n",
      "2_604 train_acc: 0.9631 train_loss: 0.085864\tval_acc: 1.000000 val_loss: 0.0017875\n",
      "2_624 train_acc: 0.9590 train_loss: 0.133168\tval_acc: 1.000000 val_loss: 0.0017638\n",
      "2_651 train_acc: 0.9221 train_loss: 0.173510\tval_acc: 1.000000 val_loss: 0.0014259\n",
      "2_653 train_acc: 0.9467 train_loss: 0.113398\tval_acc: 1.000000 val_loss: 0.0013833\n",
      "2_685 train_acc: 0.9467 train_loss: 0.147748\tval_acc: 1.000000 val_loss: 0.0012590\n",
      "2_694 train_acc: 0.9385 train_loss: 0.155572\tval_acc: 1.000000 val_loss: 0.0012560\n",
      "2_735 train_acc: 0.9385 train_loss: 0.129518\tval_acc: 1.000000 val_loss: 0.0010918\n",
      "2_738 train_acc: 0.9590 train_loss: 0.118402\tval_acc: 1.000000 val_loss: 0.0009926\n",
      "2_779 train_acc: 0.9549 train_loss: 0.132797\tval_acc: 1.000000 val_loss: 0.0009294\n",
      "2_782 train_acc: 0.9385 train_loss: 0.123335\tval_acc: 1.000000 val_loss: 0.0007292\n",
      "2_784 train_acc: 0.9508 train_loss: 0.120845\tval_acc: 1.000000 val_loss: 0.0006707\n",
      "2_791 train_acc: 0.9713 train_loss: 0.127408\tval_acc: 1.000000 val_loss: 0.0006628\n",
      "2_826 train_acc: 0.9713 train_loss: 0.091014\tval_acc: 1.000000 val_loss: 0.0005543\n",
      "2_839 train_acc: 0.9590 train_loss: 0.099468\tval_acc: 1.000000 val_loss: 0.0005532\n",
      "2_850 train_acc: 0.9631 train_loss: 0.097310\tval_acc: 1.000000 val_loss: 0.0005443\n",
      "2_866 train_acc: 0.9795 train_loss: 0.055550\tval_acc: 1.000000 val_loss: 0.0004592\n",
      "2_867 train_acc: 0.9631 train_loss: 0.097018\tval_acc: 1.000000 val_loss: 0.0003785\n",
      "2_960 train_acc: 0.9631 train_loss: 0.093640\tval_acc: 1.000000 val_loss: 0.0003542\n",
      "2_965 train_acc: 0.9631 train_loss: 0.066923\tval_acc: 1.000000 val_loss: 0.0003142\n",
      "2_970 train_acc: 0.9754 train_loss: 0.060358\tval_acc: 1.000000 val_loss: 0.0003020\n",
      "2_972 train_acc: 0.9672 train_loss: 0.083015\tval_acc: 1.000000 val_loss: 0.0002255\n",
      "epoch:  972 \tThe test accuracy is: 0.675\n",
      " THE BEST ACCURACY IS 0.675\tkappa is 0.35\n",
      "subject 2 duration: 1:42:49.792717\n",
      "seed is 599\n",
      "Subject 3\n",
      "-------------------- train size： (400, 1, 3, 1000) test size： (320, 3, 1000)\n",
      "3_0 train_acc: 0.4959 train_loss: 0.874941\tval_acc: 0.590106 val_loss: 0.6764562\n",
      "3_1 train_acc: 0.5656 train_loss: 0.738503\tval_acc: 0.689046 val_loss: 0.6295140\n",
      "3_2 train_acc: 0.6393 train_loss: 0.723327\tval_acc: 0.681979 val_loss: 0.5856676\n",
      "3_3 train_acc: 0.7172 train_loss: 0.616345\tval_acc: 0.710247 val_loss: 0.5533025\n",
      "3_9 train_acc: 0.7213 train_loss: 0.551454\tval_acc: 0.766784 val_loss: 0.5117105\n",
      "3_16 train_acc: 0.7910 train_loss: 0.463964\tval_acc: 0.749117 val_loss: 0.4965458\n",
      "3_19 train_acc: 0.7910 train_loss: 0.473457\tval_acc: 0.763251 val_loss: 0.4815910\n",
      "3_22 train_acc: 0.7705 train_loss: 0.484387\tval_acc: 0.777385 val_loss: 0.4696849\n",
      "3_25 train_acc: 0.7623 train_loss: 0.536111\tval_acc: 0.773852 val_loss: 0.4530199\n",
      "3_26 train_acc: 0.8033 train_loss: 0.454571\tval_acc: 0.809187 val_loss: 0.4468214\n",
      "3_28 train_acc: 0.7992 train_loss: 0.457477\tval_acc: 0.791519 val_loss: 0.4213614\n",
      "3_30 train_acc: 0.7869 train_loss: 0.424196\tval_acc: 0.837456 val_loss: 0.4182560\n",
      "3_35 train_acc: 0.8156 train_loss: 0.460664\tval_acc: 0.826855 val_loss: 0.3821157\n",
      "3_39 train_acc: 0.8156 train_loss: 0.406793\tval_acc: 0.855124 val_loss: 0.3707434\n",
      "3_41 train_acc: 0.8443 train_loss: 0.347673\tval_acc: 0.805654 val_loss: 0.3659902\n",
      "3_42 train_acc: 0.7828 train_loss: 0.479714\tval_acc: 0.837456 val_loss: 0.3555307\n",
      "3_45 train_acc: 0.8484 train_loss: 0.382917\tval_acc: 0.851590 val_loss: 0.3358702\n",
      "3_58 train_acc: 0.7787 train_loss: 0.452321\tval_acc: 0.876325 val_loss: 0.2783579\n",
      "3_74 train_acc: 0.8238 train_loss: 0.379179\tval_acc: 0.876325 val_loss: 0.2744823\n",
      "3_75 train_acc: 0.8361 train_loss: 0.412183\tval_acc: 0.893993 val_loss: 0.2713266\n",
      "3_77 train_acc: 0.8361 train_loss: 0.319026\tval_acc: 0.879859 val_loss: 0.2609787\n",
      "3_80 train_acc: 0.8033 train_loss: 0.446748\tval_acc: 0.897527 val_loss: 0.2536845\n",
      "3_86 train_acc: 0.8361 train_loss: 0.386319\tval_acc: 0.904594 val_loss: 0.2368770\n",
      "3_90 train_acc: 0.8156 train_loss: 0.379918\tval_acc: 0.901060 val_loss: 0.2305304\n",
      "3_101 train_acc: 0.8566 train_loss: 0.340767\tval_acc: 0.915194 val_loss: 0.2163285\n",
      "3_108 train_acc: 0.8484 train_loss: 0.335307\tval_acc: 0.915194 val_loss: 0.2145252\n",
      "3_118 train_acc: 0.8238 train_loss: 0.394674\tval_acc: 0.925795 val_loss: 0.1965254\n",
      "3_124 train_acc: 0.8770 train_loss: 0.284284\tval_acc: 0.929329 val_loss: 0.1889431\n",
      "3_125 train_acc: 0.8525 train_loss: 0.366841\tval_acc: 0.922261 val_loss: 0.1874691\n",
      "3_130 train_acc: 0.8525 train_loss: 0.312631\tval_acc: 0.939929 val_loss: 0.1741053\n",
      "3_144 train_acc: 0.8648 train_loss: 0.316288\tval_acc: 0.929329 val_loss: 0.1672605\n",
      "3_146 train_acc: 0.8648 train_loss: 0.319755\tval_acc: 0.939929 val_loss: 0.1648033\n",
      "3_158 train_acc: 0.8566 train_loss: 0.332099\tval_acc: 0.929329 val_loss: 0.1564732\n",
      "3_166 train_acc: 0.8443 train_loss: 0.338670\tval_acc: 0.939929 val_loss: 0.1441782\n",
      "3_170 train_acc: 0.8566 train_loss: 0.388935\tval_acc: 0.957597 val_loss: 0.1318103\n",
      "3_171 train_acc: 0.8525 train_loss: 0.324768\tval_acc: 0.954064 val_loss: 0.1311245\n",
      "3_180 train_acc: 0.8689 train_loss: 0.320959\tval_acc: 0.964664 val_loss: 0.1199041\n",
      "3_194 train_acc: 0.8852 train_loss: 0.293804\tval_acc: 0.957597 val_loss: 0.1176293\n",
      "3_201 train_acc: 0.8689 train_loss: 0.277604\tval_acc: 0.961131 val_loss: 0.1174152\n",
      "3_213 train_acc: 0.9344 train_loss: 0.208902\tval_acc: 0.961131 val_loss: 0.1125022\n",
      "3_221 train_acc: 0.9057 train_loss: 0.266652\tval_acc: 0.964664 val_loss: 0.1037483\n",
      "3_225 train_acc: 0.8852 train_loss: 0.244103\tval_acc: 0.964664 val_loss: 0.0951445\n",
      "3_234 train_acc: 0.8811 train_loss: 0.284138\tval_acc: 0.964664 val_loss: 0.0881010\n",
      "3_261 train_acc: 0.8648 train_loss: 0.284624\tval_acc: 0.975265 val_loss: 0.0812456\n",
      "3_264 train_acc: 0.8934 train_loss: 0.251215\tval_acc: 0.968198 val_loss: 0.0751662\n",
      "3_274 train_acc: 0.8934 train_loss: 0.225585\tval_acc: 0.978799 val_loss: 0.0747275\n",
      "3_279 train_acc: 0.8893 train_loss: 0.267195\tval_acc: 0.985866 val_loss: 0.0712873\n",
      "3_283 train_acc: 0.8975 train_loss: 0.243625\tval_acc: 0.971731 val_loss: 0.0703728\n",
      "3_284 train_acc: 0.9057 train_loss: 0.248392\tval_acc: 0.982332 val_loss: 0.0613524\n",
      "3_302 train_acc: 0.9344 train_loss: 0.177006\tval_acc: 0.978799 val_loss: 0.0613274\n",
      "3_305 train_acc: 0.9262 train_loss: 0.148855\tval_acc: 0.982332 val_loss: 0.0544483\n",
      "3_316 train_acc: 0.8730 train_loss: 0.283261\tval_acc: 0.989399 val_loss: 0.0498494\n",
      "3_334 train_acc: 0.9139 train_loss: 0.216445\tval_acc: 0.982332 val_loss: 0.0485234\n",
      "3_341 train_acc: 0.8975 train_loss: 0.219267\tval_acc: 0.985866 val_loss: 0.0448645\n",
      "3_344 train_acc: 0.9016 train_loss: 0.254770\tval_acc: 0.992933 val_loss: 0.0432731\n",
      "3_345 train_acc: 0.9180 train_loss: 0.195537\tval_acc: 0.996466 val_loss: 0.0427131\n",
      "3_357 train_acc: 0.9303 train_loss: 0.162383\tval_acc: 0.989399 val_loss: 0.0367973\n",
      "3_380 train_acc: 0.9139 train_loss: 0.199119\tval_acc: 0.996466 val_loss: 0.0367718\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3_384 train_acc: 0.9180 train_loss: 0.207238\tval_acc: 0.996466 val_loss: 0.0294163\n",
      "3_389 train_acc: 0.9139 train_loss: 0.231242\tval_acc: 0.992933 val_loss: 0.0293332\n",
      "3_406 train_acc: 0.9426 train_loss: 0.143380\tval_acc: 0.996466 val_loss: 0.0289945\n",
      "3_424 train_acc: 0.9508 train_loss: 0.166514\tval_acc: 0.996466 val_loss: 0.0271979\n",
      "3_447 train_acc: 0.9221 train_loss: 0.185551\tval_acc: 0.996466 val_loss: 0.0262817\n",
      "3_453 train_acc: 0.9303 train_loss: 0.184258\tval_acc: 0.992933 val_loss: 0.0251587\n",
      "3_462 train_acc: 0.9344 train_loss: 0.146560\tval_acc: 1.000000 val_loss: 0.0166653\n",
      "3_514 train_acc: 0.9303 train_loss: 0.181820\tval_acc: 1.000000 val_loss: 0.0120772\n",
      "3_540 train_acc: 0.9508 train_loss: 0.144549\tval_acc: 1.000000 val_loss: 0.0120000\n",
      "3_546 train_acc: 0.9098 train_loss: 0.206978\tval_acc: 1.000000 val_loss: 0.0114043\n",
      "3_559 train_acc: 0.9385 train_loss: 0.166770\tval_acc: 1.000000 val_loss: 0.0087712\n",
      "3_569 train_acc: 0.9221 train_loss: 0.212366\tval_acc: 1.000000 val_loss: 0.0074810\n",
      "3_600 train_acc: 0.9385 train_loss: 0.149009\tval_acc: 1.000000 val_loss: 0.0059274\n",
      "3_669 train_acc: 0.9508 train_loss: 0.119938\tval_acc: 1.000000 val_loss: 0.0058817\n",
      "3_673 train_acc: 0.9631 train_loss: 0.105416\tval_acc: 1.000000 val_loss: 0.0050832\n",
      "3_680 train_acc: 0.9385 train_loss: 0.137123\tval_acc: 1.000000 val_loss: 0.0045786\n",
      "3_690 train_acc: 0.9426 train_loss: 0.155422\tval_acc: 1.000000 val_loss: 0.0044785\n",
      "3_699 train_acc: 0.9385 train_loss: 0.150888\tval_acc: 1.000000 val_loss: 0.0043771\n",
      "3_703 train_acc: 0.9467 train_loss: 0.157025\tval_acc: 1.000000 val_loss: 0.0043063\n",
      "3_705 train_acc: 0.9344 train_loss: 0.145446\tval_acc: 1.000000 val_loss: 0.0035795\n",
      "3_714 train_acc: 0.9467 train_loss: 0.113680\tval_acc: 1.000000 val_loss: 0.0026146\n",
      "3_745 train_acc: 0.9344 train_loss: 0.134477\tval_acc: 1.000000 val_loss: 0.0025839\n",
      "3_749 train_acc: 0.9426 train_loss: 0.151870\tval_acc: 1.000000 val_loss: 0.0019313\n",
      "3_825 train_acc: 0.9426 train_loss: 0.138815\tval_acc: 1.000000 val_loss: 0.0019021\n",
      "3_827 train_acc: 0.9508 train_loss: 0.148610\tval_acc: 1.000000 val_loss: 0.0018930\n",
      "3_832 train_acc: 0.9426 train_loss: 0.121855\tval_acc: 1.000000 val_loss: 0.0016639\n",
      "3_851 train_acc: 0.9426 train_loss: 0.126934\tval_acc: 1.000000 val_loss: 0.0012730\n",
      "3_858 train_acc: 0.9590 train_loss: 0.087931\tval_acc: 1.000000 val_loss: 0.0011790\n",
      "3_908 train_acc: 0.9631 train_loss: 0.095115\tval_acc: 1.000000 val_loss: 0.0011687\n",
      "3_918 train_acc: 0.9426 train_loss: 0.130923\tval_acc: 1.000000 val_loss: 0.0011285\n",
      "3_938 train_acc: 0.9713 train_loss: 0.100537\tval_acc: 1.000000 val_loss: 0.0009908\n",
      "3_959 train_acc: 0.9631 train_loss: 0.100420\tval_acc: 1.000000 val_loss: 0.0009615\n",
      "3_960 train_acc: 0.9754 train_loss: 0.064069\tval_acc: 1.000000 val_loss: 0.0008061\n",
      "3_977 train_acc: 0.9672 train_loss: 0.091608\tval_acc: 1.000000 val_loss: 0.0006742\n",
      "3_994 train_acc: 0.9549 train_loss: 0.133156\tval_acc: 1.000000 val_loss: 0.0005238\n",
      "epoch:  994 \tThe test accuracy is: 0.790625\n",
      " THE BEST ACCURACY IS 0.790625\tkappa is 0.58125\n",
      "subject 3 duration: 1:30:45.051271\n",
      "seed is 1833\n",
      "Subject 4\n",
      "-------------------- train size： (420, 1, 3, 1000) test size： (320, 3, 1000)\n",
      "4_0 train_acc: 0.6202 train_loss: 0.656307\tval_acc: 0.481481 val_loss: 1.2926145\n",
      "4_1 train_acc: 0.6667 train_loss: 0.589596\tval_acc: 0.505051 val_loss: 1.0527718\n",
      "4_3 train_acc: 0.9302 train_loss: 0.191770\tval_acc: 0.895623 val_loss: 0.7461282\n",
      "4_5 train_acc: 0.9612 train_loss: 0.108182\tval_acc: 0.909091 val_loss: 0.4532958\n",
      "4_6 train_acc: 0.9612 train_loss: 0.103941\tval_acc: 0.922559 val_loss: 0.3502537\n",
      "4_7 train_acc: 0.9535 train_loss: 0.139799\tval_acc: 0.925926 val_loss: 0.2749811\n",
      "4_10 train_acc: 0.9419 train_loss: 0.124667\tval_acc: 0.932660 val_loss: 0.2268657\n",
      "4_16 train_acc: 0.9767 train_loss: 0.062149\tval_acc: 0.952862 val_loss: 0.1634758\n",
      "4_23 train_acc: 0.9845 train_loss: 0.057532\tval_acc: 0.942761 val_loss: 0.1468966\n",
      "4_24 train_acc: 0.9806 train_loss: 0.058103\tval_acc: 0.956229 val_loss: 0.1421686\n",
      "4_26 train_acc: 0.9729 train_loss: 0.057244\tval_acc: 0.959596 val_loss: 0.1200383\n",
      "4_32 train_acc: 0.9845 train_loss: 0.036627\tval_acc: 0.969697 val_loss: 0.1082878\n",
      "4_37 train_acc: 0.9845 train_loss: 0.042340\tval_acc: 0.962963 val_loss: 0.0949760\n",
      "4_39 train_acc: 0.9767 train_loss: 0.065952\tval_acc: 0.962963 val_loss: 0.0822141\n",
      "4_43 train_acc: 0.9767 train_loss: 0.050972\tval_acc: 0.973064 val_loss: 0.0708634\n",
      "4_44 train_acc: 0.9806 train_loss: 0.062482\tval_acc: 0.983165 val_loss: 0.0469231\n",
      "4_52 train_acc: 0.9767 train_loss: 0.060040\tval_acc: 0.979798 val_loss: 0.0332859\n",
      "4_84 train_acc: 0.9729 train_loss: 0.118517\tval_acc: 0.989899 val_loss: 0.0259317\n",
      "4_100 train_acc: 0.9922 train_loss: 0.021569\tval_acc: 0.996633 val_loss: 0.0182040\n",
      "4_122 train_acc: 0.9729 train_loss: 0.054221\tval_acc: 0.996633 val_loss: 0.0178365\n",
      "4_130 train_acc: 0.9884 train_loss: 0.054956\tval_acc: 0.996633 val_loss: 0.0145043\n",
      "4_135 train_acc: 0.9884 train_loss: 0.040863\tval_acc: 0.996633 val_loss: 0.0120539\n",
      "4_141 train_acc: 0.9767 train_loss: 0.045592\tval_acc: 0.996633 val_loss: 0.0111389\n",
      "4_152 train_acc: 0.9961 train_loss: 0.022351\tval_acc: 1.000000 val_loss: 0.0086443\n",
      "4_165 train_acc: 0.9845 train_loss: 0.046679\tval_acc: 1.000000 val_loss: 0.0085613\n",
      "4_173 train_acc: 0.9961 train_loss: 0.018909\tval_acc: 1.000000 val_loss: 0.0074357\n",
      "4_198 train_acc: 0.9767 train_loss: 0.056126\tval_acc: 0.996633 val_loss: 0.0070594\n",
      "4_208 train_acc: 0.9884 train_loss: 0.040585\tval_acc: 0.996633 val_loss: 0.0069051\n",
      "4_213 train_acc: 0.9884 train_loss: 0.061751\tval_acc: 1.000000 val_loss: 0.0021202\n",
      "4_244 train_acc: 0.9884 train_loss: 0.050788\tval_acc: 1.000000 val_loss: 0.0016503\n",
      "4_259 train_acc: 0.9884 train_loss: 0.021288\tval_acc: 1.000000 val_loss: 0.0012271\n",
      "4_377 train_acc: 0.9961 train_loss: 0.012129\tval_acc: 1.000000 val_loss: 0.0004515\n",
      "4_561 train_acc: 1.0000 train_loss: 0.007451\tval_acc: 1.000000 val_loss: 0.0001585\n",
      "4_628 train_acc: 1.0000 train_loss: 0.000276\tval_acc: 1.000000 val_loss: 0.0000564\n",
      "4_745 train_acc: 0.9961 train_loss: 0.015793\tval_acc: 1.000000 val_loss: 0.0000252\n",
      "4_873 train_acc: 0.9961 train_loss: 0.012267\tval_acc: 1.000000 val_loss: 0.0000247\n",
      "epoch:  873 \tThe test accuracy is: 0.971875\n",
      " THE BEST ACCURACY IS 0.971875\tkappa is 0.94375\n",
      "subject 4 duration: 1:01:54.651070\n",
      "seed is 1247\n",
      "Subject 5\n",
      "-------------------- train size： (420, 1, 3, 1000) test size： (320, 3, 1000)\n",
      "5_0 train_acc: 0.5543 train_loss: 0.812528\tval_acc: 0.488215 val_loss: 0.7201802\n",
      "5_1 train_acc: 0.5465 train_loss: 0.826106\tval_acc: 0.602694 val_loss: 0.6559791\n",
      "5_3 train_acc: 0.6240 train_loss: 0.698986\tval_acc: 0.673401 val_loss: 0.5932328\n",
      "5_7 train_acc: 0.6628 train_loss: 0.643193\tval_acc: 0.723906 val_loss: 0.5853987\n",
      "5_9 train_acc: 0.6628 train_loss: 0.656392\tval_acc: 0.659933 val_loss: 0.5793427\n",
      "5_10 train_acc: 0.6705 train_loss: 0.687429\tval_acc: 0.710438 val_loss: 0.5751122\n",
      "5_12 train_acc: 0.6860 train_loss: 0.590003\tval_acc: 0.727273 val_loss: 0.5600991\n",
      "5_13 train_acc: 0.6938 train_loss: 0.586026\tval_acc: 0.747475 val_loss: 0.5463202\n",
      "5_14 train_acc: 0.6899 train_loss: 0.582559\tval_acc: 0.730640 val_loss: 0.5389615\n",
      "5_16 train_acc: 0.6938 train_loss: 0.571724\tval_acc: 0.707071 val_loss: 0.5338416\n",
      "5_20 train_acc: 0.7326 train_loss: 0.555322\tval_acc: 0.740741 val_loss: 0.5284843\n",
      "5_21 train_acc: 0.6977 train_loss: 0.618207\tval_acc: 0.754209 val_loss: 0.4925355\n",
      "5_24 train_acc: 0.7326 train_loss: 0.522773\tval_acc: 0.767677 val_loss: 0.4773948\n",
      "5_27 train_acc: 0.7791 train_loss: 0.486922\tval_acc: 0.791246 val_loss: 0.4550361\n",
      "5_28 train_acc: 0.7326 train_loss: 0.510313\tval_acc: 0.797980 val_loss: 0.4275485\n",
      "5_36 train_acc: 0.6977 train_loss: 0.586353\tval_acc: 0.801347 val_loss: 0.4252695\n",
      "5_38 train_acc: 0.7674 train_loss: 0.473266\tval_acc: 0.808081 val_loss: 0.4244221\n",
      "5_40 train_acc: 0.7442 train_loss: 0.545559\tval_acc: 0.787879 val_loss: 0.4109960\n",
      "5_42 train_acc: 0.7868 train_loss: 0.474613\tval_acc: 0.824916 val_loss: 0.3953672\n",
      "5_45 train_acc: 0.7984 train_loss: 0.394980\tval_acc: 0.861953 val_loss: 0.3125484\n",
      "5_52 train_acc: 0.8605 train_loss: 0.322876\tval_acc: 0.905724 val_loss: 0.2608716\n",
      "5_54 train_acc: 0.8140 train_loss: 0.399837\tval_acc: 0.892256 val_loss: 0.2350643\n",
      "5_62 train_acc: 0.9070 train_loss: 0.223166\tval_acc: 0.929293 val_loss: 0.2066057\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5_66 train_acc: 0.9302 train_loss: 0.175630\tval_acc: 0.925926 val_loss: 0.2040293\n",
      "5_70 train_acc: 0.9147 train_loss: 0.213165\tval_acc: 0.925926 val_loss: 0.2032105\n",
      "5_76 train_acc: 0.9496 train_loss: 0.165164\tval_acc: 0.936027 val_loss: 0.1871921\n",
      "5_86 train_acc: 0.9264 train_loss: 0.168132\tval_acc: 0.939394 val_loss: 0.1561217\n",
      "5_96 train_acc: 0.9419 train_loss: 0.140750\tval_acc: 0.939394 val_loss: 0.1499123\n",
      "5_100 train_acc: 0.9109 train_loss: 0.206050\tval_acc: 0.939394 val_loss: 0.1387348\n",
      "5_105 train_acc: 0.9612 train_loss: 0.127908\tval_acc: 0.946128 val_loss: 0.1326508\n",
      "5_111 train_acc: 0.9574 train_loss: 0.112821\tval_acc: 0.946128 val_loss: 0.1233876\n",
      "5_129 train_acc: 0.9419 train_loss: 0.144130\tval_acc: 0.959596 val_loss: 0.1197082\n",
      "5_132 train_acc: 0.9574 train_loss: 0.109922\tval_acc: 0.946128 val_loss: 0.1193534\n",
      "5_135 train_acc: 0.9806 train_loss: 0.082856\tval_acc: 0.949495 val_loss: 0.1143817\n",
      "5_143 train_acc: 0.9496 train_loss: 0.134846\tval_acc: 0.962963 val_loss: 0.1106636\n",
      "5_145 train_acc: 0.9457 train_loss: 0.145053\tval_acc: 0.956229 val_loss: 0.0985456\n",
      "5_152 train_acc: 0.9264 train_loss: 0.131072\tval_acc: 0.976431 val_loss: 0.0594170\n",
      "5_180 train_acc: 0.9535 train_loss: 0.121022\tval_acc: 0.976431 val_loss: 0.0563724\n",
      "5_215 train_acc: 0.9651 train_loss: 0.106534\tval_acc: 0.979798 val_loss: 0.0478130\n",
      "5_216 train_acc: 0.9302 train_loss: 0.181121\tval_acc: 0.976431 val_loss: 0.0402365\n",
      "5_282 train_acc: 0.9729 train_loss: 0.072050\tval_acc: 0.996633 val_loss: 0.0249857\n",
      "5_299 train_acc: 0.9690 train_loss: 0.107139\tval_acc: 0.993266 val_loss: 0.0184327\n",
      "5_312 train_acc: 0.9767 train_loss: 0.067761\tval_acc: 0.996633 val_loss: 0.0169185\n",
      "5_325 train_acc: 0.9612 train_loss: 0.103478\tval_acc: 0.996633 val_loss: 0.0129871\n",
      "5_391 train_acc: 0.9341 train_loss: 0.153774\tval_acc: 1.000000 val_loss: 0.0113915\n",
      "5_424 train_acc: 0.9651 train_loss: 0.095093\tval_acc: 1.000000 val_loss: 0.0087005\n",
      "5_473 train_acc: 0.9884 train_loss: 0.038374\tval_acc: 1.000000 val_loss: 0.0072979\n",
      "5_494 train_acc: 0.9612 train_loss: 0.082863\tval_acc: 1.000000 val_loss: 0.0072141\n",
      "5_507 train_acc: 0.9884 train_loss: 0.042072\tval_acc: 0.996633 val_loss: 0.0054672\n",
      "5_585 train_acc: 0.9574 train_loss: 0.090803\tval_acc: 1.000000 val_loss: 0.0049595\n",
      "5_601 train_acc: 0.9845 train_loss: 0.045152\tval_acc: 1.000000 val_loss: 0.0040936\n",
      "5_627 train_acc: 0.9729 train_loss: 0.090682\tval_acc: 1.000000 val_loss: 0.0039708\n",
      "5_657 train_acc: 0.9690 train_loss: 0.070133\tval_acc: 1.000000 val_loss: 0.0039625\n",
      "5_670 train_acc: 0.9496 train_loss: 0.176714\tval_acc: 1.000000 val_loss: 0.0031448\n",
      "5_704 train_acc: 0.9806 train_loss: 0.053789\tval_acc: 1.000000 val_loss: 0.0020663\n",
      "5_764 train_acc: 0.9729 train_loss: 0.077288\tval_acc: 1.000000 val_loss: 0.0018465\n",
      "5_833 train_acc: 0.9845 train_loss: 0.041537\tval_acc: 1.000000 val_loss: 0.0016979\n",
      "5_834 train_acc: 0.9690 train_loss: 0.058309\tval_acc: 1.000000 val_loss: 0.0013439\n",
      "5_927 train_acc: 0.9806 train_loss: 0.078750\tval_acc: 1.000000 val_loss: 0.0012626\n",
      "5_939 train_acc: 0.9884 train_loss: 0.028624\tval_acc: 1.000000 val_loss: 0.0007238\n",
      "epoch:  939 \tThe test accuracy is: 0.96875\n",
      " THE BEST ACCURACY IS 0.96875\tkappa is 0.9375\n",
      "subject 5 duration: 1:03:25.864194\n",
      "seed is 550\n",
      "Subject 6\n",
      "-------------------- train size： (400, 1, 3, 1000) test size： (320, 3, 1000)\n",
      "6_0 train_acc: 0.5082 train_loss: 0.861854\tval_acc: 0.593640 val_loss: 0.6775923\n",
      "6_1 train_acc: 0.5861 train_loss: 0.761649\tval_acc: 0.646643 val_loss: 0.6437697\n",
      "6_2 train_acc: 0.6475 train_loss: 0.660703\tval_acc: 0.600707 val_loss: 0.6346233\n",
      "6_3 train_acc: 0.7213 train_loss: 0.600514\tval_acc: 0.745583 val_loss: 0.5256382\n",
      "6_4 train_acc: 0.6967 train_loss: 0.581968\tval_acc: 0.770318 val_loss: 0.5097228\n",
      "6_10 train_acc: 0.7254 train_loss: 0.527434\tval_acc: 0.805654 val_loss: 0.4673043\n",
      "6_11 train_acc: 0.7869 train_loss: 0.495755\tval_acc: 0.784452 val_loss: 0.4615335\n",
      "6_13 train_acc: 0.7541 train_loss: 0.488992\tval_acc: 0.791519 val_loss: 0.4374323\n",
      "6_16 train_acc: 0.7500 train_loss: 0.581415\tval_acc: 0.802120 val_loss: 0.4183484\n",
      "6_17 train_acc: 0.7664 train_loss: 0.472254\tval_acc: 0.795053 val_loss: 0.4164529\n",
      "6_22 train_acc: 0.7582 train_loss: 0.509217\tval_acc: 0.833922 val_loss: 0.3870385\n",
      "6_30 train_acc: 0.8320 train_loss: 0.410628\tval_acc: 0.833922 val_loss: 0.3828005\n",
      "6_32 train_acc: 0.8279 train_loss: 0.359411\tval_acc: 0.855124 val_loss: 0.3481021\n",
      "6_37 train_acc: 0.7746 train_loss: 0.461431\tval_acc: 0.862191 val_loss: 0.3444797\n",
      "6_38 train_acc: 0.7500 train_loss: 0.496132\tval_acc: 0.869258 val_loss: 0.3172765\n",
      "6_47 train_acc: 0.7705 train_loss: 0.491502\tval_acc: 0.869258 val_loss: 0.2889909\n",
      "6_50 train_acc: 0.8811 train_loss: 0.333745\tval_acc: 0.893993 val_loss: 0.2427648\n",
      "6_60 train_acc: 0.9139 train_loss: 0.253554\tval_acc: 0.911661 val_loss: 0.2085052\n",
      "6_61 train_acc: 0.8975 train_loss: 0.251881\tval_acc: 0.911661 val_loss: 0.1962574\n",
      "6_66 train_acc: 0.8893 train_loss: 0.269022\tval_acc: 0.925795 val_loss: 0.1940619\n",
      "6_71 train_acc: 0.8770 train_loss: 0.256901\tval_acc: 0.932862 val_loss: 0.1897414\n",
      "6_83 train_acc: 0.8607 train_loss: 0.310261\tval_acc: 0.943463 val_loss: 0.1647711\n",
      "6_115 train_acc: 0.9180 train_loss: 0.236150\tval_acc: 0.950530 val_loss: 0.1457906\n",
      "6_117 train_acc: 0.8607 train_loss: 0.268882\tval_acc: 0.939929 val_loss: 0.1396264\n",
      "6_118 train_acc: 0.8934 train_loss: 0.238313\tval_acc: 0.943463 val_loss: 0.1187914\n",
      "6_147 train_acc: 0.9139 train_loss: 0.209496\tval_acc: 0.950530 val_loss: 0.1145330\n",
      "6_154 train_acc: 0.9262 train_loss: 0.170778\tval_acc: 0.961131 val_loss: 0.1139546\n",
      "6_163 train_acc: 0.9016 train_loss: 0.241363\tval_acc: 0.964664 val_loss: 0.1012215\n",
      "6_181 train_acc: 0.9262 train_loss: 0.251721\tval_acc: 0.985866 val_loss: 0.0703472\n",
      "6_219 train_acc: 0.9303 train_loss: 0.167944\tval_acc: 0.978799 val_loss: 0.0642375\n",
      "6_242 train_acc: 0.9467 train_loss: 0.134164\tval_acc: 0.971731 val_loss: 0.0600593\n",
      "6_254 train_acc: 0.9057 train_loss: 0.233914\tval_acc: 0.989399 val_loss: 0.0529953\n",
      "6_265 train_acc: 0.9180 train_loss: 0.212916\tval_acc: 0.982332 val_loss: 0.0525887\n",
      "6_275 train_acc: 0.9057 train_loss: 0.236607\tval_acc: 0.978799 val_loss: 0.0497575\n",
      "6_300 train_acc: 0.9303 train_loss: 0.157896\tval_acc: 0.992933 val_loss: 0.0481667\n",
      "6_302 train_acc: 0.9467 train_loss: 0.134561\tval_acc: 0.978799 val_loss: 0.0455039\n",
      "6_313 train_acc: 0.9426 train_loss: 0.141441\tval_acc: 0.982332 val_loss: 0.0415464\n",
      "6_314 train_acc: 0.9631 train_loss: 0.138965\tval_acc: 0.992933 val_loss: 0.0373938\n",
      "6_316 train_acc: 0.9344 train_loss: 0.167600\tval_acc: 0.989399 val_loss: 0.0304764\n",
      "6_353 train_acc: 0.9385 train_loss: 0.122387\tval_acc: 0.992933 val_loss: 0.0296955\n",
      "6_374 train_acc: 0.9508 train_loss: 0.140499\tval_acc: 0.992933 val_loss: 0.0263835\n",
      "6_380 train_acc: 0.9590 train_loss: 0.113782\tval_acc: 0.996466 val_loss: 0.0257335\n",
      "6_386 train_acc: 0.9426 train_loss: 0.139547\tval_acc: 1.000000 val_loss: 0.0213109\n",
      "6_408 train_acc: 0.8975 train_loss: 0.186702\tval_acc: 0.989399 val_loss: 0.0209991\n",
      "6_411 train_acc: 0.9467 train_loss: 0.132425\tval_acc: 0.996466 val_loss: 0.0180382\n",
      "6_413 train_acc: 0.9426 train_loss: 0.129776\tval_acc: 1.000000 val_loss: 0.0138689\n",
      "6_460 train_acc: 0.9467 train_loss: 0.117690\tval_acc: 0.996466 val_loss: 0.0129301\n",
      "6_464 train_acc: 0.9549 train_loss: 0.113100\tval_acc: 1.000000 val_loss: 0.0118417\n",
      "6_477 train_acc: 0.9508 train_loss: 0.118441\tval_acc: 1.000000 val_loss: 0.0109199\n",
      "6_500 train_acc: 0.9426 train_loss: 0.127522\tval_acc: 1.000000 val_loss: 0.0104719\n",
      "6_503 train_acc: 0.9385 train_loss: 0.196913\tval_acc: 1.000000 val_loss: 0.0090073\n",
      "6_509 train_acc: 0.9754 train_loss: 0.094640\tval_acc: 1.000000 val_loss: 0.0059197\n",
      "6_587 train_acc: 0.9549 train_loss: 0.149692\tval_acc: 1.000000 val_loss: 0.0044067\n",
      "6_592 train_acc: 0.9590 train_loss: 0.098980\tval_acc: 1.000000 val_loss: 0.0039508\n",
      "6_632 train_acc: 0.9631 train_loss: 0.117602\tval_acc: 1.000000 val_loss: 0.0031702\n",
      "6_669 train_acc: 0.9754 train_loss: 0.058019\tval_acc: 1.000000 val_loss: 0.0028623\n",
      "6_677 train_acc: 0.9508 train_loss: 0.111615\tval_acc: 1.000000 val_loss: 0.0025446\n",
      "6_724 train_acc: 0.9426 train_loss: 0.151669\tval_acc: 1.000000 val_loss: 0.0020714\n",
      "6_729 train_acc: 0.9672 train_loss: 0.116145\tval_acc: 1.000000 val_loss: 0.0019165\n",
      "6_730 train_acc: 0.9918 train_loss: 0.029833\tval_acc: 1.000000 val_loss: 0.0017626\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6_736 train_acc: 0.9672 train_loss: 0.095610\tval_acc: 1.000000 val_loss: 0.0017166\n",
      "6_760 train_acc: 0.9672 train_loss: 0.070845\tval_acc: 1.000000 val_loss: 0.0014565\n",
      "6_816 train_acc: 0.9795 train_loss: 0.052410\tval_acc: 1.000000 val_loss: 0.0014169\n",
      "6_818 train_acc: 0.9836 train_loss: 0.043009\tval_acc: 1.000000 val_loss: 0.0013949\n",
      "6_852 train_acc: 0.9590 train_loss: 0.077055\tval_acc: 1.000000 val_loss: 0.0009286\n",
      "6_914 train_acc: 0.9672 train_loss: 0.078325\tval_acc: 1.000000 val_loss: 0.0008453\n",
      "6_915 train_acc: 0.9836 train_loss: 0.066440\tval_acc: 1.000000 val_loss: 0.0006100\n",
      "6_918 train_acc: 0.9713 train_loss: 0.065291\tval_acc: 1.000000 val_loss: 0.0005620\n",
      "6_984 train_acc: 0.9508 train_loss: 0.135954\tval_acc: 1.000000 val_loss: 0.0005009\n",
      "6_988 train_acc: 0.9795 train_loss: 0.056521\tval_acc: 1.000000 val_loss: 0.0004210\n",
      "6_997 train_acc: 0.9754 train_loss: 0.061832\tval_acc: 1.000000 val_loss: 0.0003732\n",
      "epoch:  997 \tThe test accuracy is: 0.83125\n",
      " THE BEST ACCURACY IS 0.83125\tkappa is 0.6625\n",
      "subject 6 duration: 1:00:27.989069\n",
      "seed is 1186\n",
      "Subject 7\n",
      "-------------------- train size： (400, 1, 3, 1000) test size： (320, 3, 1000)\n",
      "7_0 train_acc: 0.5123 train_loss: 0.829377\tval_acc: 0.643110 val_loss: 0.6372594\n",
      "7_1 train_acc: 0.7582 train_loss: 0.528778\tval_acc: 0.798587 val_loss: 0.4760320\n",
      "7_3 train_acc: 0.7295 train_loss: 0.549917\tval_acc: 0.805654 val_loss: 0.4432149\n",
      "7_10 train_acc: 0.8648 train_loss: 0.313781\tval_acc: 0.805654 val_loss: 0.4379901\n",
      "7_13 train_acc: 0.8689 train_loss: 0.318570\tval_acc: 0.833922 val_loss: 0.3736718\n",
      "7_15 train_acc: 0.8115 train_loss: 0.453140\tval_acc: 0.844523 val_loss: 0.3686272\n",
      "7_22 train_acc: 0.8074 train_loss: 0.372284\tval_acc: 0.837456 val_loss: 0.3599532\n",
      "7_25 train_acc: 0.8115 train_loss: 0.428770\tval_acc: 0.830389 val_loss: 0.3524739\n",
      "7_27 train_acc: 0.8279 train_loss: 0.368091\tval_acc: 0.855124 val_loss: 0.3462685\n",
      "7_31 train_acc: 0.8361 train_loss: 0.332750\tval_acc: 0.855124 val_loss: 0.3260187\n",
      "7_40 train_acc: 0.8238 train_loss: 0.388133\tval_acc: 0.876325 val_loss: 0.3158174\n",
      "7_44 train_acc: 0.8525 train_loss: 0.323170\tval_acc: 0.886926 val_loss: 0.2916979\n",
      "7_47 train_acc: 0.9139 train_loss: 0.214501\tval_acc: 0.911661 val_loss: 0.2387249\n",
      "7_54 train_acc: 0.9098 train_loss: 0.214061\tval_acc: 0.904594 val_loss: 0.2342823\n",
      "7_68 train_acc: 0.8934 train_loss: 0.275132\tval_acc: 0.908127 val_loss: 0.2158286\n",
      "7_70 train_acc: 0.9221 train_loss: 0.202636\tval_acc: 0.901060 val_loss: 0.1957130\n",
      "7_73 train_acc: 0.9180 train_loss: 0.184622\tval_acc: 0.936396 val_loss: 0.1407532\n",
      "7_99 train_acc: 0.9180 train_loss: 0.189593\tval_acc: 0.954064 val_loss: 0.1270274\n",
      "7_102 train_acc: 0.9221 train_loss: 0.226579\tval_acc: 0.954064 val_loss: 0.1175544\n",
      "7_103 train_acc: 0.9426 train_loss: 0.146073\tval_acc: 0.961131 val_loss: 0.1064913\n",
      "7_131 train_acc: 0.9262 train_loss: 0.154374\tval_acc: 0.968198 val_loss: 0.0825650\n",
      "7_147 train_acc: 0.9426 train_loss: 0.131386\tval_acc: 0.964664 val_loss: 0.0761574\n",
      "7_150 train_acc: 0.9590 train_loss: 0.136657\tval_acc: 0.971731 val_loss: 0.0756723\n",
      "7_163 train_acc: 0.9344 train_loss: 0.178279\tval_acc: 0.971731 val_loss: 0.0756191\n",
      "7_168 train_acc: 0.9508 train_loss: 0.135735\tval_acc: 0.978799 val_loss: 0.0574277\n",
      "7_176 train_acc: 0.9344 train_loss: 0.164665\tval_acc: 0.985866 val_loss: 0.0455632\n",
      "7_197 train_acc: 0.9631 train_loss: 0.110111\tval_acc: 0.982332 val_loss: 0.0415185\n",
      "7_198 train_acc: 0.9508 train_loss: 0.130968\tval_acc: 0.989399 val_loss: 0.0382729\n",
      "7_202 train_acc: 0.9590 train_loss: 0.100372\tval_acc: 0.992933 val_loss: 0.0314357\n",
      "7_225 train_acc: 0.9549 train_loss: 0.138936\tval_acc: 0.989399 val_loss: 0.0295300\n",
      "7_230 train_acc: 0.9713 train_loss: 0.093015\tval_acc: 0.992933 val_loss: 0.0234421\n",
      "7_274 train_acc: 0.9713 train_loss: 0.107594\tval_acc: 1.000000 val_loss: 0.0206366\n",
      "7_278 train_acc: 0.9672 train_loss: 0.080812\tval_acc: 1.000000 val_loss: 0.0145138\n",
      "7_285 train_acc: 0.9426 train_loss: 0.146934\tval_acc: 1.000000 val_loss: 0.0119955\n",
      "7_293 train_acc: 0.9713 train_loss: 0.046693\tval_acc: 1.000000 val_loss: 0.0105295\n",
      "7_349 train_acc: 0.9590 train_loss: 0.140821\tval_acc: 1.000000 val_loss: 0.0086100\n",
      "7_378 train_acc: 0.9795 train_loss: 0.046464\tval_acc: 1.000000 val_loss: 0.0078255\n",
      "7_382 train_acc: 0.9672 train_loss: 0.077342\tval_acc: 1.000000 val_loss: 0.0061426\n",
      "7_420 train_acc: 0.9713 train_loss: 0.056720\tval_acc: 1.000000 val_loss: 0.0048438\n",
      "7_429 train_acc: 0.9713 train_loss: 0.086798\tval_acc: 1.000000 val_loss: 0.0029410\n",
      "7_520 train_acc: 0.9836 train_loss: 0.042791\tval_acc: 1.000000 val_loss: 0.0026456\n",
      "7_530 train_acc: 0.9754 train_loss: 0.056848\tval_acc: 1.000000 val_loss: 0.0017624\n",
      "7_647 train_acc: 0.9672 train_loss: 0.081770\tval_acc: 1.000000 val_loss: 0.0012038\n",
      "7_669 train_acc: 0.9918 train_loss: 0.023192\tval_acc: 1.000000 val_loss: 0.0010707\n",
      "7_674 train_acc: 0.9672 train_loss: 0.074974\tval_acc: 1.000000 val_loss: 0.0010561\n",
      "7_748 train_acc: 0.9754 train_loss: 0.055618\tval_acc: 1.000000 val_loss: 0.0008076\n",
      "7_816 train_acc: 0.9836 train_loss: 0.073394\tval_acc: 1.000000 val_loss: 0.0007420\n",
      "7_863 train_acc: 0.9672 train_loss: 0.095863\tval_acc: 1.000000 val_loss: 0.0002477\n",
      "epoch:  863 \tThe test accuracy is: 0.93125\n",
      " THE BEST ACCURACY IS 0.93125\tkappa is 0.8625\n",
      "subject 7 duration: 0:57:20.679311\n",
      "seed is 1832\n",
      "Subject 8\n",
      "-------------------- train size： (440, 1, 3, 1000) test size： (320, 3, 1000)\n",
      "8_0 train_acc: 0.5405 train_loss: 0.793944\tval_acc: 0.516026 val_loss: 0.8683277\n",
      "8_6 train_acc: 0.8468 train_loss: 0.326275\tval_acc: 0.746795 val_loss: 0.8552064\n",
      "8_7 train_acc: 0.8423 train_loss: 0.330955\tval_acc: 0.766026 val_loss: 0.8392708\n",
      "8_9 train_acc: 0.8964 train_loss: 0.286742\tval_acc: 0.804487 val_loss: 0.6440594\n",
      "8_21 train_acc: 0.8829 train_loss: 0.303250\tval_acc: 0.810897 val_loss: 0.5354711\n",
      "8_33 train_acc: 0.9234 train_loss: 0.204076\tval_acc: 0.830128 val_loss: 0.4903915\n",
      "8_34 train_acc: 0.8964 train_loss: 0.244958\tval_acc: 0.826923 val_loss: 0.4537430\n",
      "8_38 train_acc: 0.9189 train_loss: 0.217165\tval_acc: 0.849359 val_loss: 0.3626798\n",
      "8_53 train_acc: 0.8423 train_loss: 0.322837\tval_acc: 0.887821 val_loss: 0.2765192\n",
      "8_70 train_acc: 0.8874 train_loss: 0.290929\tval_acc: 0.871795 val_loss: 0.2745718\n",
      "8_88 train_acc: 0.9144 train_loss: 0.205152\tval_acc: 0.884615 val_loss: 0.2639124\n",
      "8_92 train_acc: 0.9144 train_loss: 0.246914\tval_acc: 0.913462 val_loss: 0.2209049\n",
      "8_115 train_acc: 0.8874 train_loss: 0.257001\tval_acc: 0.907051 val_loss: 0.1944630\n",
      "8_129 train_acc: 0.8964 train_loss: 0.220110\tval_acc: 0.932692 val_loss: 0.1666949\n",
      "8_159 train_acc: 0.9324 train_loss: 0.188789\tval_acc: 0.935897 val_loss: 0.1660097\n",
      "8_160 train_acc: 0.9009 train_loss: 0.252735\tval_acc: 0.935897 val_loss: 0.1397560\n",
      "8_171 train_acc: 0.9234 train_loss: 0.160263\tval_acc: 0.951923 val_loss: 0.1286713\n",
      "8_180 train_acc: 0.9279 train_loss: 0.171174\tval_acc: 0.945513 val_loss: 0.1245206\n",
      "8_188 train_acc: 0.9189 train_loss: 0.186309\tval_acc: 0.967949 val_loss: 0.1015574\n",
      "8_224 train_acc: 0.9595 train_loss: 0.112734\tval_acc: 0.964744 val_loss: 0.0989491\n",
      "8_227 train_acc: 0.9505 train_loss: 0.118112\tval_acc: 0.958333 val_loss: 0.0962331\n",
      "8_230 train_acc: 0.9505 train_loss: 0.115655\tval_acc: 0.983974 val_loss: 0.0612079\n",
      "8_257 train_acc: 0.9414 train_loss: 0.180257\tval_acc: 0.987179 val_loss: 0.0533370\n",
      "8_276 train_acc: 0.9550 train_loss: 0.122305\tval_acc: 0.990385 val_loss: 0.0335260\n",
      "8_352 train_acc: 0.9324 train_loss: 0.170138\tval_acc: 0.993590 val_loss: 0.0260723\n",
      "8_375 train_acc: 0.9595 train_loss: 0.105318\tval_acc: 0.996795 val_loss: 0.0212199\n",
      "8_407 train_acc: 0.9414 train_loss: 0.117083\tval_acc: 0.996795 val_loss: 0.0203715\n",
      "8_408 train_acc: 0.9595 train_loss: 0.092142\tval_acc: 0.996795 val_loss: 0.0170436\n",
      "8_422 train_acc: 0.9459 train_loss: 0.137923\tval_acc: 1.000000 val_loss: 0.0143099\n",
      "8_437 train_acc: 0.9369 train_loss: 0.138392\tval_acc: 1.000000 val_loss: 0.0100143\n",
      "8_524 train_acc: 0.9820 train_loss: 0.062077\tval_acc: 1.000000 val_loss: 0.0050889\n",
      "8_586 train_acc: 0.9640 train_loss: 0.120326\tval_acc: 1.000000 val_loss: 0.0047815\n",
      "8_627 train_acc: 0.9640 train_loss: 0.140864\tval_acc: 1.000000 val_loss: 0.0042670\n",
      "8_632 train_acc: 0.9730 train_loss: 0.103634\tval_acc: 1.000000 val_loss: 0.0035847\n",
      "8_636 train_acc: 0.9595 train_loss: 0.086904\tval_acc: 1.000000 val_loss: 0.0034760\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8_643 train_acc: 0.9775 train_loss: 0.052930\tval_acc: 1.000000 val_loss: 0.0025450\n",
      "8_682 train_acc: 0.9730 train_loss: 0.075195\tval_acc: 1.000000 val_loss: 0.0018492\n",
      "8_735 train_acc: 0.9820 train_loss: 0.036802\tval_acc: 1.000000 val_loss: 0.0014335\n",
      "8_745 train_acc: 0.9595 train_loss: 0.061406\tval_acc: 1.000000 val_loss: 0.0014145\n",
      "8_748 train_acc: 0.9820 train_loss: 0.078801\tval_acc: 1.000000 val_loss: 0.0012711\n",
      "8_796 train_acc: 0.9865 train_loss: 0.040257\tval_acc: 1.000000 val_loss: 0.0011821\n",
      "8_808 train_acc: 0.9640 train_loss: 0.097603\tval_acc: 1.000000 val_loss: 0.0011058\n",
      "8_818 train_acc: 0.9550 train_loss: 0.110861\tval_acc: 1.000000 val_loss: 0.0008342\n",
      "8_834 train_acc: 0.9414 train_loss: 0.180813\tval_acc: 1.000000 val_loss: 0.0005599\n",
      "8_911 train_acc: 0.9640 train_loss: 0.100402\tval_acc: 1.000000 val_loss: 0.0004088\n",
      "8_932 train_acc: 0.9550 train_loss: 0.135827\tval_acc: 1.000000 val_loss: 0.0003086\n",
      "8_990 train_acc: 0.9865 train_loss: 0.040848\tval_acc: 1.000000 val_loss: 0.0002272\n",
      "epoch:  990 \tThe test accuracy is: 0.928125\n",
      " THE BEST ACCURACY IS 0.928125\tkappa is 0.85625\n",
      "subject 8 duration: 1:00:28.919425\n",
      "seed is 776\n",
      "Subject 9\n",
      "-------------------- train size： (400, 1, 3, 1000) test size： (320, 3, 1000)\n",
      "9_0 train_acc: 0.5820 train_loss: 0.808074\tval_acc: 0.494700 val_loss: 0.7097274\n",
      "9_4 train_acc: 0.5902 train_loss: 0.740897\tval_acc: 0.674912 val_loss: 0.5859421\n",
      "9_25 train_acc: 0.8852 train_loss: 0.284768\tval_acc: 0.798587 val_loss: 0.5450700\n",
      "9_29 train_acc: 0.8484 train_loss: 0.382222\tval_acc: 0.826855 val_loss: 0.4714384\n",
      "9_33 train_acc: 0.8730 train_loss: 0.259237\tval_acc: 0.833922 val_loss: 0.4539757\n",
      "9_36 train_acc: 0.8934 train_loss: 0.277371\tval_acc: 0.840989 val_loss: 0.4015994\n",
      "9_40 train_acc: 0.8566 train_loss: 0.390169\tval_acc: 0.862191 val_loss: 0.3651031\n",
      "9_57 train_acc: 0.8525 train_loss: 0.361302\tval_acc: 0.862191 val_loss: 0.3382570\n",
      "9_60 train_acc: 0.8934 train_loss: 0.243637\tval_acc: 0.855124 val_loss: 0.3150999\n",
      "9_64 train_acc: 0.8607 train_loss: 0.319200\tval_acc: 0.851590 val_loss: 0.3131080\n",
      "9_77 train_acc: 0.8852 train_loss: 0.285722\tval_acc: 0.862191 val_loss: 0.2969736\n",
      "9_85 train_acc: 0.9303 train_loss: 0.223874\tval_acc: 0.876325 val_loss: 0.2329811\n",
      "9_86 train_acc: 0.9385 train_loss: 0.172935\tval_acc: 0.890459 val_loss: 0.2297946\n",
      "9_117 train_acc: 0.9221 train_loss: 0.198380\tval_acc: 0.904594 val_loss: 0.2154899\n",
      "9_139 train_acc: 0.9139 train_loss: 0.195413\tval_acc: 0.908127 val_loss: 0.1956778\n",
      "9_140 train_acc: 0.9098 train_loss: 0.258335\tval_acc: 0.932862 val_loss: 0.1638198\n",
      "9_172 train_acc: 0.9221 train_loss: 0.163669\tval_acc: 0.939929 val_loss: 0.1620065\n",
      "9_175 train_acc: 0.9057 train_loss: 0.224463\tval_acc: 0.936396 val_loss: 0.1404879\n",
      "9_180 train_acc: 0.9180 train_loss: 0.169800\tval_acc: 0.957597 val_loss: 0.1326502\n",
      "9_185 train_acc: 0.8852 train_loss: 0.246450\tval_acc: 0.946996 val_loss: 0.1248832\n",
      "9_206 train_acc: 0.9221 train_loss: 0.180208\tval_acc: 0.954064 val_loss: 0.1111387\n",
      "9_225 train_acc: 0.9262 train_loss: 0.192006\tval_acc: 0.946996 val_loss: 0.1082060\n",
      "9_233 train_acc: 0.9344 train_loss: 0.156416\tval_acc: 0.964664 val_loss: 0.1054883\n",
      "9_259 train_acc: 0.9385 train_loss: 0.161161\tval_acc: 0.968198 val_loss: 0.1038432\n",
      "9_260 train_acc: 0.9672 train_loss: 0.134878\tval_acc: 0.964664 val_loss: 0.0929379\n",
      "9_262 train_acc: 0.8934 train_loss: 0.246196\tval_acc: 0.975265 val_loss: 0.0901630\n",
      "9_267 train_acc: 0.9426 train_loss: 0.173207\tval_acc: 0.964664 val_loss: 0.0887669\n",
      "9_272 train_acc: 0.9467 train_loss: 0.152025\tval_acc: 0.975265 val_loss: 0.0885907\n",
      "9_276 train_acc: 0.9467 train_loss: 0.131223\tval_acc: 0.961131 val_loss: 0.0876937\n",
      "9_279 train_acc: 0.8770 train_loss: 0.288777\tval_acc: 0.961131 val_loss: 0.0860807\n",
      "9_283 train_acc: 0.9180 train_loss: 0.196733\tval_acc: 0.968198 val_loss: 0.0775996\n",
      "9_292 train_acc: 0.9426 train_loss: 0.118517\tval_acc: 0.978799 val_loss: 0.0665853\n",
      "9_315 train_acc: 0.9221 train_loss: 0.182672\tval_acc: 0.982332 val_loss: 0.0608315\n",
      "9_321 train_acc: 0.9016 train_loss: 0.195280\tval_acc: 0.985866 val_loss: 0.0516534\n",
      "9_327 train_acc: 0.9262 train_loss: 0.172616\tval_acc: 0.982332 val_loss: 0.0457978\n",
      "9_328 train_acc: 0.9549 train_loss: 0.126373\tval_acc: 0.992933 val_loss: 0.0371840\n",
      "9_366 train_acc: 0.9303 train_loss: 0.214895\tval_acc: 0.989399 val_loss: 0.0330832\n",
      "9_417 train_acc: 0.9467 train_loss: 0.111837\tval_acc: 1.000000 val_loss: 0.0245119\n",
      "9_422 train_acc: 0.9303 train_loss: 0.140019\tval_acc: 1.000000 val_loss: 0.0179566\n",
      "9_457 train_acc: 0.9713 train_loss: 0.060746\tval_acc: 0.996466 val_loss: 0.0161469\n",
      "9_491 train_acc: 0.9426 train_loss: 0.111955\tval_acc: 1.000000 val_loss: 0.0154506\n",
      "9_510 train_acc: 0.9344 train_loss: 0.163411\tval_acc: 0.996466 val_loss: 0.0129909\n",
      "9_534 train_acc: 0.9508 train_loss: 0.133596\tval_acc: 0.996466 val_loss: 0.0114738\n",
      "9_548 train_acc: 0.9590 train_loss: 0.073763\tval_acc: 1.000000 val_loss: 0.0081197\n",
      "9_577 train_acc: 0.9508 train_loss: 0.135148\tval_acc: 1.000000 val_loss: 0.0076912\n",
      "9_583 train_acc: 0.9590 train_loss: 0.104203\tval_acc: 1.000000 val_loss: 0.0063686\n",
      "9_601 train_acc: 0.9549 train_loss: 0.096465\tval_acc: 1.000000 val_loss: 0.0059097\n",
      "9_605 train_acc: 0.9672 train_loss: 0.079850\tval_acc: 1.000000 val_loss: 0.0058441\n",
      "9_669 train_acc: 0.9590 train_loss: 0.116643\tval_acc: 1.000000 val_loss: 0.0045928\n",
      "9_692 train_acc: 0.9426 train_loss: 0.137571\tval_acc: 1.000000 val_loss: 0.0045873\n",
      "9_713 train_acc: 0.9549 train_loss: 0.146023\tval_acc: 1.000000 val_loss: 0.0029589\n",
      "9_791 train_acc: 0.9836 train_loss: 0.065180\tval_acc: 1.000000 val_loss: 0.0029383\n",
      "9_798 train_acc: 0.9836 train_loss: 0.074585\tval_acc: 1.000000 val_loss: 0.0028733\n",
      "9_819 train_acc: 0.9672 train_loss: 0.090286\tval_acc: 1.000000 val_loss: 0.0019471\n",
      "9_880 train_acc: 0.9549 train_loss: 0.091201\tval_acc: 1.000000 val_loss: 0.0016026\n",
      "9_937 train_acc: 0.9631 train_loss: 0.073113\tval_acc: 1.000000 val_loss: 0.0011570\n",
      "9_954 train_acc: 0.9672 train_loss: 0.097847\tval_acc: 1.000000 val_loss: 0.0009228\n",
      "9_980 train_acc: 0.9754 train_loss: 0.094798\tval_acc: 1.000000 val_loss: 0.0005364\n",
      "epoch:  980 \tThe test accuracy is: 0.9\n",
      " THE BEST ACCURACY IS 0.9\tkappa is 0.8\n",
      "subject 9 duration: 1:12:26.599516\n",
      "**The average Best accuracy is: 85.86805555555556kappa is: 71.73611111111111\n",
      "\n",
      "best epochs:  [835, 972, 994, 873, 939, 997, 863, 990, 980]\n",
      "---------  all result  ---------\n",
      "        accuray  precision     recall         f1      kappa\n",
      "0     73.125000  79.417611  73.125000  71.606620  46.250000\n",
      "1     67.500000  67.828369  67.500000  67.349659  35.000000\n",
      "2     79.062500  79.072721  79.062500  79.060660  58.125000\n",
      "3     97.187500  97.189343  97.187500  97.187473  94.375000\n",
      "4     96.875000  96.941011  96.875000  96.873901  93.750000\n",
      "5     83.125000  85.333333  83.125000  82.857143  66.250000\n",
      "6     93.125000  93.185730  93.125000  93.122582  86.250000\n",
      "7     92.812500  92.854350  92.812500  92.810745  85.625000\n",
      "8     90.000000  90.156863  90.000000  89.990225  80.000000\n",
      "mean  85.868056  86.886592  85.868056  85.651001  71.736111\n",
      "std   10.728787   9.846933  10.728787  11.002165  21.457575\n",
      "****************************************\n",
      "Wed May  1 11:20:05 2024\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This script fine-tunes the Conformer project (https://github.com/eeyhsong/EEG-Conformer) and achieves better experimental results compared to the previous project. \n",
    "\n",
    "fine-tunes by author: zhaowei701@163.com\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "gpus = [1]\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1'\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "from pandas import ExcelWriter\n",
    "from torchsummary import summary\n",
    "import torch\n",
    "from torch.backends import cudnn\n",
    "from utils import calMetrics\n",
    "from utils import calculatePerClass\n",
    "from utils import numberClassChannel\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "cudnn.benchmark = False\n",
    "cudnn.deterministic = True\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "from einops import rearrange, reduce, repeat\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from utils import numberClassChannel\n",
    "from utils import load_data_evaluate\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class EEGNet(nn.Module):\n",
    "    def __init__(self, nb_classes=4, kernel_size=125, number_channel=22, signal_length=1000, \n",
    "                 dropoutRate=0.5, pooling_size1=8, pooling_size2=8, f1=8, \n",
    "                 D=2, f2=16, norm_rate=0.25, dropout_rate=0.5):\n",
    "        super(EEGNet, self).__init__()\n",
    "        # 类似与不同频率的滤波：alpha\\beta\\gama滤波\n",
    "        self.conv1 = nn.Conv2d(1, f1, (1, kernel_size), (1,1), padding='same', bias=False) \n",
    "        self.batch_norm1 = nn.BatchNorm2d(f1)\n",
    "        # 计算对个频率滤波器进行各通道间的卷积\n",
    "        self.depthwise_conv = nn.Conv2d(f1, f1*D, (number_channel, 1), (1, 1), groups=f1, bias=False)\n",
    "        self.batch_norm2 = nn.BatchNorm2d(f1*D)\n",
    "        self.elu = nn.ELU()\n",
    "        # 平均池化，采样率降低到32Hz左右\n",
    "        self.avg_pool1 = nn.AvgPool2d((1, pooling_size1))\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        # 先计算各频率滤波器500ms时长的卷积，提特征\n",
    "        self.separable_conv = nn.Conv2d(f1*D, f2, (1, 16), padding='same', groups=f1*D, bias=False)\n",
    "        # 再融合各通道信息，构成完整的可分离卷积\n",
    "        self.seperable_conv_1x1 = nn.Conv2d(f2, f2, (1, 1), padding='same', bias=False)\n",
    "        self.batch_norm3 = nn.BatchNorm2d(f2)\n",
    "        # 再次平均池化控制特征的长度达到降低纬度的目的\n",
    "        self.avg_pool2 = nn.AvgPool2d((1, pooling_size2))\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        self.classifier = nn.Linear(f2 * (signal_length // pooling_size1 // pooling_size2), nb_classes)\n",
    "        self.norm_rate = norm_rate\n",
    "        # 注册forward hook\n",
    "        # Register forward hooks to apply max_norm constraint\n",
    "        self.depthwise_conv.register_forward_pre_hook(self.apply_max_norm_depthwise)\n",
    "        self.classifier.register_forward_pre_hook(self.apply_max_norm_classifier)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.elu(x)\n",
    "        x = self.depthwise_conv(x)\n",
    "        x = self.batch_norm2(x)\n",
    "        x = self.elu(x)\n",
    "        x = self.avg_pool1(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = self.separable_conv(x)\n",
    "        x = self.seperable_conv_1x1(x)\n",
    "        x = self.batch_norm3(x)\n",
    "        x = self.elu(x)\n",
    "        x = self.avg_pool2(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def apply_max_norm_depthwise(self, module, input):\n",
    "        with torch.no_grad():\n",
    "            norm = self.depthwise_conv.weight.data.norm(2, dim=1, keepdim=True)\n",
    "            desired = torch.clamp(norm, max=1.0)\n",
    "            scale = desired / (norm + 1e-8)\n",
    "            self.depthwise_conv.weight.data *= scale\n",
    "\n",
    "    def apply_max_norm_classifier(self, module, input):\n",
    "        with torch.no_grad():\n",
    "            norm = self.classifier.weight.data.norm(2, dim=0, keepdim=True)\n",
    "            desired = torch.clamp(norm, max=self.norm_rate)\n",
    "            scale = desired / (norm + 1e-8)\n",
    "            self.classifier.weight.data *= scale\n",
    "\n",
    "\n",
    "class ShallowConvNet(nn.Module):\n",
    "    def __init__(self, number_channel=22, nb_classes=4, dropout_rate=0.5):\n",
    "        # self.patch_size = patch_size\n",
    "        super().__init__()\n",
    "\n",
    "        self.shallownet = nn.Sequential(\n",
    "            nn.Conv2d(1, 40, (1, 25), (1, 1)),\n",
    "            nn.Conv2d(40, 40, (number_channel, 1), (1, 1)),\n",
    "            nn.BatchNorm2d(40),\n",
    "            nn.ELU(),\n",
    "            nn.AvgPool2d((1, 75), (1, 15)),  # pooling acts as slicing to obtain 'patch' along the time dimension as in ViT\n",
    "            nn.Dropout(dropout_rate),\n",
    "        )\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.classifier = nn.Linear(2440, nb_classes)\n",
    "\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = self.shallownet(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "   \n",
    "   \n",
    " \n",
    "\n",
    "class DeepConvNet(nn.Module):\n",
    "    def __init__(self, number_channel=22, nb_classes=4, dropout_rate=0.5):\n",
    "        # self.patch_size = patch_size\n",
    "        super().__init__()\n",
    "\n",
    "        self.deepet = nn.Sequential(\n",
    "            nn.Conv2d(1, 25, (1, 10), (1, 1)),\n",
    "            nn.Conv2d(25, 25, (number_channel, 1), (1, 1)),\n",
    "            nn.BatchNorm2d(25),\n",
    "            nn.ELU(),\n",
    "            nn.MaxPool2d((1,3), (1,3)),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            \n",
    "            nn.Conv2d(25, 50, (1, 10), (1, 1)),\n",
    "            nn.BatchNorm2d(50),\n",
    "            nn.ELU(),\n",
    "            nn.MaxPool2d((1,3), (1,3)),\n",
    "            nn.Dropout(dropout_rate),\n",
    "\n",
    "            \n",
    "            nn.Conv2d(50, 100, (1, 10), (1, 1)),\n",
    "            nn.BatchNorm2d(100),\n",
    "            nn.ELU(),\n",
    "            nn.MaxPool2d((1,3), (1,3)),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            \n",
    "            nn.Conv2d(100, 200, (1, 10), (1, 1)),\n",
    "            nn.BatchNorm2d(200),\n",
    "            nn.ELU(),\n",
    "            nn.MaxPool2d((1,3), (1,3)),\n",
    "            nn.Dropout(dropout_rate),            \n",
    "        )\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.classifier = nn.Linear(1400, nb_classes)    \n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "#         b, _, _, _ = x.shape\n",
    "        x = self.deepet(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.classifier(x)\n",
    "        return x     \n",
    "    \n",
    "    \n",
    "class Conformer(nn.Module):\n",
    "    def __init__(self, number_channel=22, nb_classes=4, dropout_rate=0.5):\n",
    "        # self.patch_size = patch_size\n",
    "        super().__init__()\n",
    "\n",
    "        self.shallownet = nn.Sequential(\n",
    "            nn.Conv2d(1, 40, (1, 25), (1, 1)),\n",
    "            nn.Conv2d(40, 40, (number_channel, 1), (1, 1)),\n",
    "            nn.BatchNorm2d(40),\n",
    "            nn.ELU(),\n",
    "            nn.AvgPool2d((1, 75), (1, 15)),  # pooling acts as slicing to obtain 'patch' along the time dimension as in ViT\n",
    "            nn.Dropout(dropout_rate),\n",
    "        )\n",
    "\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Conv2d(40, 40, (1, 1), stride=(1, 1)),  # transpose, conv could enhance fiting ability slightly\n",
    "            Rearrange('b e (h) (w) -> b (h w) e'),\n",
    "        )\n",
    "        self.trans = TransformerEncoder(10, 6, 40)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(2440, nb_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "#         b, _, _, _ = x.shape\n",
    "        x = self.shallownet(x)\n",
    "        x = self.projection(x)\n",
    "        x = self.trans(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class PatchEmbeddingCNN(nn.Module):\n",
    "    def __init__(self, f1=16, kernel_size=64, D=2, pooling_size1=8, pooling_size2=8, dropout_rate=0.3, number_channel=22, emb_size=40):\n",
    "        super().__init__()\n",
    "        f2 = D*f1\n",
    "        self.cnn_module = nn.Sequential(\n",
    "            # temporal conv kernel size 64=0.25fs\n",
    "            nn.Conv2d(1, f1, (1, kernel_size), (1, 1), padding='same', bias=False), # [batch, 22, 1000] \n",
    "            nn.BatchNorm2d(f1),\n",
    "            # channel depth-wise conv\n",
    "            nn.Conv2d(f1, f2, (number_channel, 1), (1, 1), groups=f1, padding='valid', bias=False), # \n",
    "            nn.BatchNorm2d(f2),\n",
    "            nn.ELU(),\n",
    "            # average pooling 1\n",
    "            nn.AvgPool2d((1, pooling_size1)),  # pooling acts as slicing to obtain 'patch' along the time dimension as in ViT\n",
    "            nn.Dropout(dropout_rate),\n",
    "            # spatial conv\n",
    "            nn.Conv2d(f2, f2, (1, 16), padding='same', bias=False), \n",
    "            nn.Conv2d(f2, f2, (1, 1), bias=False), \n",
    "            nn.BatchNorm2d(f2),\n",
    "            nn.ELU(),\n",
    "            # average pooling 2 to adjust the length of feature into transformer encoder\n",
    "            nn.AvgPool2d((1, pooling_size2)),\n",
    "            nn.Dropout(dropout_rate),  \n",
    "                    \n",
    "        )\n",
    "\n",
    "        self.projection = nn.Sequential(\n",
    "            Rearrange('b e (h) (w) -> b (h w) e'),\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        b, _, _, _ = x.shape\n",
    "        x = self.cnn_module(x)\n",
    "        x = self.projection(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "  \n",
    "    \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, emb_size, num_heads, dropout):\n",
    "        super().__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.num_heads = num_heads\n",
    "        self.keys = nn.Linear(emb_size, emb_size)\n",
    "        self.queries = nn.Linear(emb_size, emb_size)\n",
    "        self.values = nn.Linear(emb_size, emb_size)\n",
    "        self.att_drop = nn.Dropout(dropout)\n",
    "        self.projection = nn.Linear(emb_size, emb_size)\n",
    "\n",
    "    def forward(self, x: Tensor, mask: Tensor = None) -> Tensor:\n",
    "        queries = rearrange(self.queries(x), \"b n (h d) -> b h n d\", h=self.num_heads)\n",
    "        keys = rearrange(self.keys(x), \"b n (h d) -> b h n d\", h=self.num_heads)\n",
    "        values = rearrange(self.values(x), \"b n (h d) -> b h n d\", h=self.num_heads)\n",
    "        energy = torch.einsum('bhqd, bhkd -> bhqk', queries, keys)  \n",
    "        if mask is not None:\n",
    "            fill_value = torch.finfo(torch.float32).min\n",
    "            energy.mask_fill(~mask, fill_value)\n",
    "\n",
    "        scaling = self.emb_size ** (1 / 2)\n",
    "        att = F.softmax(energy / scaling, dim=-1)\n",
    "        att = self.att_drop(att)\n",
    "        out = torch.einsum('bhal, bhlv -> bhav ', att, values)\n",
    "        out = rearrange(out, \"b h n d -> b n (h d)\")\n",
    "        out = self.projection(out)\n",
    "        return out\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ClassificationHead(nn.Sequential):\n",
    "    def __init__(self, flatten_number, n_classes):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(flatten_number, n_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc(x)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "class ResidualAdd(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        res = x\n",
    "        x = self.fn(x, **kwargs)\n",
    "        x += res\n",
    "        return x\n",
    "\n",
    "\n",
    "class FeedForwardBlock(nn.Sequential):\n",
    "    def __init__(self, emb_size, expansion, drop_p):\n",
    "        super().__init__(\n",
    "            nn.Linear(emb_size, expansion * emb_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop_p),\n",
    "            nn.Linear(expansion * emb_size, emb_size),\n",
    "        )\n",
    "\n",
    "\n",
    "class GELU(nn.Module):\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        return input*0.5*(1.0+torch.erf(input/math.sqrt(2.0)))\n",
    "\n",
    "\n",
    "class TransformerEncoderBlock(nn.Sequential):\n",
    "    def __init__(self,\n",
    "                 emb_size,\n",
    "                 num_heads=10,\n",
    "                 drop_p=0.5,\n",
    "                 forward_expansion=4,\n",
    "                 forward_drop_p=0.5):\n",
    "        super().__init__(\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                nn.LayerNorm(emb_size),\n",
    "                MultiHeadAttention(emb_size, num_heads, drop_p),\n",
    "                nn.Dropout(drop_p)\n",
    "            )),\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                nn.LayerNorm(emb_size),\n",
    "                FeedForwardBlock(\n",
    "                    emb_size, expansion=forward_expansion, drop_p=forward_drop_p),\n",
    "                nn.Dropout(drop_p)\n",
    "            )\n",
    "            ))\n",
    "\n",
    "\n",
    "class TransformerEncoder(nn.Sequential):\n",
    "    def __init__(self, num_heads, depth, emb_size):\n",
    "        super().__init__(*[TransformerEncoderBlock(emb_size, num_heads) for _ in range(depth)])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class BranchEEGNetTransformer(nn.Sequential):\n",
    "    def __init__(self, heads=4, \n",
    "                 depth=6, \n",
    "                 emb_size=40, \n",
    "                 number_channel=22,\n",
    "                 f1 = 20,\n",
    "                 kernel_size = 64,\n",
    "                 D = 2,\n",
    "                 pooling_size1 = 8,\n",
    "                 pooling_size2 = 8,\n",
    "                 dropout_rate = 0.3,\n",
    "                 **kwargs):\n",
    "        super().__init__(\n",
    "            PatchEmbeddingCNN(f1=f1, \n",
    "                                 kernel_size=kernel_size,\n",
    "                                 D=D, \n",
    "                                 pooling_size1=pooling_size1, \n",
    "                                 pooling_size2=pooling_size2, \n",
    "                                 dropout_rate=dropout_rate,\n",
    "                                 number_channel=number_channel,\n",
    "                                 emb_size=emb_size),\n",
    "            TransformerEncoder(heads, depth, emb_size),\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "class EEGTransformer(nn.Module):\n",
    "    def __init__(self, heads=4, \n",
    "                 emb_size=40,\n",
    "                 depth=6, \n",
    "                 database_type='A', \n",
    "                 eeg1_f1 = 20,\n",
    "                 eeg1_kernel_size = 64,\n",
    "                 eeg1_D = 2,\n",
    "                 eeg1_pooling_size1 = 8,\n",
    "                 eeg1_pooling_size2 = 8,\n",
    "                 eeg1_dropout_rate = 0.3,\n",
    "                 eeg1_number_channel = 22,\n",
    "                 flatten_eeg1 = 600,\n",
    "                 **kwargs):\n",
    "        super().__init__()\n",
    "        self.number_class, self.number_channel = numberClassChannel(database_type)\n",
    "        self.flatten_eeg1 = flatten_eeg1\n",
    "        self.net = Conformer(number_channel=self.number_channel, nb_classes=self.number_class)\n",
    "        # print('self.number_channel', self.number_channel)\n",
    "#         self.eegNet = BranchEEGNetTransformer(heads, depth, emb_size, number_channel=self.number_channel,\n",
    "#                                               f1 = eeg1_f1,\n",
    "#                                               kernel_size = eeg1_kernel_size,\n",
    "#                                               D = eeg1_D,\n",
    "#                                               pooling_size1 = eeg1_pooling_size1,\n",
    "#                                               pooling_size2 = eeg1_pooling_size2,\n",
    "#                                               dropout_rate = eeg1_dropout_rate,\n",
    "#                                               )\n",
    "#         # self.cnn_module = Branchcnn_moduleTransformer(heads, depth, emb_size)\n",
    "#         self.flatten = nn.Flatten()\n",
    "#         self.classification = ClassificationHead(self.flatten_eeg1 , self.number_class) # FLATTEN_EEGNet + FLATTEN_cnn_module\n",
    "    def forward(self, x):\n",
    "#         branchEEGNet1 = self.eegNet(x)\n",
    "#         # branchcnn_module1 = self.cnn_module(x)\n",
    "#         features = torch.cat([branchEEGNet1], dim=-2) # branchcnn_module1, \n",
    "#         net = torch.cat([self.flatten(branchEEGNet1)], dim=-1) # , self.flatten(branchcnn_module1), \n",
    "#         out = self.classification(net)\n",
    "        out = self.net(x)\n",
    "        return 0, out\n",
    "\n",
    "\n",
    "class ExP():\n",
    "    def __init__(self, nsub, data_dir, result_name, \n",
    "                 epochs=2000, \n",
    "                 number_aug=2,\n",
    "                 number_seg=8, \n",
    "                 gpus=[0], \n",
    "                 evaluate_mode = 'subject-dependent',\n",
    "                 heads=4, \n",
    "                 emb_size=40,\n",
    "                 depth=6, \n",
    "                 dataset_type='A',\n",
    "                 eeg1_f1 = 20,\n",
    "                 eeg1_kernel_size = 64,\n",
    "                 eeg1_D = 2,\n",
    "                 eeg1_pooling_size1 = 8,\n",
    "                 eeg1_pooling_size2 = 8,\n",
    "                 eeg1_dropout_rate = 0.3,\n",
    "                 flatten_eeg1 = 600, \n",
    "                 validate_ratio = 0.2,\n",
    "                 learning_rate = 0.001,\n",
    "                 batch_size = 72,  \n",
    "                 ):\n",
    "        \n",
    "        super(ExP, self).__init__()\n",
    "        self.dataset_type = dataset_type\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = learning_rate\n",
    "        self.b1 = 0.5\n",
    "        self.b2 = 0.999\n",
    "        self.n_epochs = epochs\n",
    "        self.nSub = nsub\n",
    "        self.number_augmentation = number_aug\n",
    "        self.number_seg = number_seg\n",
    "        self.root = data_dir\n",
    "        self.heads=heads\n",
    "        self.emb_size=emb_size\n",
    "        self.depth=depth\n",
    "        self.result_name = result_name\n",
    "        self.evaluate_mode = evaluate_mode\n",
    "        self.validate_ratio = validate_ratio\n",
    "\n",
    "        self.Tensor = torch.cuda.FloatTensor\n",
    "        self.LongTensor = torch.cuda.LongTensor\n",
    "        self.criterion_cls = torch.nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "        self.number_class, self.number_channel = numberClassChannel(self.dataset_type)\n",
    "        self.model = EEGTransformer(\n",
    "             heads=self.heads, \n",
    "             emb_size=self.emb_size,\n",
    "             depth=self.depth, \n",
    "            database_type=self.dataset_type, \n",
    "            eeg1_f1=eeg1_f1, \n",
    "            eeg1_D=eeg1_D,\n",
    "            eeg1_kernel_size=eeg1_kernel_size,\n",
    "            eeg1_pooling_size1 = eeg1_pooling_size1,\n",
    "            eeg1_pooling_size2 = eeg1_pooling_size2,\n",
    "            eeg1_dropout_rate = eeg1_dropout_rate,\n",
    "            eeg1_number_channel = self.number_channel,\n",
    "            flatten_eeg1 = flatten_eeg1,  \n",
    "            ).cuda()\n",
    "        #self.model = nn.DataParallel(self.model, device_ids=gpus)\n",
    "        self.model = self.model.cuda()\n",
    "        self.model_filename = self.result_name + '/model_{}.pth'.format(self.nSub)\n",
    "\n",
    "    # Segmentation and Reconstruction (S&R) data augmentation\n",
    "    def interaug(self, timg, label):  \n",
    "        aug_data = []\n",
    "        aug_label = []\n",
    "        number_records_by_augmentation = self.number_augmentation * int(self.batch_size / self.number_class)\n",
    "        number_segmentation_points = 1000 // self.number_seg\n",
    "        for clsAug in range(self.number_class):\n",
    "            cls_idx = np.where(label == clsAug + 1)\n",
    "            tmp_data = timg[cls_idx]\n",
    "            tmp_label = label[cls_idx]\n",
    "            \n",
    "            tmp_aug_data = np.zeros((number_records_by_augmentation, 1, self.number_channel, 1000))\n",
    "            for ri in range(number_records_by_augmentation):\n",
    "                for rj in range(self.number_seg):\n",
    "                    rand_idx = np.random.randint(0, tmp_data.shape[0], self.number_seg)\n",
    "                    tmp_aug_data[ri, :, :, rj * number_segmentation_points:(rj + 1) * number_segmentation_points] = \\\n",
    "                        tmp_data[rand_idx[rj], :, :, rj * number_segmentation_points:(rj + 1) * number_segmentation_points]\n",
    "\n",
    "            aug_data.append(tmp_aug_data)\n",
    "            aug_label.append(tmp_label[:number_records_by_augmentation])\n",
    "        aug_data = np.concatenate(aug_data)\n",
    "        aug_label = np.concatenate(aug_label)\n",
    "        aug_shuffle = np.random.permutation(len(aug_data))\n",
    "        aug_data = aug_data[aug_shuffle, :, :]\n",
    "        aug_label = aug_label[aug_shuffle]\n",
    "\n",
    "        aug_data = torch.from_numpy(aug_data).cuda()\n",
    "        aug_data = aug_data.float()\n",
    "        aug_label = torch.from_numpy(aug_label-1).cuda()\n",
    "        aug_label = aug_label.long()\n",
    "        return aug_data, aug_label\n",
    "\n",
    "\n",
    "\n",
    "    def get_source_data(self):\n",
    "        (self.train_data,    # (batch, channel, length)\n",
    "         self.train_label, \n",
    "         self.test_data, \n",
    "         self.test_label) = load_data_evaluate(self.root, self.dataset_type, self.nSub, mode_evaluate=self.evaluate_mode)\n",
    "\n",
    "        self.train_data = np.expand_dims(self.train_data, axis=1)  # (288, 1, 22, 1000)\n",
    "        self.train_label = np.transpose(self.train_label)  \n",
    "\n",
    "        self.allData = self.train_data\n",
    "        self.allLabel = self.train_label[0]  \n",
    "\n",
    "        shuffle_num = np.random.permutation(len(self.allData))\n",
    "        # print(\"len(self.allData):\", len(self.allData))\n",
    "        self.allData = self.allData[shuffle_num, :, :, :]  # (288, 1, 22, 1000)\n",
    "        # print(\"shuffle_num\", shuffle_num)\n",
    "        # print(\"self.allLabel\", self.allLabel)\n",
    "        self.allLabel = self.allLabel[shuffle_num]\n",
    "\n",
    "\n",
    "        print('-'*20, \"train size：\", self.train_data.shape, \"test size：\", self.test_data.shape)\n",
    "        # self.test_data = np.transpose(self.test_data, (2, 1, 0))\n",
    "        self.test_data = np.expand_dims(self.test_data, axis=1)\n",
    "        self.test_label = np.transpose(self.test_label)\n",
    "\n",
    "        self.testData = self.test_data\n",
    "        self.testLabel = self.test_label[0]\n",
    "\n",
    "\n",
    "        # standardize\n",
    "        target_mean = np.mean(self.allData)\n",
    "        target_std = np.std(self.allData)\n",
    "        self.allData = (self.allData - target_mean) / target_std\n",
    "        self.testData = (self.testData - target_mean) / target_std\n",
    "        \n",
    "        isSaveDataLabel = False #True\n",
    "        if isSaveDataLabel:\n",
    "            np.save(\"./gradm_data/train_data_{}.npy\".format(self.nSub), self.allData)\n",
    "            np.save(\"./gradm_data/train_lable_{}.npy\".format(self.nSub), self.allLabel)\n",
    "            np.save(\"./gradm_data/test_data_{}.npy\".format(self.nSub), self.testData)\n",
    "            np.save(\"./gradm_data/test_label_{}.npy\".format(self.nSub), self.testLabel)\n",
    "\n",
    "        \n",
    "        # data shape: (trial, conv channel, electrode channel, time samples)\n",
    "        return self.allData, self.allLabel, self.testData, self.testLabel\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        img, label, test_data, test_label = self.get_source_data()\n",
    "        # print(\"label size:\", label.shape)\n",
    "        # print(\"label size:\", label)\n",
    "        \n",
    "        img = torch.from_numpy(img)\n",
    "        label = torch.from_numpy(label - 1)\n",
    "        dataset = torch.utils.data.TensorDataset(img, label)\n",
    "        self.dataloader = torch.utils.data.DataLoader(dataset=dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "        test_data = torch.from_numpy(test_data)\n",
    "        test_label = torch.from_numpy(test_label - 1)\n",
    "        test_dataset = torch.utils.data.TensorDataset(test_data, test_label)\n",
    "        self.test_dataloader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "        # Optimizers\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr, betas=(self.b1, self.b2))\n",
    "\n",
    "        test_data = Variable(test_data.type(self.Tensor))\n",
    "        test_label = Variable(test_label.type(self.LongTensor))\n",
    "        best_epoch = 0\n",
    "        num = 0\n",
    "        min_loss = 100\n",
    "        # recording train_acc, train_loss, test_acc, test_loss\n",
    "        result_process = []\n",
    "        # Train the cnn model\n",
    "        for e in range(self.n_epochs):\n",
    "            epoch_process = {}\n",
    "            epoch_process['epoch'] = e\n",
    "            # in_epoch = time.time()\n",
    "            self.model.train()\n",
    "            outputs_list = []\n",
    "            label_list = []\n",
    "            # 验证集\n",
    "            val_data_list = []\n",
    "            val_label_list = []\n",
    "            for i, (img, label) in enumerate(self.dataloader):\n",
    "                number_sample = img.shape[0]\n",
    "                number_validate = int(self.validate_ratio * number_sample)\n",
    "                \n",
    "                # split raw train dataset into real train dataset and validate dataset\n",
    "                train_data = img[:-number_validate]\n",
    "                train_label = label[:-number_validate]\n",
    "                \n",
    "                val_data_list.append(img[number_validate:])\n",
    "                val_label_list.append(label[number_validate:])\n",
    "                \n",
    "                # real train dataset\n",
    "                img = Variable(train_data.type(self.Tensor))\n",
    "                label = Variable(train_label.type(self.LongTensor))\n",
    "                \n",
    "                # data augmentation\n",
    "                aug_data, aug_label = self.interaug(self.allData, self.allLabel)\n",
    "                # concat real train dataset and generate aritifical train dataset\n",
    "                img = torch.cat((img, aug_data))\n",
    "                label = torch.cat((label, aug_label))\n",
    "\n",
    "                # training model\n",
    "                features, outputs = self.model(img)\n",
    "                outputs_list.append(outputs)\n",
    "                label_list.append(label)\n",
    "                # print(\"train outputs: \", outputs.shape, type(outputs))\n",
    "                # print(features.size())\n",
    "                loss = self.criterion_cls(outputs, label) \n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "            del img\n",
    "            torch.cuda.empty_cache()\n",
    "            # out_epoch = time.time()\n",
    "            # test process\n",
    "            if (e + 1) % 1 == 0:\n",
    "                self.model.eval()\n",
    "                # validate model\n",
    "                val_data = torch.cat(val_data_list).cuda()\n",
    "                val_label = torch.cat(val_label_list).cuda()\n",
    "                val_data = val_data.type(self.Tensor)\n",
    "                val_label = val_label.type(self.LongTensor)            \n",
    "                \n",
    "                val_dataset = torch.utils.data.TensorDataset(val_data, val_label)\n",
    "                self.val_dataloader = torch.utils.data.DataLoader(dataset=val_dataset, batch_size=self.batch_size, shuffle=False)\n",
    "                outputs_list = []\n",
    "                with torch.no_grad():\n",
    "                    for i, (img, _) in enumerate(self.val_dataloader):\n",
    "                        # val model\n",
    "                        img = img.type(self.Tensor).cuda()\n",
    "                        _, Cls = self.model(img)\n",
    "                        outputs_list.append(Cls)\n",
    "                        del img, Cls\n",
    "                        torch.cuda.empty_cache()\n",
    "                    \n",
    "                Cls = torch.cat(outputs_list)\n",
    "                \n",
    "                val_loss = self.criterion_cls(Cls, val_label)\n",
    "                val_pred = torch.max(Cls, 1)[1]\n",
    "                val_acc = float((val_pred == val_label).cpu().numpy().astype(int).sum()) / float(val_label.size(0))\n",
    "                \n",
    "                epoch_process['val_acc'] = val_acc                \n",
    "                epoch_process['val_loss'] = val_loss.detach().cpu().numpy()  \n",
    "                \n",
    "                train_pred = torch.max(outputs, 1)[1]\n",
    "                train_acc = float((train_pred == label).cpu().numpy().astype(int).sum()) / float(label.size(0))\n",
    "                epoch_process['train_acc'] = train_acc\n",
    "                epoch_process['train_loss'] = loss.detach().cpu().numpy()\n",
    "\n",
    "                num = num + 1\n",
    "\n",
    "                # if min_loss>val_loss:                \n",
    "                if min_loss>val_loss:\n",
    "                    min_loss = val_loss\n",
    "                    best_epoch = e\n",
    "                    epoch_process['epoch'] = e\n",
    "                    torch.save(self.model, self.model_filename)\n",
    "                    print(\"{}_{} train_acc: {:.4f} train_loss: {:.6f}\\tval_acc: {:.6f} val_loss: {:.7f}\".format(self.nSub,\n",
    "                                                                                           epoch_process['epoch'],\n",
    "                                                                                           epoch_process['train_acc'],\n",
    "                                                                                           epoch_process['train_loss'],\n",
    "                                                                                           epoch_process['val_acc'],\n",
    "                                                                                           epoch_process['val_loss'],\n",
    "                                                                                        ))\n",
    "            \n",
    "                \n",
    "            result_process.append(epoch_process)  \n",
    "\n",
    "        \n",
    "            del label, val_data, val_label\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        # load model for test\n",
    "        self.model.eval()\n",
    "        self.model = torch.load(self.model_filename).cuda()\n",
    "        outputs_list = []\n",
    "        with torch.no_grad():\n",
    "            for i, (img, label) in enumerate(self.test_dataloader):\n",
    "                img_test = Variable(img.type(self.Tensor)).cuda()\n",
    "                # label_test = Variable(label.type(self.LongTensor))\n",
    "\n",
    "                # test model\n",
    "                features, outputs = self.model(img_test)\n",
    "                val_pred = torch.max(outputs, 1)[1]\n",
    "                outputs_list.append(outputs)\n",
    "        outputs = torch.cat(outputs_list) \n",
    "        y_pred = torch.max(outputs, 1)[1]\n",
    "        \n",
    "        \n",
    "        test_acc = float((y_pred == test_label).cpu().numpy().astype(int).sum()) / float(test_label.size(0))\n",
    "        \n",
    "        print(\"epoch: \", best_epoch, '\\tThe test accuracy is:', test_acc)\n",
    "\n",
    "\n",
    "        df_process = pd.DataFrame(result_process)\n",
    "\n",
    "        return test_acc, test_label, y_pred, df_process, best_epoch\n",
    "        # writer.close()\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main(dirs,                \n",
    "         evaluate_mode = 'subject-dependent', # 评估模式：LOSO（跨个体）或其他（subject-dependent, subject-specific），\n",
    "         heads=8,             # heads of MHA\n",
    "         emb_size=48,         # token embding dim\n",
    "         depth=3,             # Transformer encoder depth\n",
    "         dataset_type='A',    # A->'BCI IV2a', B->'BCI IV2b'\n",
    "         eeg1_f1=20,          # features of temporal conv\n",
    "         eeg1_kernel_size=64, # kernel size of temporal conv\n",
    "         eeg1_D=2,            # depth-wise conv \n",
    "         eeg1_pooling_size1=8,# p1\n",
    "         eeg1_pooling_size2=8,# p2\n",
    "         eeg1_dropout_rate=0.3,\n",
    "         flatten_eeg1=600,   \n",
    "         validate_ratio = 0.2\n",
    "         ):\n",
    "\n",
    "    if not os.path.exists(dirs):\n",
    "        os.makedirs(dirs)\n",
    "\n",
    "    result_write_metric = ExcelWriter(dirs+\"/result_metric.xlsx\")\n",
    "    \n",
    "    result_metric_dict = {}\n",
    "    y_true_pred_dict = { }\n",
    "\n",
    "    process_write = ExcelWriter(dirs+\"/process_train.xlsx\")\n",
    "    pred_true_write = ExcelWriter(dirs+\"/pred_true.xlsx\")\n",
    "    subjects_result = []\n",
    "    best_epochs = []\n",
    "    \n",
    "    for i in range(N_SUBJECT):      \n",
    "        \n",
    "        starttime = datetime.datetime.now()\n",
    "        seed_n = np.random.randint(2024)\n",
    "        print('seed is ' + str(seed_n))\n",
    "        random.seed(seed_n)\n",
    "        np.random.seed(seed_n)\n",
    "        torch.manual_seed(seed_n)\n",
    "        torch.cuda.manual_seed(seed_n)\n",
    "        torch.cuda.manual_seed_all(seed_n)\n",
    "        index_round =0\n",
    "        print('Subject %d' % (i+1))\n",
    "        exp = ExP(i + 1, DATA_DIR, dirs, EPOCHS, N_AUG, N_SEG, gpus, \n",
    "                  evaluate_mode = evaluate_mode,\n",
    "                  heads=heads, \n",
    "                  emb_size=emb_size,\n",
    "                  depth=depth, \n",
    "                  dataset_type=dataset_type,\n",
    "                  eeg1_f1 = eeg1_f1,\n",
    "                  eeg1_kernel_size = eeg1_kernel_size,\n",
    "                  eeg1_D = eeg1_D,\n",
    "                  eeg1_pooling_size1 = eeg1_pooling_size1,\n",
    "                  eeg1_pooling_size2 = eeg1_pooling_size2,\n",
    "                  eeg1_dropout_rate = eeg1_dropout_rate,\n",
    "                  flatten_eeg1 = flatten_eeg1,  \n",
    "                  validate_ratio = validate_ratio\n",
    "                  )\n",
    "\n",
    "        testAcc, Y_true, Y_pred, df_process, best_epoch = exp.train()\n",
    "        true_cpu = Y_true.cpu().numpy().astype(int)\n",
    "        pred_cpu = Y_pred.cpu().numpy().astype(int)\n",
    "        df_pred_true = pd.DataFrame({'pred': pred_cpu, 'true': true_cpu})\n",
    "        df_pred_true.to_excel(pred_true_write, sheet_name=str(i+1))\n",
    "        y_true_pred_dict[i] = df_pred_true\n",
    "\n",
    "        accuracy, precison, recall, f1, kappa = calMetrics(true_cpu, pred_cpu)\n",
    "        subject_result = {'accuray': accuracy*100,\n",
    "                          'precision': precison*100,\n",
    "                          'recall': recall*100,\n",
    "                          'f1': f1*100, \n",
    "                          'kappa': kappa*100\n",
    "                          }\n",
    "        subjects_result.append(subject_result)\n",
    "        df_process.to_excel(process_write, sheet_name=str(i+1))\n",
    "        best_epochs.append(best_epoch)\n",
    "    \n",
    "        print(' THE BEST ACCURACY IS ' + str(testAcc) + \"\\tkappa is \" + str(kappa) )\n",
    "    \n",
    "\n",
    "        endtime = datetime.datetime.now()\n",
    "        print('subject %d duration: '%(i+1) + str(endtime - starttime))\n",
    "\n",
    "        if i == 0:\n",
    "            yt = Y_true\n",
    "            yp = Y_pred\n",
    "        else:\n",
    "            yt = torch.cat((yt, Y_true))\n",
    "            yp = torch.cat((yp, Y_pred))\n",
    "                \n",
    "        df_result = pd.DataFrame(subjects_result)\n",
    "    process_write.close()\n",
    "    pred_true_write.close()\n",
    "\n",
    "\n",
    "    print('**The average Best accuracy is: ' + str(df_result['accuray'].mean()) + \"kappa is: \" + str(df_result['kappa'].mean()) + \"\\n\" )\n",
    "    print(\"best epochs: \", best_epochs)\n",
    "    #df_result.to_excel(result_write_metric, index=False)\n",
    "    result_metric_dict = df_result\n",
    "\n",
    "    mean = df_result.mean(axis=0)\n",
    "    mean.name = 'mean'\n",
    "    std = df_result.std(axis=0)\n",
    "    std.name = 'std'\n",
    "    df_result = pd.concat([df_result, pd.DataFrame(mean).T, pd.DataFrame(std).T])\n",
    "    \n",
    "    df_result.to_excel(result_write_metric, index=False)\n",
    "    print('-'*9, ' all result ', '-'*9)\n",
    "    print(df_result)\n",
    "    \n",
    "    print(\"*\"*40)\n",
    "\n",
    "    result_write_metric.close()\n",
    "\n",
    "    \n",
    "    return result_metric_dict\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #----------------------------------------\n",
    "    DATA_DIR = r'../mymat_raw/'\n",
    "    EVALUATE_MODE = 'LOSO-No' # leaving one subject out subject-dependent  subject-indenpedent\n",
    "\n",
    "    N_SUBJECT = 9       # BCI \n",
    "    N_AUG = 3           # data augmentation times for benerating artificial training data set\n",
    "    N_SEG = 8           # segmentation times for S&R\n",
    "\n",
    "    EPOCHS = 1000\n",
    "    EMB_DIM = 48\n",
    "    HEADS = 8\n",
    "    DEPTH = 3\n",
    "    TYPE = 'B'\n",
    "    validate_ratio = 0.3 # split raw train dataset into real train dataset and validate dataset\n",
    "\n",
    "    EEGNet1_F1 = 24\n",
    "    EEGNet1_KERNEL_SIZE=64\n",
    "    EEGNet1_D=2\n",
    "    EEGNet1_POOL_SIZE1 = 8\n",
    "    EEGNet1_POOL_SIZE2 = 8\n",
    "\n",
    "    FLATTEN_EEGNet1 = 720\n",
    "\n",
    "    \n",
    "    if EVALUATE_MODE!='LOSO':\n",
    "        EEGNet1_DROPOUT_RATE = 0.5\n",
    "    else:\n",
    "        EEGNet1_DROPOUT_RATE = 0.25  \n",
    "    \n",
    "    \n",
    "    parameters_list = ['A', 'B']\n",
    "    for TYPE in parameters_list:\n",
    "        number_class, number_channel = numberClassChannel(TYPE)\n",
    "        RESULT_NAME = \"Conformer_同等_{}_\".format(TYPE)\n",
    "    \n",
    "        sModel = EEGTransformer(\n",
    "            heads=HEADS, \n",
    "            emb_size=EMB_DIM,\n",
    "            depth=DEPTH, \n",
    "            database_type=TYPE,\n",
    "            eeg1_f1=EEGNet1_F1, \n",
    "            eeg1_D=EEGNet1_D,\n",
    "            eeg1_kernel_size=EEGNet1_KERNEL_SIZE,\n",
    "            eeg1_pooling_size1 = EEGNet1_POOL_SIZE1,\n",
    "            eeg1_pooling_size2 = EEGNet1_POOL_SIZE2,\n",
    "            eeg1_dropout_rate = EEGNet1_DROPOUT_RATE,\n",
    "            eeg1_number_channel = number_channel,\n",
    "            flatten_eeg1 = FLATTEN_EEGNet1,  \n",
    "            ).cuda()\n",
    "        summary(sModel, (1, number_channel, 1000)) \n",
    "    \n",
    "        print(time.asctime(time.localtime(time.time())))\n",
    "        \n",
    "        result = main(RESULT_NAME,\n",
    "                        evaluate_mode = EVALUATE_MODE,\n",
    "                        heads=HEADS, \n",
    "                        emb_size=EMB_DIM,\n",
    "                        depth=DEPTH, \n",
    "                        dataset_type=TYPE,\n",
    "                        eeg1_f1 = EEGNet1_F1,\n",
    "                        eeg1_kernel_size = EEGNet1_KERNEL_SIZE,\n",
    "                        eeg1_D = EEGNet1_D,\n",
    "                        eeg1_pooling_size1 = EEGNet1_POOL_SIZE1,\n",
    "                        eeg1_pooling_size2 = EEGNet1_POOL_SIZE2,\n",
    "                        eeg1_dropout_rate = EEGNet1_DROPOUT_RATE,\n",
    "                        flatten_eeg1 = FLATTEN_EEGNet1,\n",
    "                        validate_ratio = validate_ratio,\n",
    "                      )\n",
    "        print(time.asctime(time.localtime(time.time())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e87c09c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
